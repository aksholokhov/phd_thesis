\documentclass[8pt]{beamer}
\input{useful_packages}
\input{math_macros}


% \usetheme[progressbar=frametitle]{metropolis}
\usepackage{appendixnumberbeamer}


% PRESETS OF PACKAGES AND MATH %

\usepackage{xspace}
\newcommand{\themename}{\textbf{\textsc{metropolis}}\xspace}
\newcommand{\ouralgo}{\ensuremath{\mathcal{MSR}3}}
\title{Feature Selection for Mixed-Effects Models}
\date{\today}
\author{Aleksei Sholokhov}
% \titlegraphic{\hfill\includegraphics[height=1.5cm]{logo.pdf}}

\begin{document}

\maketitle

\begin{frame}{Plan}
	\tableofcontents
\end{frame}

\section{Feature Selection for Linear Mixed-Effect Models}
\subsection{Linear Mixed-Effects Models}
\begin{frame}{Linear Mixed-Effect Models}

Linear Mixed-Effect (LME) models are often used for analyzing combined data across a range of groups. \newline

They use use covariates to separate the population variability (fixed-effects) from the group variability (random effects). \newline

LMEs borrow strength across groups to estimate key statistics in cases when data within units are sparse or highly variable. \newline

\end{frame}

\begin{frame}{Linear Mixed-Effect Models}

	Dataset: $m$ groups $(X_i, Z_i, y_i),\quad i = 1, \dots m$, each has $n_i$ observations
	\begin{itemize}
		\item 	$X_i \in \R^{n_i \times p}$ -- group $i$ design matrix for fixed features
		\item 	$Z_i \in \R^{n_i \times q}$ -- group $i$ design matrix for random effects
		\item 	$y_i \in \R^{n_i}$ -- group $i$ observations  
	\end{itemize}
	

	\begin{columns}[T,onlytextwidth]
    \column{0.5\textwidth}
	 \centering Standard Linear Regression:
   	\begin{figure}
   		\includegraphics[width=0.9\textwidth]{Figures/lme_example_mean_prediction}
   	\end{figure}
   	\[
   		y = X\beta + \varepsilon, \quad \varepsilon \sim \NN(0, \Lambda) 
   	\]

   	
    \column{0.5\textwidth}
    	\centering  Linear Mixed-Effect Model:
   	\begin{figure}
   		\includegraphics[width=0.9\textwidth]{Figures/lme_example_random_prediction}
   	\end{figure}
   	   		\[
   		\begin{split}
   			y_i & = X_i\beta + {\color{red}Z_i u_i} + \varepsilon_i, \quad \varepsilon_i \sim \NN(0, \Lambda_i) \\
   			{\color{red} u_i} & {\color{red}\sim \NN(0, \Gamma4)}
   		\end{split}
   		\]

   	
  \end{columns}
\end{frame}

\begin{frame}{Notation}
	\eq{
   		y_i & = X_i\beta + Z_iu_i + \varepsilon_i \quad i = 1\dots m \\
   		\varepsilon_i & \sim \NN(0, \Lambda_i) \\
   		u_i & \sim \NN(0, \Gamma)
   	}   	
   	
   	\begin{itemize}
   		\item $p$ -- number of fixed features, $q$ -- number of random effects.
   		\item $\beta \in \R^p$ -- fixed effects, or mean effects
   		\item $u_i \in \R^q$ -- random effects
   		\item $\Gamma \in \R^{q \times q}$ -- covariance matrix of random effects, often $\Gamma = \diag{(\gamma)}$
   		\item $\varepsilon_i \in \R^{n_i}$ -- observation noise
   		\item $\Lambda_i \in R^{n_i \times n_i}$ -- covariance matrix for noise
   	\end{itemize}
   	Unknowns: $\beta$, $u_i$, $\gamma$, sometimes $\Lambda_i$.
\end{frame}

\begin{frame}{Likelihood for Mixed Models}
Negative log-likelihood:
\eq{
	\label{eq:lmm_objective}
	\mathcal{L}(\beta, \gamma) & = \sum_{i = 1}^m \half(y_i - X_i\beta)^T(Z_i\Gamma Z_i^T + \Lambda_i)^{-1}(y_i - X_i\beta) + \\ & + \half\log{\det{\pa{Z_i \Gamma Z_i^T + \Lambda_i}}}, \quad \Gamma = \diag{(\gamma)}
	}
Maximum likelihood estimates for $\beta$ and $\gamma$ solve the problem:
\eq{
	\mathcal{LME} \quad \min_{\beta \in \R^p,\, \gamma \in \R^{q}_+} \LL(\beta, \gamma)
}

To select covariates we add a sparsity-promoting regularizer $R(\beta, \gamma)$
\eq{
	\mathcal{FS-LME} \quad \min_{\beta \in \R^p,\, \gamma \in \R^{q}_+} \LL(\beta, \gamma) + R(\beta, \gamma)
}

\begin{itemize}
	\item $\LL(\beta, \gamma)$ is smooth on its domain, quadratic w.r.t. $\beta$ and $\bar\eta$-weakly-convex w.r.t. $\gamma$.
	\item $R(\beta, \gamma)$ is closed, proper, convex, with easily computed \textit{prox operator}
\end{itemize}

\end{frame}

\begin{frame}{Regularization}
\begin{itemize}
	\item $R(\beta, \gamma)$ is closed, proper, convex, with easily computed \textit{prox operator}
\end{itemize}

\eq{
	\prox_{\alpha R + \delta_{\CC}}(\tbeta, \tgamma) & := \argmin_{(\beta, \gamma) \in \CC} R(\beta, \gamma) + \frac{1}{2\alpha}\|(\beta, \gamma) - (\tbeta, \tgamma)\|_2^2, \\ & \text{ where } \CC := \R^p \times R^q_+ \\
}

Examples: 
\begin{itemize}
	\item $R(x) = \lambda\sum_{j=1}^p w_j\|x_j\|_1$ -- LASSO and Adaptive LASSO penalties \cite{Bondell2010,Lin2013}
	\item $R(x) = \lambda \|x\|_0$ -- $\ell_0$ penalty \cite{Vaida2005,Jones2011}
	\item $R(x)$ -- SCAD penalty (\cite{Fan2001,Fan2012})
\end{itemize}

TODO: add picture

\end{frame}

\begin{frame}{SR3-Relaxation for Mixed-Effect Models}
Original problem $\mathcal{FS-LME}$:
\eq{
	\min_{\beta \in \R^p,\, \gamma \in \R^{q}_+} \LL(\beta, \gamma) + R(\beta, \gamma)
}
Relaxed problem $\ouralgo$:
\eq{
	\label{eq:msr3_formulation_explicit}
	\min_{\beta, \tbeta \in \R^p,\, \gamma, \tgamma \in \R^{q}_+} \LL(\beta, \gamma) + \phi_\mu(\gamma) + \kappa_\eta(\beta - \tbeta, \gamma - \tgamma) + R(\beta, \gamma)
}
where the \textit{relaxation} $\kappa_\eta$ decouples the likelihood and the regularizer 
\eq{
	\kappa_\eta(\beta - \tbeta, \gamma - \tgamma) := \frac{\eta}{2}\|\beta - \tbeta\|^2_2 + \frac{\eta + \bar\eta}{2}\|\gamma - \tgamma\|_2^2
}
and the \textit{projection function} $\phi_\mu$ replaces $\gamma \geq 0$ with a log-barrier 
\eq{
	\phi_\mu(\gamma) := \begin{cases}
		-\mu\sum_{i=1}^q \ln(\gamma_i/\mu), & \mu > 0 \\
		\delta_{\R_+^q}(\gamma), & \mu = 0 \\
		+\infty, & \mu < 0
	\end{cases}
} 
\end{frame}

\begin{frame}{Value Function Reformulation}
$\ouralgo$-relaxation replaces the original likelihood $\LL$ with a \textit{value function} $u_{\eta,\mu}$:
\eq{
	\label{eq:value_function_definition}
	u_{\eta,\mu}(\tbeta,\tgamma) & := \min_{(\beta, \gamma)} \LL_{\eta,\mu}((\beta, \gamma),(\tbeta, \tgamma)) \\  & := \min_{(\beta, \gamma)} \LL(\beta, \gamma) + \phi_\mu(\gamma) + \kappa_\eta(\beta - \tbeta, \gamma - \tgamma)
}
so $\ouralgo$-formulation~(\ref{eq:msr3_formulation_explicit}) becomes
\eq{
	\min_{(\tbeta, \tgamma) \in \CC} u_{\eta,\mu}(\tbeta, \tgamma) + R(\tbeta, \tgamma)
}
When $\bar\eta$ is larger than the weak-convexity constant
\begin{itemize}
	\item $u_{\eta,\mu}$ is well-defined and continuously differentiable.
	\item Solutions $(\tbeta^*, \tgamma^*)$ for $\ouralgo$ converge to solutions $(\beta^*, \gamma^*)$ of $\mathcal{FS-LME}$ \\ when $\mu \rightarrow 0$ and $\eta \rightarrow \infty$.
\end{itemize}

\textbf{Key observation}: in practice, we don't need accurate solutions for~(\ref{eq:value_function_definition}): a few Newton iterations is enough.
\end{frame}

\begin{frame}{Value Function Reformulation}
TODO: insert image here
\end{frame}

\begin{frame}{Designing an Algorithm}
	Gradient of a Lagrangian:
	\eq{
		G_{\nu,\eta}((\beta,\gamma,v),(\tbeta,\tgamma)) := \begin{bmatrix}
			\nabla_\beta \LL(\beta, \gamma) + \eta(\beta-\tbeta) \\
			\nabla_\gamma \LL(\beta, \gamma) + (\bar\eta + \eta)(\gamma-\tgamma) - v\\
			v \bigodot \gamma - \mu\textbf{1}
		\end{bmatrix}
	}
	
\textbf{Lemma:} For every $(\mu,\eta) \in \R_+\times\R_{++}$,
\eq{
	(\hat\beta,\hat\gamma) = \argmin_{(\beta,\gamma)}\LL_{\eta,\mu}((\beta, \gamma),(\tbeta, \tgamma)) \\
	\text{(equivalent to)} \\
	\exists \hat{v} \in \R_{+}^q \text{ s.t. } G_{\nu,\eta}((\beta,\gamma,\hat{v}),(\tbeta,\tgamma)) = 0
}
If $\mu > 0$, then $\hat{v} = -\nabla\phi_\mu(\hat{\gamma})$, and if $\mu = 0$, then $\hat{v}$ is the unique KKT multiplier associated with the constraint $0 \leq \gamma$.
\end{frame}

\begin{frame}{$\ouralgo$ Algorithm}
\input{../extras/08Pseudocode}
\end{frame}

\section{Experiments}
\subsection{Application to Synthetic Problems}

\begin{frame}{Performance in Comparison to Other Algorithms}
\begin{itemize}
	\item \textbf{Scenario 1}: $n=30$, $n_i=5$, $p = 9$, $q = 4$, with true parameters $\beta = (1, 1, 0, \dots, 0)$ and the covariance matrix $\Gamma$ being: 
		\eq{
			\Gamma = \begin{bmatrix}
				9 & 4.8 & 0.6 & 0 \\
				4.8 & 4 & 1 & 0 \\
				0.6 & 1 & 1 & 0\\
				0 & 0 & 0 & 0
			\end{bmatrix}
		}
	\item \textbf{Scenario 2}: everything as in Scenario 1, but $n=60$ and $n_i=10$.
\end{itemize}
\textbf{Competitors}:
\begin{itemize}
	\item \textbf{ALASSO:} 2 stage: A-LASSO+Newton and A-LASSO+PCO
	\item \textbf{M-ALASSO:} Adaptive LASSO + EM Algorithm
	\item \textbf{SCAD-P:} SCAD + Proxy Matrix for $\Gamma$
	\item \textbf{rPQL:} Quasi-Likelihood + Adaptive LASSO (for GLMMs)
\end{itemize}
\end{frame}

\begin{frame}{Performance in Comparison to Other Algorithms}
\begin{table}[H]
\begin{center}
\begin{tabular}{|l|l|c|c|c|c|c|}
\hline
Setup & Algoritm & \% C & \% CF & \% CR & MSE & TIME \\
\hline 
\hline
$n = 30$, $n_i = 5$ & \textbf{\ouralgo} & 58 & 72 & 78 & 0.66 & 0.015 \\
& rPQL & 88 & 98 & 88 & 0.88 & 26-59 \\ 
& M-ALASSO & 71 & 73 & 79 & - & -\\
& ALASSO & 79 & 81 & 96 & - & - \\
& SCAD-P & - & 90 & 86 & - & - \\
\hline 
$n = 60$, $n_i = 10$ & \textbf{\ouralgo} & 98 & 100  & 98 & 0.69& 0.018 \\
& rPQL & 98 & 99 & 98 & 0.97 &  26-59 \\ 
& M-ALASSO & 83 & 83 & 89 & - & - \\
& ALASSO & 95 & 96 & 99 & - & -\\
& SCAD-P & 100 & 100 & 100 & - & - \\
\hline
	
\end{tabular}
\end{center}
\caption{\label{table:krishna_setup_results} Comparison of feature selection algorithms. \% CF -- percent of models where true fixed effects were identified correctly, \% CR -- percent of models where true random effects were identified correctly, \% C -- both fixed and random effects were identified correctly.}	
\end{table}
	
\end{frame}

\begin{frame}{Scalability Experiment}
\begin{itemize}
	\item $n = 60$, $n_i = 10$
	\item $p = q \in [4, 7, 10, \dots, 90]$, 200 experiments for each.
	\item $X_i = Z_i$, columns are drawn form $\NN(0, \Psi)$ where
	\[
		\Psi = \begin{bmatrix}
			9 & 4.8 & 0.6 \\
			4.8 & 4 & 1 \\
			0.6 & 1 & 1 \\
		\end{bmatrix}
	\]
	\item 50\% random coordinate in $\beta$ are active
	\item 70\% of those are also active in $\gamma$
\end{itemize}
\end{frame}

\begin{frame}{Scalability Experiment}
		\begin{center}
			\includegraphics<+>[width=0.8\textwidth]{Figures/scalability_accuracy}
			%\includegraphics<+>[width=0.8\textwidth]{Figures/scalability_mse}
		\end{center}
\end{frame}

\subsection{Application to Real-World Problems}

\begin{frame}{Contact Rate Modeling for COVID-19 Forecasting}
	\begin{itemize}
		\item $n = 60$ groups (countries and US states), $n_i \approx 50$
		\item $Y_i$ -- contact rate for COVID SEIIR Model 
		\item $p=q=5$ covariates related to temperature, mobility, population, testing; plus intercept.
	\end{itemize}
\end{frame}

\begin{frame}{Contact Rate Modeling for COVID-19 Forecasting}
	\begin{center}
		\includegraphics<+>[width=\textwidth]{Figures/fit_Alaska}
		\includegraphics<+>[width=\textwidth]{Figures/fit_Slovenia}
		\includegraphics<+>[width=\textwidth]{Figures/fit_Switzerland}
		\includegraphics<+>[width=\textwidth]{Figures/fit_Turkey}
	\end{center}
\end{frame}

\begin{frame}{Burden of Anxiety and Depression as Result of Bullying}
	\begin{itemize}
		\item $m = 10$ cohort studies, $n = 77$, highly unbalanced 
		\item $p = 13$, $q = 12$ (\texttt{time} was preselected fixed-only)
		\item Covariates are related to studies' designs.
	\end{itemize}
\end{frame}

\begin{frame}{Burden of Anxiety and Depression as Result of Bullying}
	\begin{center}
		\includegraphics[width=\textwidth]{Figures/bullying_data.csv_inclusion}
		%\includegraphics<+>[width=\textwidth]{Figures/bullying_data.csv_fixed_feature_selection}
		%\includegraphics<+>[width=\textwidth]{Figures/bullying_data.csv_random_feature_selection}

	\end{center}
\end{frame}
\section{Future Work}
\begin{frame}{Future Work: Theory}
\begin{theorem}[Conditions for Convergence to True Estimator]
	Under certain conditions the method converges in a finite number of iterations to $(\hat\beta, \hat\gamma)$ which projections $(\tbeta, \tgamma)$ belong to a $k$- and $j$-subspaces respectively that contain the true minimum $(\beta^*,\gamma^*)$.
\end{theorem}
\begin{theorem}[Consistency of Estimator]
	There exists a local minimizer $(\hat{\beta}, \hat{\gamma})$ for the proposed loss function, such that it is asymptotically consistent with true minimum $(\beta^*, \gamma^*)$. 
\end{theorem}
\begin{theorem}[Consistency in Zeros]
	If some coordinates of the true minimizer ($\beta^*, \gamma^*$) are zero, then it is also zero in $(\hat{\beta}, \hat{\gamma})$, given that the later is sufficiently close to the former.
\end{theorem}
\begin{theorem}[Asymptotic Normality]
	The proposed estimator $(\hat{\beta}, \hat{\gamma})$ asymptotically normally distributed around true minimizer ($\beta^*, \gamma^*$) in its true non-zero $k+j$-subspace.
\end{theorem}	
\end{frame}

\begin{frame}{Future work: Algorithm}
	\textbf{Question:} Will exponential smoothing of projection improve the accuracy? 
	{\small
	\begin{algorithm}[H]
	$\lambda_\beta = 0$; $\lambda_\gamma = 0$ \\
	\Repeat{$\tbeta \approx \beta, \tgamma \approx \gamma$}{
	$\lambda_\beta \leftarrow 2(1 + \lambda_\beta)$ \\
	$\lambda_\gamma \leftarrow 2(1 + \lambda_\gamma)$\\
	\Repeat{converges}{
		$\tbeta^{(k+1)} \leftarrow {\color{red}\delta}\proj_{\|\beta\|_0 \leq k}(\beta^{(k)}) + {\color{red}(1-\delta)\tbeta^{(k)}}$\\
		$\tgamma^{(k+1)} \leftarrow {\color{red}\delta}\proj_{\|\gamma\|_0 \leq s}(\gamma^{(k)}) + {\color{red}(1-\delta)\tgamma^{(k)}}$\\
		$\beta^{(k+1)}, \gamma^{(k+1)} \leftarrow \argmin_{\gamma \geq 0, \beta}\LL(\beta, \gamma) + \frac{\lambda_\beta}{2}\|\beta - \tbeta^{(k)}\|^2_2 + \frac{\lambda_\gamma}{2}\|\gamma - \tgamma^{(k)}\|_2^2$
	}
	}
	\BlankLine
	\end{algorithm}
	}
	\end{frame}
	
\begin{frame}{Future Work: Implementation}
		Can we increase $\lambda_\beta$ and $\lambda_\gamma$ in a more careful way to avoid potential stacking? 
		The approach can be based on the theorem:
		\begin{theorem}[Distance Between Minima]
	For a fixed dataset $(X_i, Y_i)$ and relaxation parameters $\lambda_\beta$, $\lambda_\gamma$ the distance between $(\beta^*, \gamma^*)$, the unconstrained minimizers of relaxed problem, and their projections $(\tbeta^*, \tgamma^*)$ is bounded by a constant $M$ depending on $(X_i, Y_i)$ and the relaxation parameters.
		\end{theorem}
\end{frame}

\begin{frame}{The End}
	\begin{center}
		\large Thank you for your attention!
	\end{center}
\end{frame}


\begin{frame}[allowframebreaks]{References}

  \bibliography{bibliography}
  \bibliographystyle{alpha}

\end{frame}

\end{document}