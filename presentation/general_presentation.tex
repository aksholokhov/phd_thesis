\documentclass[8pt]{beamer}
\input{useful_packages}
\input{math_macros}


% \usetheme[progressbar=frametitle]{metropolis}
\usepackage{appendixnumberbeamer}


% PRESETS OF PACKAGES AND MATH %

\usepackage{xspace}
\usepackage{caption}
\usepackage{subcaption}
\newcommand{\themename}{\textbf{\textsc{metropolis}}\xspace}
\newcommand{\ouralgo}{\ensuremath{\mathcal{MSR}3}}
\title{A Relaxation Approach to Feature Selection for Mixed-Effects Models}
\date{\today}
\author{Aleksei Sholokhov, James V. Burke, Peng Zheng, and Aleksandr Aravkin}
% \titlegraphic{\hfill\includegraphics[height=1.5cm]{logo.pdf}}

\begin{document}

\maketitle

\section{Relaxed for Linear Mixed-Effect Models}
\subsection{Linear Mixed-Effects Models}
\begin{frame}{Linear Mixed-Effect Models}

Linear Mixed-Effect (LME) models are often used for analyzing combined data across a range of groups. \newline

They use use covariates to separate the population variability (fixed-effects) from the group variability (random effects). \newline

LMEs borrow strength across groups to estimate key statistics in cases when data within units are sparse or highly variable. \newline

\end{frame}

\begin{frame}{Linear Mixed-Effect Models}

	Dataset: $m$ groups $(X_i, Z_i, y_i),\quad i = 1, \dots m$, each has $n_i$ observations
	\begin{itemize}
		\item 	$X_i \in \R^{n_i \times p}$ -- group $i$ design matrix for fixed features
		\item 	$Z_i \in \R^{n_i \times q}$ -- group $i$ design matrix for random effects
		\item 	$y_i \in \R^{n_i}$ -- group $i$ observations  
	\end{itemize}
	

	\begin{columns}[T,onlytextwidth]
    \column{0.5\textwidth}
	 \centering Standard Linear Regression:
   	\begin{figure}
   		\includegraphics[width=0.9\textwidth]{Figures/lme_example_mean_prediction}
   	\end{figure}
   	\[
   		y = X\beta + \varepsilon, \quad \varepsilon \sim \NN(0, \Lambda) 
   	\]

   	
    \column{0.5\textwidth}
    	\centering  Linear Mixed-Effect Model:
   	\begin{figure}
   		\includegraphics[width=0.9\textwidth]{Figures/lme_example_random_prediction}
   	\end{figure}
   	   		\[
   		\begin{split}
   			y_i & = X_i\beta + {\color{red}Z_i u_i} + \varepsilon_i, \quad \varepsilon_i \sim \NN(0, \Lambda_i) \\
   			{\color{red} u_i} & {\color{red}\sim \NN(0, \Gamma4)}
   		\end{split}
   		\]

   	
  \end{columns}
\end{frame}

\begin{frame}{Notation}
	\eq{
   		y_i & = X_i\beta + Z_iu_i + \varepsilon_i \quad i = 1\dots m \\
   		\varepsilon_i & \sim \NN(0, \Lambda_i) \\
   		u_i & \sim \NN(0, \Gamma)
   	}   	
   	
   	\begin{itemize}
   		\item $p$ -- number of fixed features, $q$ -- number of random effects.
   		\item $\beta \in \R^p$ -- fixed effects, or mean effects
   		\item $u_i \in \R^q$ -- random effects
   		\item $\Gamma \in \R^{q \times q}$ -- covariance matrix of random effects, often $\Gamma = \diag{(\gamma)}$
   		\item $\varepsilon_i \in \R^{n_i}$ -- observation noise
   		\item $\Lambda_i \in R^{n_i \times n_i}$ -- covariance matrix for noise
   	\end{itemize}
   	Unknowns: $\beta$, $u_i$, $\gamma$, sometimes $\Lambda_i$.
\end{frame}

\begin{frame}{Likelihood for Mixed Models}
Negative log-likelihood:
\eq{
	\label{eq:lmm_objective}
	\mathcal{L}(\beta, \gamma) & = \sum_{i = 1}^m \half(y_i - X_i\beta)^T(Z_i\Gamma Z_i^T + \Lambda_i)^{-1}(y_i - X_i\beta) + \\ & + \half\log{\det{\pa{Z_i \Gamma Z_i^T + \Lambda_i}}}, \quad \Gamma = \diag{(\gamma)}
	}
Maximum likelihood estimates for $\beta$ and $\gamma$ solve the problem:
\eq{
	\mathcal{LME} \quad \min_{\beta \in \R^p,\, \gamma \in \R^{q}_+} \LL(\beta, \gamma)
}

To select covariates we add a sparsity-promoting regularizer $R(\beta, \gamma)$
\eq{
	\mathcal{FS-LME} \quad \min_{\beta \in \R^p,\, \gamma \in \R^{q}_+} \LL(\beta, \gamma) + R(\beta, \gamma)
}

\begin{itemize}
	\item $\LL(\beta, \gamma)$ is smooth on its domain, quadratic w.r.t. $\beta$ and $\bar\eta$-weakly-convex w.r.t. $\gamma$.
	\item $R(\beta, \gamma)$ is closed, proper, convex, with easily computed \textit{prox operator}
\end{itemize}

\end{frame}

\begin{frame}{Regularization}
\begin{itemize}
	\item $R(\beta, \gamma)$ is closed, proper, convex, with easily computed \textit{prox operator}
\end{itemize}

\eq{
	\prox_{\alpha R + \delta_{\CC}}(\tbeta, \tgamma) & := \argmin_{(\beta, \gamma) \in \CC} R(\beta, \gamma) + \frac{1}{2\alpha}\|(\beta, \gamma) - (\tbeta, \tgamma)\|_2^2, \\ & \text{ where } \CC := \R^p \times R^q_+ \\
}

Examples: 
\begin{itemize}
	\item $R(x) = \lambda\sum_{j=1}^p w_j\|x_j\|_1$ -- LASSO and Adaptive LASSO penalties \cite{Bondell2010,Lin2013}
	\item $R(x) = \lambda \|x\|_0$ -- $\ell_0$ penalty \cite{Vaida2005,Jones2011}
	\item $R(x)$ -- SCAD penalty (\cite{Fan2001,Fan2012})
\end{itemize}

\begin{figure}
     \centering
     \begin{subfigure}[b]{0.25\textwidth}
         \centering
         \includegraphics[width=\textwidth]{Figures/l0_regularizer.pdf}
         \caption{$\ell_0$}
     \end{subfigure}%
     \begin{subfigure}[b]{0.25\textwidth}
         \centering
         \includegraphics[width=\textwidth]{Figures/l1_regularizer.pdf}
         \caption{$\ell_1$}
     \end{subfigure}%
     \begin{subfigure}[b]{0.25\textwidth}
         \centering
         \includegraphics[width=\textwidth]{Figures/lh_regularizer.pdf}
         \caption{$\ell_p,\, p=1/2$}
     \end{subfigure}%
     \begin{subfigure}[b]{0.25\textwidth}
         \centering
         \includegraphics[width=\textwidth]{Figures/scad_regularizer.pdf}
         \caption{SCAD}
     \end{subfigure}
     \caption{Three simple graphs}
     \label{fig:three graphs}
\end{figure}

\end{frame}

\begin{frame}{SR3-Relaxation for Mixed-Effect Models}
Original problem $\mathcal{FS-LME}$:
\eq{
	\min_{\beta \in \R^p,\, \gamma \in \R^{q}_+} \LL(\beta, \gamma) + R(\beta, \gamma)
}
Relaxed problem $\ouralgo$:
\eq{
	\label{eq:msr3_formulation_explicit}
	\min_{\beta, \tbeta \in \R^p,\, \gamma, \tgamma \in \R^{q}_+} \LL(\beta, \gamma) + \phi_\mu(\gamma) + \kappa_\eta(\beta - \tbeta, \gamma - \tgamma) + R(\tbeta, \tgamma)
}
where the \textit{relaxation} $\kappa_\eta$ decouples the likelihood and the regularizer 
\eq{
	\kappa_\eta(\beta - \tbeta, \gamma - \tgamma) := \frac{\eta}{2}\|\beta - \tbeta\|^2_2 + \frac{\eta + \bar\eta}{2}\|\gamma - \tgamma\|_2^2
}
and the \textit{projection function} $\phi_\mu$ replaces $\gamma \geq 0$ with a log-barrier 
\eq{
	\phi_\mu(\gamma) := \begin{cases}
		-\mu\sum_{i=1}^q \ln(\gamma_i/\mu), & \mu > 0 \\
		\delta_{\R_+^q}(\gamma), & \mu = 0 \\
		+\infty, & \mu < 0
	\end{cases}
} 
\end{frame}

\begin{frame}{Value Function Reformulation}
$\ouralgo$-relaxation replaces the original likelihood $\LL$ with a \textit{value function} $u_{\eta,\mu}$:
\eq{
	\label{eq:value_function_definition}
	u_{\eta,\mu}(\tbeta,\tgamma) & := \min_{(\beta, \gamma)} \LL_{\eta,\mu}((\beta, \gamma),(\tbeta, \tgamma)) \\  & := \min_{(\beta, \gamma)} \LL(\beta, \gamma) + \phi_\mu(\gamma) + \kappa_\eta(\beta - \tbeta, \gamma - \tgamma)
}
so $\ouralgo$-formulation~(\ref{eq:msr3_formulation_explicit}) becomes
\eq{
	\min_{(\tbeta, \tgamma) \in \CC} u_{\eta,\mu}(\tbeta, \tgamma) + R(\tbeta, \tgamma)
}
When $\bar\eta$ is larger than the weak-convexity constant
\begin{itemize}
	\item $u_{\eta,\mu}$ is well-defined and continuously differentiable.
	\item Solutions $(\tbeta^*, \tgamma^*)$ for $\ouralgo$ converge to solutions $(\beta^*, \gamma^*)$ of $\mathcal{FS-LME}$ \\ when $\mu \rightarrow 0$ and $\eta \rightarrow \infty$.
\end{itemize}

\textbf{Key observation}: in practice, we don't need accurate solutions for~(\ref{eq:value_function_definition}): a few Newton iterations keep the solution close to the central path.
\end{frame}

\begin{frame}{Value Function Reformulation}
\begin{figure}
	\includegraphics[width=0.7\textwidth]{Figures/intuition_current.pdf}
	\caption{\label{fig:intuition_sr3} Comparison of the level-sets for the original likelihood (left) and $\ouralgo$-likelihood (right), for fixed (top) and random (bottom) effects.}
\end{figure}
\end{frame}

\begin{frame}{Designing an Algorithm}
	Gradient of a Lagrangian:
	\eq{
		G_{\nu,\eta}((\beta,\gamma,v),(\tbeta,\tgamma)) := \begin{bmatrix}
			\nabla_\beta \LL(\beta, \gamma) + \eta(\beta-\tbeta) \\
			\nabla_\gamma \LL(\beta, \gamma) + (\bar\eta + \eta)(\gamma-\tgamma) - v\\
			v \bigodot \gamma - \mu\textbf{1}
		\end{bmatrix}
	}
	
\textbf{Lemma:} For every $(\mu,\eta) \in \R_+\times\R_{++}$,
\eq{
	&(\hat\beta,\hat\gamma) = \argmin_{(\beta,\gamma)}\LL_{\eta,\mu}((\beta, \gamma),(\tbeta, \tgamma)) \\
	& \iff \\
	& \exists \hat{v} \in \R_{+}^q \text{ s.t. } G_{\nu,\eta}((\beta,\gamma,\hat{v}),(\tbeta,\tgamma)) = 0
}
If $\mu > 0$, then $\hat{v} = -\nabla\phi_\mu(\hat{\gamma})$, and if $\mu = 0$, then $\hat{v}$ is the unique KKT multiplier associated with the constraint $0 \leq \gamma$.
\end{frame}

\begin{frame}{$\ouralgo$-fast Algorithm}
\input{../extras/08Pseudocode}
\end{frame}

\section{Experiments}
\subsection{Application to Synthetic Problems}

\begin{frame}{Application to Synthetic Problems}
\textbf{The Experiment} 
\begin{itemize}
	\item The number of fixed effects $p$ and random effects $q$ is 20.
	\item $\beta = \gamma = \frac{1}{2}[1,2,3,\dots,10, 0\dots,0]$
	\item 9 groups with sizes [10, 15, 4, 8, 3, 5, 18, 9, 6]
	\item $X_i \sim \NN(0, I)^p$, $Z_i = X_i$, $\varepsilon_i \sim \NN(0, 0.3^2I)$
	\item Each experiment is repeated 100 times.
	\item Grid-search for $\eta \in [10^{-4}, 10^{2}]$, golden search for $\lambda \in [0, 10^5]$
	\item Final model is chosen to maximize BIC
\end{itemize}
\begin{table}
	\input{../extras/performance_table_short_current}
\end{table}
\end{frame}

\begin{frame}{Application to Synthetic Problems}
\begin{figure}
	\includegraphics[width=\textwidth]{Figures/performance_picture_current.pdf}
\end{figure}
Benefits:
\begin{itemize}
	\item $\ouralgo$-relaxation has similar (and sometimes better!) feature selection performance than the original likelihood.
	\item $\ouralgo$-fast optimization accelerates the compute time by $\sim 10^2$.
\end{itemize}
Setbacks:
   \begin{itemize}
   	\item $\eta$ is a new hyperparameter to tune.
   \end{itemize}
\end{frame}

\begin{frame}{$\ell_0$-based Covariate Selection for Bullying Study from GBD}
	\begin{figure}
		\includegraphics[width=\textwidth]{Figures/bullying_data_assessment_selection.pdf}
		\caption{Fixed and random covariate selection for Bullying dataset from~\cite{GBD}. The model selected 9 covariates, 7 of which were historically significant, and did not select 4 covariates, 1 of which was historically significant.}
	\end{figure}
\end{frame}

\begin{frame}{Thank You!}
	The code is available on GitHub: \\
	\href{github.com/aksholokhov/pysr3}{https://github.com/aksholokhov/pysr3}
	\begin{itemize}
		\item All estimators are fully compatible to \texttt{sklearn} library.
		\item Implements SR3 for linear, generalized-linear, and linear mixed-effect models.
		\item Has tutorials, tests, and documentation.
	\end{itemize}
	
	\textbf{References}: 
	\bibliographystyle{plain}
	\bibliography{bibliography.bib}
\end{frame}

\end{document}