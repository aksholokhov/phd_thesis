
@article{Zheng2018RelaxAndSplit,
abstract = {We develop and analyze a new `relax-and-split' (RS) approach for compositions of separable nonconvex nonsmooth functions with linear maps. RS uses a relaxation technique together with partial minimization, and brings classic techniques including direct factorization, matrix decompositions, and fast iterative methods to bear on nonsmooth nonconvex problems. We also extend the approach to trimmed nonconvex-composite formulations; the resulting Trimmed RS (TRS) can fit models while detecting outliers in the data. We then test RS and TRS on a diverse set of applications: (1) phase retrieval, (2) stochastic shortest path problems, (3) semi-supervised classification, and (4) new clustering approaches. RS/TRS can be applied to models with very weak functional assumptions, are easy to implement, competitive with existing methods, and enable a new level of modeling formulations to be put forward to address emerging challenges in the mathematical sciences.},
archivePrefix = {arXiv},
arxivId = {1802.02654},
author = {Zheng, Peng and Aravkin, Aleksandr},
eprint = {1802.02654},
file = {:Users/aksholokhov/Documents/Papers/Zheng, Aravkin/2018/Unknown/Zheng, Aravkin - 2018 - Relax-and-split method for nonsmooth nonconvex problems.pdf:pdf},
month = {feb},
pages = {1--38},
title = {{Relax-and-split method for nonsmooth nonconvex problems}},
url = {http://arxiv.org/abs/1802.02654},
volume = {1},
year = {2018}
}

@article{Kucukelbir2017,
abstract = {Derivatives, mostly in the form of gradients and Hessians, are ubiquitous in machine learning. Automatic differentiation (AD), also called algorithmic differentiation or simply "autodiff", is a family of techniques similar to but more general than backpropagation for efficiently and accurately evaluating derivatives of numeric functions expressed as computer programs. AD is a small but established field with applications in areas including computational fluid dynamics, atmospheric sciences, and engineering design optimization. Until very recently, the fields of machine learning and AD have largely been unaware of each other and, in some cases, have independently discovered each other's results. Despite its relevance, general-purpose AD has been missing from the machine learning toolbox, a situation slowly changing with its ongoing adoption under the names "dynamic computational graphs" and "differentiable programming". We survey the intersection of AD and machine learning, cover applications where AD has direct relevance, and address the main implementation techniques. By precisely defining the main differentiation techniques and their interrelationships, we aim to bring clarity to the usage of the terms "autodiff", "automatic differentiation", and "symbolic differentiation" as these are encountered more and more in machine learning settings.},
archivePrefix = {arXiv},
arxivId = {1502.05767},
author = {Kucukelbir, Alp and Tran, Dustin and Ranganath, Rajesh and Gelman, Andrew and Blei, David M.},
doi = {10.1016/j.advwatres.2018.01.009},
eprint = {1502.05767},
file = {:Users/aksh/Downloads/17-468.pdf:pdf},
isbn = {0002-9645 (Print)$\backslash$r0002-9645 (Linking)},
issn = {03091708},
journal = {Journal of Machine Learning},
mendeley-groups = {Algorithmic Differentiation},
number = {14},
pages = {1--45},
pmid = {7020497},
title = {{Automatic Differentiation in Machine Learning: a Survey Atılım}},
url = {http://jmlr.org/papers/v18/16-107.html},
volume = {18},
year = {2017}
}

@book{Shalev-Shwartz2014,
abstract = {Machine learning is one of the fastest growing areas of computer science, with far-reaching applications. The aim of this textbook is to introduce machine learning, and the algorithmic paradigms it offers, in a principled way. The book provides an extensive theoretical account of the fundamental ideas underlying machine learning and the mathematical derivations that transform these principles into practical algorithms. Following a presentation of the basics of the field, the book covers a wide array of central topics that have not been addressed by previous textbooks. These include a discussion of the computational complexity of learning and the concepts of convexity and stability; important algorithmic paradigms including stochastic gradient descent, neural networks, and structured output learning; and emerging theoretical concepts such as the PAC-Bayes approach and compression-based bounds. Designed for an advanced undergraduate or beginning graduate course, the text makes the fundamentals and algorithms of machine learning accessible to students and non-expert readers in statistics, computer science, mathematics, and engineering.},
address = {Cambridge},
author = {Shalev-Shwartz, Shai and Ben-David, Shai},
booktitle = {Understanding Machine Learning: From Theory to Algorithms},
doi = {10.1017/CBO9781107298019},
file = {:Users/aksh/Documents/Books/Машинное обучение и анализ данных /Про все и сразу/understanding-machine-learning-theory-algorithms.pdf:pdf},
isbn = {9781107298019},
mendeley-groups = {Essential Books},
pages = {1--397},
publisher = {Cambridge University Press},
title = {{Understanding Machine Learning}},
url = {http://ebooks.cambridge.org/ref/id/CBO9781107298019},
volume = {9781107057},
year = {2014}
}

@book{Hastie2017,
abstract = {During the past decade there has been an explosion in computation and information technology. With it has come a vast amount of data in a variety of fields such as medicine, biology, finance, and marketing. The challenge of understanding these data has led to the development of new tools in the field of statistics, and spawned new areas such as data mining, machine learning, and bioinformatics. Many of these tools have common underpinnings but are often expressed with different terminology. This book describes the important ideas in these areas in a common conceptual framework. While the approach is statistical, the emphasis is on concepts rather than mathematics.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Hastie, Trevor and Tibshirani, Robert},
booktitle = {Math. Intell.},
doi = {111},
eprint = {arXiv:1011.1669v3},
isbn = {978-0-387-84857-0},
issn = {03436993},
mendeley-groups = {Essential Books},
pmid = {15512507},
title = {{The Elements of Statistical Learning Second Edition}},
year = {2017}
}

@article{Moody1992,
author = {Moody, John E},
mendeley-groups = {Other},
number = {1},
pages = {847--854},
title = {{The Effective Number of Parameters : An Analysis of Generalization and Regularization in Nonlinear Learning Systems}}
}

@incollection{Tipping2004,
author = {Tipping, Michael E.},
doi = {10.1007/978-3-540-28650-9_3},
file = {:Users/aksh/Downloads/bayesian{\_}regression{\_}overview.pdf:pdf},
mendeley-groups = {Bayesian Approach},
pages = {41--62},
title = {{Bayesian Inference: An Introduction to Principles and Practice in Machine Learning}},
url = {http://link.springer.com/10.1007/978-3-540-28650-9{\_}3},
year = {2004}
}

@article{Fox2006,
abstract = {Statistics lectures have been a source of much bewilderment and frustration for generations of students. This book attempts to remedy the situation by expounding a logical and unified approach to the whole subject of data analysis. This text is intended as a tutorial guide for senior undergraduates and research students in science and engineering. After explaining the basic principles of Bayesian probability theory, their use is illustrated with a variety of examples ranging from elementary parameter estimation to image processing. Other topics covered include reliability analysis, multivariate optimization, least-squares and maximum likelihood, error-propagation, hypothesis testing, maximum entropy and experimental design. The Second Edition of this successful tutorial book contains a new chapter on extensions to the ubiquitous least-squares procedure, allowing for the straightforward handling of outliers and unknown correlated noise, and a cutting-edge contribution from John Skilling on a novel numerical technique for Bayesian computation called 'nested sampling'.},
author = {Fox, Eric P. and Sivia, D. S.},
doi = {10.2307/1270652},
issn = {00401706},
journal = {Technometrics},
mendeley-groups = {Bayesian Approach},
title = {{Data Analysis: A Bayesian Tutorial}},
year = {2006}
}

@article{Zheng2019TrimmedMixedModels,
author = {Zheng, Peng and Aravkin, Aleksandr and Barber, Ryan and Sorensen, Reed and Murray, Christopher},
file = {:Users/aksh/Documents/Papers/Zheng et al/2019/Unknown/Zheng et al. - 2019 - Trimmed Constrained Mixed Effects Models Formulations and Algorithms.pdf:pdf},
keywords = {meta-analysis,mixed effects models,nonsmooth nonconvex optimization,trimming},
mendeley-groups = {Mixed Effect Models},
pages = {1--15},
title = {{Trimmed Constrained Mixed Effects Models : Formulations and Algorithms}},
year = {2019}
}

@article{Mosegaard1995SamplingFromPosterior,
abstract = {Probabilistic formulation of inverse problems leads to the definition of a probability distribution in the model space. This probability distribution combines a priori information with new information obtained by measuring some observable parameters (data). As, in the general case, the theory linking data with model parameters is nonlinear, the a posteriori probability in the model space may not be easy to describe (it may be multimodal, some moments may not be defined, etc.). When analysing an inverse problem, obtaining a maximum likelihood model is usually not sufficient, as we normally also wish to have information on the resolution power of the data. In the general case we may have a large number of model parameters, and an inspection of the marginal probability densities of interest may be impractical, or even useless. But it is possible to pseudorandomly generate a large collection of models according to the posterior probability distribution and to analyse and display the models in such a way that information on the relative likelihoods of model properties is conveyed to the spectator. This can be accomplished by means of an efficient Monte Carlo method, even in cases where no explicit formula for the a priori distribution is available. The most well known importance sampling method, the Metropolis algorithm, can be generalized, and this gives a method that allows analysis of (possibly highly nonlinear) inverse problems with complex a priori information and data with an arbitrary noise distribution.},
author = {Mosegaard, Klaus and Tarantola, Albert},
doi = {10.1029/94jb03097},
file = {:Users/aksh/Documents/Papers/Mosegaard, Tarantola/1995/Journal of Geophysical Research Solid Earth/Mosegaard, Tarantola - 1995 - Monte Carlo sampling of solutions to inverse problems.pdf:pdf},
journal = {Journal of Geophysical Research: Solid Earth},
keywords = {doi:10.1029/9,http://dx.doi.org/10.1029/94JB03097},
mendeley-groups = {Bayesian Approach,Monte Carlo Methods},
number = {B7},
pages = {12431--12447},
title = {{Monte Carlo sampling of solutions to inverse problems}},
volume = {100},
year = {1995}
}

@article{Murray2017MeasuringGlobalHealth,
abstract = {The 2014-2015 outbreak of Ebola virus (EBOV), originating from Guinea, is now responsible for the infection of {\textgreater} 20,000 people in 9 countries. Whereas past filovirus outbreaks in sub-Saharan Africa have been rapidly brought under control with comparably few cases, this outbreak has been particularly resistant to containment efforts. Both the general population and primary health care workers have been affected by this outbreak, with hundreds of doctors and nurses being infected in the line of duty. In the absence of approved therapeutics, several caregivers have turned to investigational new drugs as well as experimental therapies in an effort to save lives. This review aims to summarize the candidates currently under consideration for postexposure use in infected patients during the largest EBOV outbreak in history.},
author = {Murray, Christopher J.L. and Lopez, Alan D.},
doi = {10.1016/S0140-6736(17)32367-X},
file = {:Users/aksh/Documents/Papers/Murray, Lopez/2017/The Lancet/Murray, Lopez - 2017 - Measuring global health motivation and evolution of the Global Burden of Disease Study.pdf:pdf},
issn = {1474547X},
journal = {The Lancet},
mendeley-groups = {Epidemiology Modelling},
number = {10100},
pages = {1460--1464},
publisher = {Elsevier Ltd},
title = {{Measuring global health: motivation and evolution of the Global Burden of Disease Study}},
url = {http://dx.doi.org/10.1016/S0140-6736(17)32367-X},
volume = {390},
year = {2017}
}

@article{Polson2019BayesianRegularization,
abstract = {Bayesian regularization is a central tool in modern-day statistical and machine learning methods. Many applications involve high-dimensional sparse signal recovery problems. The goal of our paper is to provide a review of the literature on penalty-based regularization approaches, from Tikhonov (Ridge, Lasso) to horseshoe regularization.},
archivePrefix = {arXiv},
arxivId = {arXiv:1902.06269v1},
author = {Polson, Nicholas G. and Sokolov, Vadim},
doi = {10.1002/wics.1463},
eprint = {arXiv:1902.06269v1},
file = {:Users/aksh/Documents/Papers/Polson, Sokolov/2019/Wiley Interdisciplinary Reviews Computational Statistics/Polson, Sokolov - 2019 - Bayesian regularization From Tikhonov to horseshoe.pdf:pdf},
issn = {19390068},
journal = {Wiley Interdisciplinary Reviews: Computational Statistics},
keywords = {Bayesian regression,horseshoe,lasso,regularization},
mendeley-groups = {Bayesian Approach},
number = {4},
pages = {1--16},
title = {{Bayesian regularization: From Tikhonov to horseshoe}},
volume = {11},
year = {2019}
}

@article{Kuczera1998AssessmentOfParametersWithMCMC,
abstract = {Two Monte Carlo-based approaches for assessing parameter uncertainty in complex hydrologic models are considered. The first, known as importance sampling, has been implemented in the generalised likelihood uncertainty estimation (GLUE) framework of Beven and Binley. The second, known as the Metropolis algorithm, differs from importance sampling in that it uses a random walk that adapts to the true probability distribution describing parameter uncertainty. Three case studies are used to investigate and illustrate these Monte Carlo approaches. The first considers a simple water balance model for which exact results are known. It is shown that importance sampling is inferior to Metropolis sampling. Unless a large number of random samples are drawn, importance sampling can produce seriously misleading results. The second and third case studies consider more complex catchment models to illustrate the insights the Metropolis algorithm can offer. They demonstrate assessment of parameter uncertainty in the presence of bimodality, evaluation of the significance of split-sample tests, use of prior information and the assessment of confidence limits on hydrologic responses not used in calibration. When compared with the capabilities of traditional inference based on first-order approximation, the Metropolis algorithm provides a quantum advance in our capability to deal with parameter uncertainty in hydrologic models.},
author = {Kuczera, George and Parent, Eric},
doi = {10.1016/S0022-1694(98)00198-X},
file = {:Users/aksh/Documents/Papers/Kuczera, Parent/1998/Journal of Hydrology/Kuczera, Parent - 1998 - Monte Carlo assessment of parameter uncertainty in conceptual catchment models The Metropolis algorithm.pdf:pdf},
issn = {00221694},
journal = {Journal of Hydrology},
keywords = {Bayesian inference,Conceptual catchment models,Importance sampling,Markov Chain Monte Carlo sampling,Parameter uncertainty,Rainfall-runoff models},
mendeley-groups = {Monte Carlo Methods},
number = {1-4},
pages = {69--85},
title = {{Monte Carlo assessment of parameter uncertainty in conceptual catchment models: The Metropolis algorithm}},
volume = {211},
year = {1998}
}

@article{Carlin1995BayesianModelChoice,
abstract = {Markov chain Monte Carlo (MCMC) integration methods enable the fitting of models of virtually unlimited complexity, and as such have revolutionized the practice of Bayesian data analysis. However, comparison across models may not proceed in a completely analogous fashion, owing to violations of the conditions sufficient to ensure convergence of the Markov chain. In this paper we present a framework for Bayesian model choice, along with an MCMC algorithm that does not suffer from convergence difficulties. Our algorithm applies equally well to problems where only one model is contemplated but its proper size is not known at the outset, such as problems involving integer-valued parameters, multiple changepoints or finite mixture distributions. We illustrate our approach with two published examples.},
author = {Carlin, Bradley P. and Chib, Siddhartha},
doi = {10.1111/j.2517-6161.1995.tb02042.x},
file = {:Users/aksh/Documents/Papers/Carlin, Chib/1995/Journal of the Royal Statistical Society Series B (Methodological)/Carlin, Chib - 1995 - Bayesian Model Choice Via Markov Chain Monte Carlo Methods.pdf:pdf},
journal = {Journal of the Royal Statistical Society: Series B (Methodological)},
keywords = {bayes factor,finite mixture model,gibbs sampler,integer-valued,models of varying size,multiple changepoint model,parameters},
mendeley-groups = {Monte Carlo Methods},
number = {3},
pages = {473--484},
title = {{Bayesian Model Choice Via Markov Chain Monte Carlo Methods}},
volume = {57},
year = {1995}
}

@misc{Abraham2015MetaregEpidemBook,
address = {Seattle},
author = {Abraham, D},
isbn = {9780295991849},
keywords = {Epidemiology -- Statistical methods,Meta-analysis,World health -- Statistical methods},
mendeley-groups = {Epidemiology Modelling},
publisher = {University of Washington Press},
series = {Publications on global health},
title = {{An integrative metaregression framework for descriptive epidemiology}},
year = {2015}
}

@book{Davison1998BootstrapMethods,
abstract = {Bootstrap methods are computer-intensive methods of statistical analysis, which use simulation to calculate standard errors, confidence intervals, and significance tests. The methods apply for any level of modelling, and so can be used for fully parametric, semiparametric, and completely nonparametric analysis. This 1997 book gives a broad and up-to-date coverage of bootstrap methods, with numerous applied examples, developed in a coherent way with the necessary theoretical basis. Applications include stratified data; finite populations; censored and missing data; linear, nonlinear, and smooth regression models; classification; time series and spatial problems. Special features of the book include: extensive discussion of significance tests and confidence intervals; material on various diagnostic methods; and methods for efficient computation, including improved Monte Carlo simulation. Each chapter includes both practical and theoretical exercises. S-Plus programs for implementing the methods described in the text are available from the supporting website.},
author = {Buckland, S. T. and Davison, A. C. and Hinkley, D. V.},
booktitle = {Biometrics},
doi = {10.2307/3109789},
issn = {0006341X},
mendeley-groups = {Bootstrap},
title = {{Bootstrap Methods and Their Application}},
year = {1998}
}



@article{lindstrom_newton-raphson_1988,
	title = {Newton-{Raphson} and {EM} {Algorithms} for {Linear} {Mixed}-{Effects} {Models} for {Repeated}-{Measures} {Data}},
	doi = {10.1080/01621459.1988.10478693},
	language = {en},
	author = {Lindstrom, Mary J and Bates, Douglas M},
	month = dec,
	year = {1988},
	pages = {10},
	file = {Lindstrom and Bates - Newton-Raphson and EM Algorithms for Linear Mixed-.pdf:/Users/aksh/Documents/Papers/storage/JSIEDAXD/Lindstrom and Bates - Newton-Raphson and EM Algorithms for Linear Mixed-.pdf:application/pdf}
}


@article{Harville1977ML,
	title = {Maximum {Likelihood} {Approaches} to {Variance} {Component} {Estimation} and to {Related} {Problems}},
	volume = {72},
	issn = {0162-1459, 1537-274X},
	url = {http://www.tandfonline.com/doi/abs/10.1080/01621459.1977.10480998},
	doi = {10/ggbzhj},
	language = {en},
	number = {358},
	urldate = {2019-10-29},
	journal = {Journal of the American Statistical Association},
	author = {Harville, David A.},
	month = jun,
	year = {1977},
	note = {ZSCC: 0002743},
	pages = {320--338},
	file = {Harville - 1977 - Maximum Likelihood Approaches to Variance Componen.pdf:/Users/aksh/Documents/Papers/storage/7LWJ8HF6/Harville - 1977 - Maximum Likelihood Approaches to Variance Componen.pdf:application/pdf}
}

@article{Jain2017,
abstract = {A vast majority of machine learning algorithms train their models and perform inference by solving optimization problems. In order to capture the learning and prediction problems accurately structural constraints such as sparsity or low rank are frequently imposed or else the objective itself is designed to be a non-convex function. This is especially true of algorithms that operate in high-dimensional spaces or that train non-linear models such as tensor models and deep networks. The freedom to express the learning problem as a non-convex optimization problem gives immense modeling power to the algorithm designer, but often such problems are NP-hard to solve. A popular workaround to this has been to relax non-convex problems to convex ones and use traditional methods to solve the (convex) relaxed optimization problems. However this approach may be lossy and nevertheless presents significant challenges for large scale optimization. On the other hand, direct approaches to non-convex optimization have met with resounding success in several domains and remain the methods of choice for the practitioner, as they frequently outperform relaxation-based techniques - popular heuristics include projected gradient descent and alternating minimization. However, these are often poorly understood in terms of their convergence and other properties. This monograph presents a selection of recent advances that bridge a long-standing gap in our understanding of these heuristics. We hope that an insight into the inner workings of these methods will allow the reader to appreciate the unique marriage of task structure and generative models that allow these heuristic techniques to (provably) succeed. The monograph will lead the reader through several widely used non-convex optimization techniques, as well as applications thereof. The goal of this monograph is to both, introduce the rich literature in this area, as well as equip the reader with the tools and techniques needed to analyze these simple procedures for non-convex problems.},
archivePrefix = {arXiv},
arxivId = {1712.07897},
author = {Jain, Prateek and Kar, Purushottam},
doi = {10.1561/2200000058},
eprint = {1712.07897},
file = {:Users/aksh/Downloads/1712.07897.pdf:pdf},
issn = {19358245},
journal = {Foundations and Trends in Machine Learning},
mendeley-groups = {Other},
number = {3-4},
pages = {142--336},
title = {{Non-convex optimization for machine learning}},
volume = {10},
year = {2017}
}

@article{Ueda2010,
abstract = {The regularized Newton method (RNM) is one of the efficient solution methods for the unconstrained convex optimization. It is well-known that the RNM has good convergence properties as compared to the steepest descent method and the pure Newton's method. For example, Li, Fukushima, Qi and Yamashita showed that the RNM has a quadratic rate of convergence under the local error bound condition. Recently, Polyak showed that the global complexity bound of the RNM, which is the first iteration k such that ∥ ∇ f(x k)∥ ≤ $\epsilon$, is O($\epsilon$ -4), where f is the objective function and $\epsilon$ is a given positive constant. In this paper, we consider a RNM extended to the unconstrained "nonconvex" optimization. We show that the extended RNM (E-RNM) has the following properties. (a) The E-RNM has a global convergence property under appropriate conditions. (b) The global complexity bound of the E-RNM is O($\epsilon$ -2) if ∇ 2 f is Lipschitz continuous on a certain compact set. (c) The E-RNM has a superlinear rate of convergence under the local error bound condition. {\textcopyright} 2009 Springer Science+Business Media, LLC.},
author = {Ueda, Kenji and Yamashita, Nobuo},
doi = {10.1007/s00245-009-9094-9},
file = {:Users/aksh/Downloads/Ueda-Yamashita2010{\_}Article{\_}ConvergencePropertiesOfTheRegu.pdf:pdf},
issn = {0095-4616},
journal = {Applied Mathematics and Optimization},
keywords = {Global complexity bound,Global convergence,Local error bound,Regularized Newton methods,Superlinear convergence},
mendeley-groups = {Second-Order Optimization Methods},
month = {aug},
number = {1},
pages = {27--46},
title = {{Convergence Properties of the Regularized Newton Method for the Unconstrained Nonconvex Optimization}},
url = {http://link.springer.com/10.1007/s00245-009-9094-9},
volume = {62},
year = {2010}
}

@article{Peng2012LMMSelectionOverview,
abstract = {Mixed effect models are fundamental tools for the analysis of longitudinal data, panel data and cross-sectional data. They are widely used by various fields of social sciences, medical and biological sciences. However, the complex nature of these models has made variable selection and parameter estimation a challenging problem. In this paper, we propose a simple iterative procedure that estimates and selects fixed and random effects for linear mixed models. In particular, we propose to utilize the partial consistency property of the random effect coefficients and select groups of random effects simultaneously via a data-oriented penalty function (the smoothly clipped absolute deviation penalty function). We show that the proposed method is a consistent variable selection procedure and possesses some oracle properties. Simulation studies and a real data analysis are also conducted to empirically examine the performance of this procedure. {\textcopyright} 2012 Elsevier Inc.},
author = {Peng, Heng and Lu, Ying},
doi = {10.1016/j.jmva.2012.02.005},
file = {:Users/aksh/Downloads/1-s2.0-S0047259X12000395-main.pdf:pdf},
issn = {0047259X},
journal = {Journal of Multivariate Analysis},
keywords = {Group selection,Model selection,Oracle property,Penalized least squares,SCAD function},
mendeley-groups = {Feature/Effects Selection},
pages = {109--129},
publisher = {Elsevier Inc.},
title = {{Model selection in linear mixed effect models}},
url = {http://dx.doi.org/10.1016/j.jmva.2012.02.005},
volume = {109},
year = {2012}
}

@article{Audenaert2010spectral,
  title={Spectral radius of Hadamard product versus conventional product for non-negative matrices},
  author={Audenaert, Koenraad MR},
  journal={Linear algebra and its applications},
  volume={432},
  number={1},
  pages={366--368},
  year={2010},
  publisher={Elsevier}
}

@article{Drnovvsek2016inequalities,
  title={Inequalities on the spectral radius and the operator norm of Hadamard products of positive operators on sequence spaces},
  author={Drnov{\v{s}}ek, Roman and Peperko, Aljo{\v{s}}a and others},
  journal={Banach Journal of Mathematical Analysis},
  volume={10},
  number={4},
  pages={800--814},
  year={2016},
  publisher={Tusi Mathematical Research Group}
}

@article{Vaida2005,
author = {Vaida, Florin and Blanchard, Suzette},
doi = {10.1093/biomet/92.2.351},
issn = {1464-3510},
journal = {Biometrika},
mendeley-groups = {AIC/BIC},
month = {jun},
number = {2},
pages = {351--370},
title = {{Conditional Akaike information for mixed-effects models}},
url = {http://academic.oup.com/biomet/article/92/2/351/233128/Conditional-Akaike-information-for-mixedeffects},
volume = {92},
year = {2005}
}
@article{Jones2011,
author = {Jones, Richard H.},
doi = {10.1002/sim.4323},
issn = {02776715},
journal = {Statistics in Medicine},
mendeley-groups = {AIC/BIC},
month = {nov},
number = {25},
pages = {3050--3056},
title = {{Bayesian information criterion for longitudinal and clustered data}},
url = {http://doi.wiley.com/10.1002/sim.4323},
volume = {30},
year = {2011}
}

@article{Bondell2010,
annote = {Krishna2008 thesis, published as a paper},
archivePrefix = {arXiv},
arxivId = {NIHMS150003},
author = {Bondell, Howard D. and Krishna, Arun and Ghosh, Sujit K.},
doi = {10.1111/j.1541-0420.2010.01391.x},
eprint = {NIHMS150003},
isbn = {6176321972},
issn = {0006341X},
journal = {Biometrics},
keywords = {epiblast,gfp fusion,histone h2b-,icm,lineage specification,live imaging,mouse blastocyst,pdgfr $\alpha$,primitive endoderm},
mendeley-groups = {Feature/Effects Selection},
month = {dec},
number = {4},
pages = {1069--1077},
pmid = {1000000221},
title = {{Joint Variable Selection for Fixed and Random Effects in Linear Mixed-Effects Models}},
url = {http://doi.wiley.com/10.1111/j.1541-0420.2010.01391.x},
volume = {66},
year = {2010}
}

@article{Lin2013,
annote = {2 stage selection: Newton method to select random effects first and then penalized log-likelihood to select fixed effects.
They do a local quadratic approximation of the first norm to make it differentiable and then apply NR.

Theory: exatctly(?) the same set of three statements as in Krishna 2008 (sqrt(n) consistency and so on), even the notation is the same. 

Synthetic examples: exactly the same setup as in Krishna, Ghosh (2010).},
author = {Lin, Bingqing and Pang, Zhen and Jiang, Jiming},
doi = {10.1080/10618600.2012.681219},
issn = {10618600},
journal = {Journal of Computational and Graphical Statistics},
keywords = {BIC,LASSO,Mixed-effects models},
mendeley-groups = {Feature/Effects Selection},
number = {2},
pages = {341--355},
title = {{Fixed and random effects selection by REML and pathwise coordinate optimization}},
volume = {22},
year = {2013}
}

@article{Fan2012,
author = {Fan, Yingying and Li, Runze},
doi = {10.1214/12-AOS1028},
issn = {0090-5364},
journal = {The Annals of Statistics},
keywords = {Adaptive Lasso,Group variable selection,Linear mixed effects models,Oracle property,SCAD},
mendeley-groups = {Feature/Effects Selection},
month = {aug},
number = {4},
pages = {2043--2068},
title = {{Variable selection in linear mixed effects models}},
url = {http://projecteuclid.org/euclid.aos/1351602536},
volume = {40},
year = {2012}
}

@article{Fan2001,
author = {Fan, Jianqing and Li, Runze},
doi = {10.1198/016214501753382273},
issn = {0162-1459},
journal = {Journal of the American Statistical Association},
keywords = {hard thresholding,lasso,nonnegative garrote,oracle estimator,penalized likelihood,scad,soft thresholding},
mendeley-groups = {Feature/Effects Selection},
month = {dec},
number = {456},
pages = {1348--1360},
title = {{Variable Selection via Nonconcave Penalized Likelihood and its Oracle Properties}},
url = {http://www.tandfonline.com/doi/abs/10.1198/016214501753382273},
volume = {96},
year = {2001}
}

@article{Zheng2019,
author = {Zheng, Peng and Askham, Travis and Brunton, Steven L. and Kutz, J. Nathan and Aravkin, Aleksandr Y.},
doi = {10.1109/ACCESS.2018.2886528},
issn = {2169-3536},
journal = {IEEE Access},
mendeley-groups = {Relax-and-Split examples},
pages = {1404--1423},
title = {{A Unified Framework for Sparse Relaxed Regularized Regression: SR3}},
url = {https://ieeexplore.ieee.org/document/8573778/},
volume = {7},
year = {2019}
}


@article{schelldorfer2011estimation,
  title={Estimation for high-dimensional linear mixed-effects models using l1-penalization},
  author={Schelldorfer, J{\"u}rg and B{\"u}hlmann, Peter and DE GEER, SARA VAN},
  journal={Scandinavian Journal of Statistics},
  volume={38},
  number={2},
  pages={197--214},
  year={2011},
  publisher={Wiley Online Library}
}

@article{schelldorfer2011estimation,
  title={Estimation for high-dimensional linear mixed-effects models using l1-penalization},
  author={Schelldorfer, J{\"u}rg and B{\"u}hlmann, Peter and DE GEER, SARA VAN},
  journal={Scandinavian Journal of Statistics},
  volume={38},
  number={2},
  pages={197--214},
  year={2011},
  publisher={Wiley Online Library}
}

@article{groll2014variable,
  title={Variable selection for generalized linear mixed models by L 1-penalized estimation},
  author={Groll, Andreas and Tutz, Gerhard},
  journal={Statistics and Computing},
  volume={24},
  number={2},
  pages={137--154},
  year={2014},
  publisher={Springer}
}
