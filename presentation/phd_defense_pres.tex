\documentclass[8pt]{beamer}
\input{useful_packages}
\input{math_macros}

% \usetheme[progressbar=frametitle]{metropolis}
\addtobeamertemplate{navigation symbols}{}{%
    \usebeamerfont{footline}%
    \usebeamercolor[fg]{footline}%
    \hspace{1em}%
    \insertframenumber/\inserttotalframenumber
}

\usepackage{appendixnumberbeamer}
\addbibresource{../bibliography.bib}

% PRESETS OF PACKAGES AND MATH %

\usepackage{xspace}
\usepackage{caption}
\usepackage{subcaption}

\newcommand{\themename}{\textbf{\textsc{metropolis}}\xspace}
\newcommand{\ouralgo}{\ensuremath{\mathcal{MSR}3}}
%\title{PhD Defense}
%\date{\today}
%\author{Aleksei Sholokhov}
%\titlegraphic{\includegraphics[width=0.5\textwidth]{Figures/UW_amath.pdf}\hspace{3em}\includegraphics[height=1cm]{Figures/IHME_Logo.jpg}}
\title{Optimization Methods for Parameter Identification \\ in Settings with Only Partial Knowledge}
\subtitle{Ph.D. Defense}
\date{\today}
\author{Aleksei Sholokhov}
\titlegraphic{\vspace{3em}\includegraphics[width=0.5\textwidth]{Figures/UW_amath.pdf}}

\AtBeginSection[]{
  \begin{frame}
  \vfill
  \centering
  \begin{beamercolorbox}[sep=8pt,center,shadow=true,rounded=true]{title}
    \usebeamerfont{title}\insertsectionhead\par%
  \end{beamercolorbox}
  \vfill
  \end{frame}
}

\begin{document}

\maketitle

\begin{frame}{Plan of the Defense}
\uncover<2->{
\includegraphics[height=35pt]{figures/sasha.png}
\includegraphics[height=35pt]{figures/jim.jpg}
\includegraphics[height=35pt]{figures/peng.jpg}
}
\uncover<3->{
\includegraphics[height=35pt]{figures/hassan.jpg}
\includegraphics[height=35pt]{figures/saleh.jpeg}
\includegraphics[height=35pt]{figures/yuying.jpeg}
}
\uncover<4->{
\includegraphics[height=35pt]{figures/nathan.jpg}
\includegraphics[height=35pt]{figures/steve.jpeg}
\includegraphics[height=35pt]{figures/josh.jpg}
}

\vspace{1em}
\uncover<2->{
\textbf{Sparse Relaxed Regularized Regression for Linear Mixed-Effect Models}\\
{\small
Together with Aleksandr Aravkin, James Burke, Damian Santomauro, and Peng Zheng
\begin{itemize}
	\item \only<1-4>{``A Relaxation Approach to Feature Selection for Linear Mixed Effects Models''}\only<5->{\underline{\textcolor{teal}{``A Relaxation Approach to Feature Selection for Linear Mixed Effects Models''}}} \\ \textit{submitted to Journal of Computational and Graphical Statistics (JCGS)}
	\item ``Analysis of Relaxation Methods for Feature Selection in Mixed Effects Models'' \\ \textit{submitted to Journal of Computational Optimization and Applications (COAP)}
	\item ``\texttt{pysr3}: A Python Package for Spare Relaxed Regularized Regression'' \\ \textit{Journal of Open-Source Software (JOSS), 2023, ICCOPT 2022, SIAM OPT 2023}
\end{itemize}
}
}

\vspace{1em}
\uncover<3->{
\textbf{PINODE: Physics-Informed Neural ODEs}\\
{\small
Together with Hassan Mansour, Saleh Nabi, and Yuying Liu
\begin{itemize}
	\item ``Physics-Informed Koopman Network'' \\
	\textit{SIAM ADS 2023, arXiv:2211.09419}
	\item \only<1-4>{``Physics-Informed Neural ODE: Embedding Physics into Models using Collocation Points''}\only<5->{\underline{\textcolor{teal}{``Physics-Informed Neural ODE: Embedding Physics into Models using Collocation Points''}}} \\
	\textit{Submitted to Nature Special Issue on Physics-Informed Machine Learning}
\end{itemize}
}
}
\uncover<4->{
\vspace{1em}
\textbf{Single-Pixel Imaging with Reduced-Order Models}\\
{\small
Together with J. Nathan Kutz, Steven Brunton, Joshua Rapp, Hassan Mansour, and Saleh Nabi
\begin{itemize}
	\item \only<1-4>{``Single pixel imaging of spatio-temporal flows using differentiable latent dynamics''}\only<5->{\underline{\textcolor{teal}{``Single pixel imaging of spatio-temporal flows using differentiable latent dynamics''}}} \textit{in preparation}
\end{itemize}
}
}


\end{frame}

\section{\texorpdfstring{$\mathcal{MSR}3$}{MSR3} -- Sparse Relaxed Regularized Regression for Linear Mixed-Effect Models}

\begin{frame}{Research Objectives and Plan of the Talk}
	
\textbf{Objectives:}
\begin{itemize}
	\item Extend $\mathcal{SR}3$ relaxation to linear mixed-effects models - $\mathcal{MSR}3$ (see \footcite{sholokhov2022relaxation}).
	\item Develop theoretical foundations for it (see \footcite{aravkin2022jimtheory}).
	\item Implement it as a \texttt{scikit-learn}-compatible Python package -- \texttt{pysr3} (see \footcite{sholokhov2023pysr3}).
\end{itemize}

\vspace{3em}

\textbf{Plan of the Talk:}
\begin{enumerate}
	\item Proximal Gradient Descent (PGD) for Linear Mixed-Effects Models (LMEs)
	\item $\mathcal{MSR}3$ --  PGD on $\mathcal{SR}3$-relaxation for LMEs
	\item $\mathcal{MSR}3$-fast -- practical algorithm for feature selection
\end{enumerate}
\end{frame}

\begin{frame}{Mixed-Effect Models}
Mixed-effect models
\begin{itemize}
	\item Used for analyzing \textbf{combined data} across a range of \textbf{groups}.
	\item \textbf{Borrow strength} across groups to estimate key statistics. % when data within units are sparse or highly variable.
	\item Use covariates to separate the \textbf{population variability} from the \textbf{group variability}.

\end{itemize}

\begin{figure}
	\centering
	\includegraphics[width=0.8\textwidth]{Figures/ihme_predictions.png}\footnote{Picture is taken from \href{covid19.healthdata.org}{covid19.healthdata.org} }
\end{figure}

\end{frame}


\begin{frame}{Linear Mixed-Effect (LME) Models}

	Dataset: $m$ groups $(X_i, Z_i, y_i),\quad i = 1, \dots m$, each has $n_i$ observations
	\begin{itemize}
		\item 	$X_i \in \R^{n_i \times p}$ -- group $i$ design matrix for fixed features
		\item 	$Z_i \in \R^{n_i \times q}$ -- group $i$ design matrix for random features
		\item 	$y_i \in \R^{n_i}$ -- group $i$ observations  
		\item   $u_i \in \R^q$ -- random effects
		\item   $\Gamma \in \R^{q \times q}$ -- covariance matrix of random effects, often $\Gamma = \diag{\gamma}$
		\item   $\Lambda_i \in R^{n_i \times n_i}$ -- covariance matrix for observation noise
	\end{itemize}
	

	\begin{columns}[T,onlytextwidth]
	
    \column{0.5\textwidth}
    \vspace{3em}
    Model:
	 	\[
   		\begin{split}
   			y_i & = X_i\beta + {\color{red}Z_i u_i} + \varepsilon_i \\
   			 \varepsilon_i & \sim \NN(0, \Lambda_i) \\
   			{\color{red} u_i} & {\color{red}\sim \NN(0, \Gamma)}
   		\end{split}
   		\]
   		   	
	Unknowns: $\beta$, $u_i$, $\gamma$, sometimes $\Lambda_i$.
	
    \column{0.5\textwidth}
    	\centering  
   	\begin{figure}
   		\includegraphics[width=0.9\textwidth]{Figures/lme_example_random_prediction}
   	\end{figure}
   	   		

   	
  \end{columns}
\end{frame}

%\begin{frame}{Notation}
%	\eq{
%   		y_i & = X_i\beta + Z_iu_i + \varepsilon_i \quad i = 1\dots m \\
%   		\varepsilon_i & \sim \NN(0, \Lambda_i) \\
%   		u_i & \sim \NN(0, \Gamma)
%   	}   	
%   	
%   	\begin{itemize}
%   		\item $p$ -- number of fixed features, $q$ -- number of random effects.
%   		\item $\beta \in \R^p$ -- fixed effects, or mean effects
%   		\item $u_i \in \R^q$ -- random effects
%   		\item $\Gamma \in \R^{q \times q}$ -- covariance matrix of random effects, often $\Gamma = \diag{(\gamma)}$
%   		\item $\varepsilon_i \in \R^{n_i}$ -- observation noise
%   		\item $\Lambda_i \in R^{n_i \times n_i}$ -- covariance matrix for noise
%   	\end{itemize}
%   	Unknowns: $\beta$, $u_i$, $\gamma$, sometimes $\Lambda_i$.
%\end{frame}

%\section{Sparse Relaxed Regularized Regression (SR3) for Linear Mixed-Effects Models}
%\subsection{ Mixed-Effects Models}


\begin{frame}{Negative Log-Likelihood for Mixed-Effect Models}
Optimization problem:
\eq{
	\mathcal{FS-LME} \quad \min_{\beta \in \R^p,\, \gamma \in \R^{q}_+} \LL(\beta, \gamma) + R(\beta, \gamma)
}

Where $\LL$:
\eq{
	\label{eq:lmm_objective}
	\mathcal{L}(\beta, \gamma) & = \sum_{i = 1}^m \half(y_i - X_i\beta)^T(Z_i\Gamma Z_i^T + \Lambda_i)^{-1}(y_i - X_i\beta) + \\ & + \half\log{\det{\pa{Z_i \Gamma Z_i^T + \Lambda_i}}}, \quad \Gamma = \diag{(\gamma)}
	}
\begin{itemize}
	\item $\LL(\beta, \gamma)$ is smooth on its domain, quadratic w.r.t. $\beta$ and $\bar\eta$-weakly-convex w.r.t. $\gamma$.
	\item $R(\beta, \gamma)$ is closed, proper, with easily computed \textit{prox operator}
\end{itemize}

\end{frame}


\begin{frame}{Regularization}
%Practitioners:
%\begin{itemize}
%	\item<1-> Often seek \textbf{sparse models} which only use \textbf{most informative} covariates. 
%	\item<2-> Want the algorithm to be \textbf{efficient} but also \textbf{flexible} in using various regularizers.
%	\item<3-> Want a library to be \textbf{universal and compatible} with e.g. \texttt{scikit-learn}.
%\end{itemize}
%\vspace{1em}
$R(\beta, \gamma)$ is closed, proper, with easily computed \textit{prox operator}

\eq{
	\prox_{\alpha R + \delta_{\CC}}(\tbeta, \tgamma) & := \argmin_{(\beta, \gamma) \in \CC} R(\beta, \gamma) + \frac{1}{2\alpha}\|(\beta, \gamma) - (\tbeta, \tgamma)\|_2^2, \\ & \text{ where } \CC := \R^p \times \mathbb{R}^q_+ \\
}

%Examples: 
%\begin{itemize}
%	\item $R(x) = \lambda\sum_{j=1}^p w_j\|x_j\|_1$ -- LASSO and Adaptive LASSO penalties \footcite{Krishna2008,Lin2013}
%	\item $R(x) = \lambda \|x\|_0$ -- $\ell_0$ penalty \footcite{Jones2011}
%	\item $R(x)$ -- SCAD penalty \footcite{Fan2012}
%\end{itemize}

\begin{figure}
     \centering
     \begin{subfigure}[b]{0.25\textwidth}
         \centering
         \includegraphics[width=\textwidth]{Figures/l0_regularizer.pdf}
         \caption{$\ell_0$}
     \end{subfigure}%
     \begin{subfigure}[b]{0.25\textwidth}
         \centering
         \includegraphics[width=\textwidth]{Figures/l1_regularizer.pdf}
         \caption{$\ell_1$}
     \end{subfigure}%
     \begin{subfigure}[b]{0.25\textwidth}
         \centering
         \includegraphics[width=\textwidth]{Figures/lh_regularizer.pdf}
         \caption{$\ell_p,\, p=1/2$}
     \end{subfigure}%
     \begin{subfigure}[b]{0.25\textwidth}
         \centering
         \includegraphics[width=\textwidth]{Figures/scad_regularizer.pdf}
         \caption{SCAD}
     \end{subfigure}
     \caption{Four commonly-used regularizers which promote sparsity}
     \label{fig:three graphs}
\end{figure}

\end{frame}

\begin{frame}{Proximal Gradient Descent for Feature Selection in MLE}
\only<1-2>{
Let
\eq{
	x := [\beta, \gamma], \quad \CC := \R^p \times \R^q_+
}
Optimization problem $\mathcal{FS-LME}$:

\eq{
	\min_{x \in \CC} \LL(x) + R(x)
}
}
\uncover<2->{
\begin{algorithm}[H]
\TitleOfAlgo{PGD for standard LMEs}
\SetAlgoLined
$\beta^+ \leftarrow\beta_0, \quad \gamma^+\leftarrow\gamma_0, \quad \alpha \leftarrow 1/L$ \tcp*[f]{Initialization}\\
$x^+ = [\beta^+, \gamma^+]$;\\
\While{\text{making progress}}{
	$x^+ \leftarrow \prox_{\alpha^{-1}R + \delta_{\mathcal{C}}}(x^+ - \alpha \nabla_x\LL(x^+))$	\tcp*[f]{PGD iterations}\\	
}
\Return{$x^+ = [\beta^+, \gamma^+]$}
\end{algorithm}
}
\only<3>{
\vspace{1em}
Basic Assumptions for the PGD Algorithm\footcite*[Theorem 10.15]{AB17}:
\begin{enumerate}
	\item $R$ is a closed proper convex function
	\item $\LL$ is closed and proper, $\dom{\LL}$ convex, $\dom R \subset int(\dom\LL)$, and $\LL$ is $L$-smooth over $int(\dom\LL)$. [$\times$].
	\item The problem has an optimal solution\footcite{aravkin2022jimtheory} with an optimal value $\LL^*$
\end{enumerate}
Algorithm converges with backtracking~\footcite{Burke-Engle18}. 
}
\only<4>{
	\begin{columns}[T,onlytextwidth]
    \column{0.5\textwidth}
    \vspace{1em}
    Synthetic Benchmark:
    \begin{itemize}
    	\item 100 randomly-generated problems.
    	\item $p = q = 20$.
		\item $\beta = \gamma = \frac{1}{2}[1,2,3,\dots,10, 0\dots,0]$
		\item 9 groups from $3$ to $15$ observations
		\item $X_i \sim \NN(0, I)^p$, $Z_i = X_i$, $\varepsilon_i \sim \NN(0, 0.3^2I)$
		\item Golden search for $\lambda \in [0, 10^5]$
		\item Final model is chosen to maximize BIC
    \end{itemize}
	
    \column{0.5\textwidth}
    	\vspace{1em}
    	\centering  
		\begin{figure}
			\includegraphics[width=\textwidth]{Figures/benchmark_pgd.jpg}
		\end{figure}
  \end{columns}
}
\end{frame}


\begin{frame}{Sparse Relaxed Regularized Regression ($\mathcal{SR}3$) \footcite{Zheng2018RelaxAndSplit}}

\begin{equation*}
		\begin{array}{c}
			\text{Original} \\
			 \\
			\min\limits_{x} f(x) + R(x)
		\end{array}
	\quad \goto \quad 
	\begin{array}{c}
		\text{Decoupled} \\
		\\
		\min\limits_{x, w} f(x) + \frac{\eta}{2} \|x-w\|_2^2 + R(w) 
	\end{array}
	\quad \goto \quad 
	\begin{array}{c}
		\text{Value Function} \\
		\\
		\min\limits_{w} v_\eta(w) + R(w)
	\end{array}
\end{equation*}

\begin{figure}
	\includegraphics[width=0.7\textwidth]{Figures/intuition_prev_paper.png}
\end{figure}

\end{frame}


\begin{frame}{SR3-Relaxation for Mixed-Effect Models ($\ouralgo$)}
Original problem $\mathcal{FS-LME}$:
\eq{
	\min_{x \in \CC} \LL(x) + R(x)
}
\uncover<2->{
Decoupled problem:
\eq{
	\label{eq:msr3_formulation_explicit}
	\min_{x, w \in \CC} \LL(x) + \frac{\eta}{2}\|x-w\|^2_2 + R(w)
}
where $w = [\tbeta, \tgamma]$. 
}

\uncover<3->{
By partially minimizing w.r.t. $x$ we get a value function $v_\eta(w)$:

\eq{
	\label{eq:value_function_definition}
	v_{\eta}(w) & := \min_{x \in \CC} \LL(x) + \frac{\eta}{2}\|x-w\|^2_2
}

so $\mathcal{SR}3$-formulation~(\ref{eq:msr3_formulation_explicit}) becomes

\eq{
	\min_{w \in \CC} v_{\eta}(w) + R(w)
}

}
\uncover<4->{
\textbf{NB:} $v_{\eta}(w)$ is smooth on $\CC$ and can be evaluated using Interior Point (IP) method
}
\end{frame}

\begin{frame}{Value Function of $\mathcal{MSR}3$}
$\ouralgo$-relaxation replaces the original likelihood $\LL$ with a \textit{value function} $v_{\eta,\mu}$:

\begin{equation}
	v_{\eta,\mu}(w) := \min_{x \in \CC} \LL(x) + \frac{\eta}{2}\|x-w\|^2_2 + \phi_\mu(x)
\end{equation}

where \textit{perspective mapping} $\phi_\mu$ replaces $\gamma \geq 0$ with a log-barrier 
\eq{
	\phi_\mu(x) = \phi_\mu(\gamma) := \begin{cases}
		-\mu\sum_{i=1}^q \ln(\gamma_i/\mu), & \mu > 0 \\
		\delta_{\R_+^q}(\gamma), & \mu = 0 \\
		+\infty, & \mu < 0
	\end{cases}
} 
\end{frame}


%When $\eta$ is larger than the weak-convexity constant
%\begin{itemize}
%	\item $v_{\eta,\mu}$ is well-defined and continuously differentiable.
%	\item As $\mu \rightarrow 0$ and $\eta \rightarrow \infty$, cluster points of solutions to $\ouralgo$ are first-order stationary points for $\mathcal{FS-LME}$ \\ 
%	\item $v_{\eta, \mu}$ don't need to be evaluated precisely.
%\end{itemize}

% \textbf{Key observation}: in practice, we don't need accurate solutions for~(\ref{eq:value_function_definition}): a few Newton iterations keep the solution close to the central path.


%\begin{frame}{Value Function Reformulation}
%\begin{figure}
%	\includegraphics[width=\textwidth]{Figures/intuition_prev_paper}
%	\caption{\label{fig:intuition_prev} Picture from \cite{Zheng2018RelaxAndSplit}: for a linear problem, value function relaxation ``squashes'' level-sets simplifying the optimization landscape.}
%\end{figure}
%\end{frame}

\begin{frame}{Value Function of $\mathcal{MSR}3$}
\[
	\min_{\beta, \gamma \in \CC} \LL(\beta, \gamma) + R(\beta, \gamma) \quad \text{vs} \quad \min_{\tbeta, \tgamma \in \CC} v_{\eta,\mu}(\tbeta, \tgamma) + R(\tbeta, \tgamma)
\]

\begin{figure}
	\includegraphics[width=0.7\textwidth]{Figures/intuition_current.pdf}
	\label{fig:intuition_sr3}
\end{figure}
\end{frame}

\begin{frame}{$\mathcal{MSR}3$: Algorithm}

\begin{algorithm}[H]
\TitleOfAlgo{PGD for $\mathcal{MSR}3$}
\SetAlgoLined
$\tbeta^+ \leftarrow \tbeta_0, \quad \tgamma^+ \leftarrow \tgamma_0, \quad \alpha \leftarrow 1/\eta, \quad \eta > \bar{\eta}$ \tcp*[f]{Initialization}\\
$\tw^+ := [\tbeta^+, \tgamma^+], \quad x^+ := [\beta,\gamma]$ \\	
\While{\text{making progress in $\tw$}}{
	\text{$x^+ \leftarrow$ IP on $\LL_{\eta,\mu}(x^+, \tw^+)$ s.t. $x^+ \in \CC$ and $\mu \goto 0$} \tcp*[f]{IP Iterations}\\	
	$\nabla_\tw v_{\eta,0}(\tw^+) \leftarrow  \nabla_\tw\LL_{\eta,0}(x^+, \tw^+)$ \tcp*[f]{Evaluate Gradient of VF} \\ 
	$\tw^+ \leftarrow \prox_{\alpha^{-1}R + \delta_{\mathcal{C}}}(\tw^+ - \alpha \nabla_\tw v_{\eta,0}(\tw^+))$	\tcp*[f]{PGD on Value Function}\\
}
\Return{$\tw^+ = [\tbeta^+, \tgamma^+]$}
\end{algorithm}
\only<2>{
\vspace{1em}
Theoretical Results \footfullcite{aravkin2022jimtheory}:
\begin{enumerate}
	\item When $\eta > \bar\eta$ the problem has an optimal solution $\Phi^*$ (\textbf{Theorem 5})
	\item $v_{\eta,\mu}$ is well-defined (\textbf{Theorem 5}) and continuously differentiable (\textbf{Theorem 10})
	\item When $R$ is 1-coercive $\nabla v_{\eta,\mu}$ is locally $\widetilde{L}$-continuous (\textbf{Theorem 14})
	\item Algorithm converges to a stationary point of $v_{\eta,\mu}$ (\textbf{Theorem 15})
	\item As $\mu \rightarrow 0$ (\textbf{Theorem 6}) or $\eta \rightarrow \infty$ (\textbf{Theorem 7}), cluster points of solutions to $\ouralgo$ are FOSPs for $\mathcal{FS-LME}$ 
\end{enumerate}
}
\end{frame}

\begin{frame}{$\mathcal{MSR}3$: Results}
    	\centering  
		\begin{figure}
			\includegraphics[width=0.7\textwidth]{Figures/benchmark_pgd_msr3.jpg}
		\end{figure}
\end{frame}

\begin{frame}{$\mathcal{MSR}3$: Algorithm}

\begin{algorithm}[H]
\TitleOfAlgo{PGD for $\mathcal{MSR}3$}
\SetAlgoLined
$\tbeta^+ \leftarrow \tbeta_0, \quad \tgamma^+ \leftarrow \tgamma_0, \quad \alpha \leftarrow 1/\eta, \quad \eta > \bar{\eta}$ \tcp*[f]{Initialization}\\
$\tw^+ := [\tbeta^+, \tgamma^+], \quad x^+ := [\beta,\gamma]$ \\	
\While{\text{making progress in $\tw$}}{
	\text{$x^+ \leftarrow$ \textcolor{red}{IP on $\LL_{\eta,\mu}(x^+, \tw^+)$ s.t. $x^+ \in \CC$ and $\mu \goto 0$}} \tcp*[f]{IP Iterations}\\	
	$\nabla_\tw v_{\eta,0}(\tw^+) \leftarrow  \nabla_\tw\LL_{\eta,0}(x^+, \tw^+)$ \tcp*[f]{Evaluate Gradient} \\ 
	$\tw^+ \leftarrow \prox_{\alpha^{-1}R + \delta_{\mathcal{C}}}(\tw^+ - \alpha \nabla_\tw v_{\eta,0}(\tw^+))$	\tcp*[f]{PGD on Value Function}\\
}
\Return{$\tw^+ = [\tbeta^+, \tgamma^+]$}
\end{algorithm}
\vspace{2em}
\only<2>{
\textbf{Key Observation:} $\nabla_\tw v(\tw)$ does not need to be evaluated exactly. We only need to come close enough to the central path.
}

\end{frame}

\begin{frame}{$\mathcal{MSR}3$-fast: Algorithm}
\begin{algorithm}[H]
\TitleOfAlgo{$\mathcal{MSR}3$-fast}
\SetAlgoLined
$\tbeta^+ \leftarrow \tbeta_0, \quad \tgamma^+ \leftarrow \tgamma_0, \quad \alpha \leftarrow 1/\eta, \quad \eta > \bar{\eta}$ \tcp*[f]{Initialization}\\
$\tw^+ := [\tbeta^+, \tgamma^+], \quad x^+ := [\beta,\gamma]$ \\	
\While{\text{making progress}}{
	\While{\textcolor{red}{not close enough to the central path}}{
		\text{$x^+ \leftarrow$ IP iteration on $\LL_{\eta,\mu}(x^+, \tw^+)$ s.t. $x^+ \in \CC$} \tcp*[f]{IP Iterations}\\	
	}
	\text{\textcolor{red}{Decrease $\mu$}}\\
	$\nabla_\tw v_{\eta,\mu}(\tw^+) \leftarrow  \nabla_\tw\LL_{\eta,\mu}(x^+, \tw^+)$ \tcp*[f]{Evaluate Gradient} \\ 
	$\tw^+ \leftarrow \prox_{\alpha^{-1}R + \delta_{\mathcal{C}}}(\tw^+ - \alpha \nabla_\tw v_{\eta,\mu}(\tw^+))$	\tcp*[f]{PGD on Value Function}\\
}
\Return{$\tw^+ = [\tbeta^+, \tgamma^+]$}
\end{algorithm}
\end{frame}

\begin{frame}{Designing an Algorithm}
	$G_{\nu,\eta}$ encodes both gradient of a Lagrangian (lines 1-2) and the complementarity condition (line 3):
	\eq{
		G_{\nu,\eta}((\beta,\gamma,v),(\tbeta,\tgamma)) := \begin{bmatrix}
			\nabla_\beta \LL(\beta, \gamma) + \eta(\beta-\tbeta) \\
			\nabla_\gamma \LL(\beta, \gamma) + \eta(\gamma-\tgamma) - v\\
			v \bigodot \gamma - \mu\textbf{1}
		\end{bmatrix}
	}
	We apply Newton method to $G$ while geometrically decreasing $\mu$. \\
\textbf{Lemma:} For every $(\mu,\eta) \in \R_+\times\R_{++}$,
\eq{
	&(\hat\beta,\hat\gamma) = \argmin_{(\beta,\gamma)}\LL_{\eta,\mu}((\beta, \gamma),(\tbeta, \tgamma)) \\
	& \iff \\
	& \exists \hat{v} \in \R_{+}^q \text{ s.t. } G_{\nu,\eta}((\beta,\gamma,\hat{v}),(\tbeta,\tgamma)) = 0
}
If $\mu > 0$, then $\hat{v} = -\nabla\phi_\mu(\hat{\gamma})$, and if $\mu = 0$, then $\hat{v}$ is the unique KKT multiplier associated with the constraint $0 \leq \gamma$.
\end{frame}

\begin{frame}{$\ouralgo$-fast Algorithm}
\input{../extras/08Pseudocode}
\end{frame}

\begin{frame}{$\ouralgo$-fast: Results}
\begin{itemize}
	\item The number of fixed effects $p$ and random effects $q$ is 20.
	\item $\beta = \gamma = \frac{1}{2}[1,2,3,\dots,10, 0\dots,0]$
	\item 9 groups with sizes [10, 15, 4, 8, 3, 5, 18, 9, 6]
	\item $X_i \sim \NN(0, I)^p$, $Z_i = X_i$, $\varepsilon_i \sim \NN(0, 0.3^2I)$
	\item Each experiment is repeated 100 times.
	\item Grid-search for $\eta \in [10^{-4}, 10^{2}]$, golden search for $\lambda \in [0, 10^5]$
	\item Final model is chosen to maximize BIC
\end{itemize}
\begin{figure}
	\includegraphics[width=0.7\textwidth]{Figures/benchmark.jpg}
\end{figure}
\begin{itemize}
	\item[\textcolor{green}{+}] $\ouralgo$-relaxation improves feature selection performance of the original likelihood.
	\item[\textcolor{green}{+}] $\ouralgo$-fast optimization accelerates the compute time by $\sim 10^2$.
   	\item[\textcolor{red}{--}] Initialization of $\eta$ is problem-specific
\end{itemize}
\end{frame}

\begin{frame}{Comparison to Other Libraries}
\begin{table}
	\input{../extras/table_glmm_current.tex}
\end{table}
\end{frame}

\begin{frame}{Choice of $\eta$}
	\begin{figure}
		\includegraphics[width=\textwidth]{Figures/eta_L1.pdf}
	\end{figure}
\end{frame}

\begin{frame}{$\ell_0$-based Covariate Selection for Bullying Study from GBD}
	\begin{figure}
		\includegraphics[width=\textwidth]{Figures/bullying_data_assessment_selection.pdf}
		\caption{Fixed and random covariate selection for Bullying dataset\footnote{Institute for Health Metrics and Evaluation (IHME). Bullying Victimization Relative Risk Bundle GBD 2020. Seattle, United States of America (USA), 2021.}. The model selected 9 covariates, 7 of which were historically significant, and did not select 4 covariates, 1 of which was historically significant.}
	\end{figure}
\end{frame}

\begin{frame}{Software}
	\includegraphics[width=\textwidth]{Figures/pysr3_screenshot.png}
	The code is available on GitHub: \href{github.com/aksholokhov/pysr3}{https://github.com/aksholokhov/pysr3}\footfullcite{sholokhov2023pysr3}
	\begin{itemize}
		\item All estimators are fully compatible to \texttt{scikit-learn} library.
		\item Implements SR3 for linear, generalized-linear, and linear mixed-effect models.
		\item Has tutorials, tests, and documentation.
	\end{itemize}
\end{frame}

\section{Physics-Informed Neural ODE (PINODE): Embedding Physics into Models using Collocation Points}

\subsection{Intro to Physics-Informed Reduced-Order Models (ROMs)}

\begin{frame}{Modeling of Physical Systems}

%1) People used to model physical systems with first-principle knowledge
%2) Data-Driven modelling of dynamical systems became a big thing  
%3) However, it requires a lot of data 
%4) Incorporating prior knowledge is a big recent trend, so history does a spiral

\uncover<2->{
\begin{columns}[T,onlytextwidth]
\column{0.6\textwidth}
\textcolor{orange}{First-Principle Models}
\begin{itemize}
	\item Require extensive knowledge of the phenomenon
	\item Require a lot of compute for simulating large-scale phenomena
\end{itemize}
\column{0.4\textwidth}
\centering
\includegraphics[width=\textwidth]{Figures/NN.jpg}
\end{columns}
}
\vspace{2em}
\uncover<3->{
\begin{columns}[T,onlytextwidth]
\column{0.6\textwidth}
\textcolor{teal}{Data-Driven Models}
\begin{itemize}
	\item Require a lot of data
	\item Often struggle to extrapolate
\end{itemize}
\column{0.4\textwidth}
\centering
\includegraphics[width=0.8\textwidth]{Figures/nn_schematics.pdf}
\end{columns}
}
\vspace{2em}
\uncover<4->{
\begin{columns}[T,onlytextwidth]
\column{0.6\textwidth}
\textcolor{blue}{Hybrid Models}
\begin{itemize}
	\item Incorporate elements of both approaches
	\item Supplement data with knowledge or priors
\end{itemize}
\column{0.4\textwidth}
\centering
\includegraphics[width=0.5\textwidth]{Figures/NS_pic.png}
\end{columns}

Pictures sources: \footnote{\href{https://en.wikipedia.org/wiki/Navier–Stokes_equations}{Wikipedia: Navier-Stokes}},\footnote{\href{https://www.tibco.com/reference-center/what-is-a-neural-network}{tibco.com}},\footnote{\href{https://www.comsol.com/model/water-purification-reactor-14049}{COMSOL Simulations}}
}
\end{frame}

\begin{frame}{Incorporating Knowledge of Physics into Neural Networks}
\begin{enumerate}
	\item<2-> \textbf{Symmetry Based:} incorporate symmetries and conservation laws as hard constraints into a network: E3NNs\footcite{geiger2022e3nn}, LieConv\footcite{Finzi2020}, RiCNN \footcite{chidester2018rotation}
	\item<3-> \textbf{Model Discovery Based:} use a library of terms to discover simple equations: SINDy\footcite{champion2019data}, Genetic Programming\footcite{schmidt2009distilling}
	\item<4-> \textbf{Equation Based:} incorporate first-principle models to aid training of networks: PINNs\footcite{raissi2017physics}, UDEs\footcite{rackauckas2020udes}, \textit{PINODE}
\end{enumerate}

\begin{columns}[T,onlytextwidth]
\column{0.3\textwidth}
	\uncover<2->{\includegraphics[width=0.7\textwidth]{Figures/ricnn.png}}
\column{0.7\textwidth}
	\uncover<3->{\includegraphics[width=\textwidth]{Figures/sindy.png}}
\end{columns}

\end{frame}


\begin{frame}{Reduced-Order Models (ROMs)}
\only<1>{\includegraphics[width=\textwidth]{Figures/roms_1}}
\only<2>{\includegraphics[width=\textwidth]{Figures/roms_2}}
\only<3>{\includegraphics[width=\textwidth]{Figures/roms_3}}
\only<4>{\includegraphics[width=\textwidth]{Figures/roms_4}}
\end{frame}

\begin{frame}{Reduced-Order Models: Data-Driven Loss}
\includegraphics[width=\textwidth]{Figures/just_rom.png}
\vspace{0.5em}

Notation:
\begin{itemize}
	\item $k$ trajectories $x_i(t_j)$, $p$ timesteps each ($i = 1, \dots, k$, $j = 1, \dots, p$)
	\item $\sigma$ -- observation noise variance
	\item $\omega_1$, $\omega_2$ -- weights (hyper-parameters) 
\end{itemize}

\begin{align}
    \LL^{data}_\theta & = \frac{1}{2\sigma^2}\sum_{i = 1}^k \left[\frac{\omega_1}{p}\sum_{j=1}^p\underbrace{\left\|\bd{x_i}(t_j) - \psi_\theta(\phi_\theta(\bd{x_i}(t_j)))\right\|^2}_{\text{reconstruction loss}}\right. + \\
     & + \left.\frac{\omega_2}{p}\sum_{j=1}^p \underbrace{\left\|\psi_\theta\left(\phi_\theta(\bd{x_i}(t_1)) + \int_{t_1}^{t_j}h(z(t))dt\right) - \bd{x_i}(t_j)\right\|^2}_{\text{prediction loss}} \right]
\end{align}

\end{frame}

\begin{frame}{Physics-Informed Loss}
%1) We introduce physics to this system by adding a term which regularizes latent gradient field. 
%2) In particular, it forces it to be equal to what a true physics should be under such projection.
%3) We can not evaluate physics everywhere but we can at particular carefully-selected points. We call these points collocation points.
%4) We feed a lot of collocation points and ask a network to do interpolation 
Using chain rule:
\only<1>{\includegraphics[width=\textwidth]{Figures/collocations_1.png}}
\only<2>{\includegraphics[width=\textwidth]{Figures/collocations_2.png}}
\only<3>{\includegraphics[width=\textwidth]{Figures/collocations_3.png}}
\only<4>{\includegraphics[width=\textwidth]{Figures/collocations_4.png}}
\only<5>{\includegraphics[width=\textwidth]{Figures/collocations_5.png}}
\only<6>{\includegraphics[width=\textwidth]{Figures/collocations_6.png}}
\only<7>{\includegraphics[width=\textwidth]{Figures/collocations_7.png}}
\uncover<6->{\textbf{Collocations:} a set of pairs $(\tilde{x}_i, \dot{\tilde{x}}_i)$, where $\tilde{x}_i$ are s.t. $\dot{\tilde{x}}_i$ are cheap to evaluate.}
\end{frame}

\begin{frame}{Physics-Informed, Data-Driven, and Hybrid Models}
	Values of $\omega_1$, $\omega_2$, $\omega_3$, and $\omega_4$ control the type of the ROM:
	\begin{itemize}
		\item If $\omega_1 = \omega_2 = 0$ then $\LL_\theta^\text{data} = 0$. Thus, the model is purely \textcolor{orange}{Physics-Informed}
		\item If $\omega_3 = \omega_4 = 0$ then $\LL_\theta^\text{physics} = 0$. Thus, the model is purely \textcolor{teal}{Data-Driven}
		\item Otherwise the model is \textcolor{blue}{Hybrid}
	\end{itemize}
\end{frame}

\begin{frame}{Results: Extrapolation to Unknown Regions}
% 1) We show that network can indeed interpolate between collocations. Moreover, it can fill the whole unknown regimes of behavior. (Duffing example with explanation)
Duffing Oscillator on a low-dimensional (2D) manifold:
\begin{equation}
    \label{eq:duffing_definition}
    \begin{split}
    \frac{dz_1}{dt} & = z_2 \\ 
    \frac{dz_2}{dt} & = z_1 - z_1^3
    \end{split}
\end{equation}

Projection to a high-dimensional (128) space:
\begin{equation}
    \label{eq:duffing_true_decoder}
    \bd{x} := \mathcal{A}(\bd{z}) = A\bd{z}^3, \quad A \in \mathbb{R}^{128 \times 2}, \quad A_{ij} \sim_{i.i.d.} \mathcal{N}(0, 1)
\end{equation}
\uncover<2->{
\begin{figure}
	\centering
	\includegraphics[width=0.8\textwidth]{Figures/duff_results.png}
\end{figure}
}
\end{frame}

\begin{frame}{Results: Stable Long-Term Predictions}
\begin{figure}
	\begin{subfigure}[b]{0.3\textwidth}
		\includegraphics[width=\textwidth]{Figures/duff_full_train.pdf}
	\end{subfigure}%
	\begin{subfigure}[b]{0.7\textwidth}
		\centering
		\includegraphics[width=0.7\textwidth]{Figures/duffing_periods.pdf}
	\end{subfigure}
\end{figure}
\uncover<2->{
\begin{figure}
	\includegraphics[width=0.7\textwidth]{Figures/duffing_periods_examples.png}
\end{figure}
}
\end{frame}

\begin{frame}{Results: Burgers' Equation}
\begin{equation}
	\begin{split}
		& u_t  + uu_x = \nu u_{xx} \\
	    & u(-\pi, t) = u(\pi, t),\quad \forall t \in [0, T], \quad \nu = 0.01
	\end{split}
\end{equation}
\uncover<2->{
\includegraphics[width=\textwidth]{Figures/burgers_examples_of_ics.pdf}
}

\only<2>{
\small
ICs and Collocations:
\begin{columns}[T,onlytextwidth]
\column{0.3\textwidth}
Harmonic:
\begin{equation*}
	\begin{split}
		& u(x) = \sum_{k=1}^{30}a_k \sin(2\pi x/k) \\ & + b_k \cos(2\pi x/k) \\
		& a_k = \alpha_ks_k \\
		& \alpha_k \sim \mathcal{U}(0, 1) \\
		& s_k \sim \mathcal{B}e(0.5)
	\end{split}
\end{equation*}
	
\column{0.3\textwidth}
Bell-curve:
\begin{equation*}
	\begin{split}
		& u(x) = \frac{1}{\sqrt{2\pi \sigma^2}}\exp^{\frac{-(x-x_0)^2}{2\sigma^2}} \\
		& \sigma \sim \mathcal{U}(0, 3) \\
		& x_0 \sim \mathcal{U}(-\pi, \pi) 
	\end{split}
\end{equation*}

\column{0.3\textwidth}
Bumps:
	\begin{equation*}
		\begin{split}
			& u(x) = \frac{a}{1+\exp^{-k(x-x_0)}} \\ & - \frac{a}{1 + \exp^{-k(x-x_1)}} \\
			& k \sim \mathcal{U}(5, 20) \\
			& x_0, x_1 \sim \mathcal{U}(-\pi, \pi) 
		\end{split}
	\end{equation*}
\end{columns}
}

\uncover<3->{
\includegraphics[width=\textwidth]{Figures/example_burgers.pdf}
}

\end{frame}

\begin{frame}{Results: Learning From Collocations}
\includegraphics[width=\textwidth]{Figures/burgers_examples_of_ics.pdf}
\includegraphics[width=\textwidth]{Figures/data_vs_collocations.pdf}
\end{frame}

\begin{frame}{Discussion and Limitations}
We showed that:
\begin{itemize}
	\item Physics-informed loss improves accuracy and forecasting stability of ROMs
	\item Collocations can supplement data to improve model's performance for unseen initial conditions.
\end{itemize}
\vspace{2em}
Limitations:
\begin{itemize}
	\item Optimal choice of collocations is problem-specific
	\item Need a lot of collocations
	\begin{itemize}
		\item \textit{Seems} possible to overcome with smarter sampling techniques
	\end{itemize}
\end{itemize}
\end{frame}

\section{Single pixel imaging of spatio-temporal flows using differentiable latent dynamics}

\begin{frame}{Single-Pixel Imaging}
%1) Now we switch gears to show you an application to single-pixel imaging
%2) It shows how powerful these differentiable reduced-order models can be in applications besides forecasting and motivate the need to improve those.
%3) SPI setup is a single-pixel camera with mirror array… 
\begin{figure}
	\includegraphics[width=\textwidth]{Figures/SPI_setup.pdf}
\end{figure}
\end{frame}


\begin{frame}{Single-Pixel Imaging}
	% This is how SPI setup translates into Math
	\includegraphics[width=\textwidth]{Figures/spi_math.png}
	\uncover<2->{
		\centering
		\textit{Single-Pixel Image Recovery}: find $\textbf{x}$ using $\textbf{y}$ and $\textbf{A}$.
		
		\vspace{1em}
				
		\textbf{SPF rate}, \% = Number of samples per frame / number of pixels per frame * 100
	}
\end{frame}


\begin{frame}{Compressive Sensing with Reduced-Order Models}
	%1) We introduce a ROM as a relaxation of a PDE-constrained compressive sensing.
	%2) We search in the latent space of a ROM for the right trajectory that matches both the ROMs predictions and the compressive-sensing observations
	\includegraphics[width=\textwidth]{Figures/cs_schematics.png}
\end{frame}

\begin{frame}{Results: Burger's Equation}
	\uncover<1->{
	Recovery with SPF=25\%:
	\includegraphics[width=\textwidth]{Figures/cs_burgers_comparison_32.pdf}
	}
	\uncover<2->{
	Recovery with SPF=6.25\%:
	\includegraphics[width=\textwidth]{Figures/cs_burgers_comparison_8.pdf}
	}
	\uncover<3->{
	Recovery with SPF=1.5\%:
	\includegraphics[width=\textwidth]{Figures/cs_burgers_comparison_2.pdf}
	}
	
	
\end{frame}

\begin{frame}{Results: Burger's Equation}
	\includegraphics[width=\textwidth]{Figures/cs_burgers_psnr.png}
	
	\vspace{1em}
	
	\hspace{2em} Higher is better \hspace{2.6em} Lower is better \hspace{2.6em} Higher is better
	
	\vspace{1em}
	Metrics:
	\begin{itemize}
		\item \textbf{PSNR}: Peak Signal-to-Noise Ratio 
		\item \textbf{RMSE}: Residual Mean Square Error
		\item \textbf{SSIM}: Structural Similarity Index Measure
	\end{itemize}
\end{frame}

\begin{frame}{Results: Kolmogorov Flow}
	A 2D velocity field $\bd{u}(x, y, t)$:

	\begin{align}
	& \partial_t \bd{u} + \bd{u} \cdot \nabla \bd{u} = -\nabla p + \nu\nabla^2\bd{u} + f \\
	& \nabla \cdot \bd{u} = 0
	\end{align}
	where 
	\begin{itemize}
		\item $p$ is a 2D pressure field
		\item $f = \alpha\sin(ky)x$ - driving force with amplitude $A$ and wavenumber $k$ ($k=4$)
		\item $\nu = 1/\text{Re}$ is the non-dimensional viscosity. Re=40
	\end{itemize}
	\vspace{1em}
	Reconstruction with SPF rate = $0.18$\% (8 SPI samples per a frame of $66\times 66$ pixels):  
	\animategraphics[loop,width=\textwidth]{5}{Figures/kolm_example_anim_}{0}{19}
\end{frame}

\begin{frame}{Results: Kolmogorov Flow}
	\includegraphics[width=\textwidth]{Figures/cs_kolm_aggregate.png}
	
	 \hspace{5em} Lower is better \hspace{9em} Higher is better
\end{frame}

\begin{frame}{Results: Interpretation}
	\only<1>{\includegraphics[width=\textwidth]{Figures/cs_int_1.png}}
	\only<2>{\includegraphics[width=\textwidth]{Figures/cs_int_2.png}}
	\only<3>{\includegraphics[width=\textwidth]{Figures/cs_int_3.png}}
\end{frame}

%\againframe{again}

\begin{frame}{Results: Interpretation}
	\only<1>{\includegraphics[width=\textwidth]{Figures/cs_int_3.png}}
	\only<2>{\includegraphics[width=\textwidth]{Figures/cs_int_4.png}}
	\only<3>{\includegraphics[width=\textwidth]{Figures/cs_int_5.png}}
\end{frame}

%\againframe{again}

\begin{frame}{Results: Interpretation}
	\only<1>{\includegraphics[width=\textwidth]{Figures/cs_int_5.png}}
	\only<2>{\includegraphics[width=\textwidth]{Figures/cs_int_6.png}}
	\only<3>{\includegraphics[width=\textwidth]{Figures/cs_int_7.png}}
\end{frame}

%\againframe{again}

\begin{frame}{Results: Gas Monitoring}
	\vspace{2em}
	Gas monitoring problem:
	\begin{itemize}
		\item 494 240$\times$320 video recordings, 10 steps each
		\item 512 samples per frame, SPF rate of 0.67\%
		\item Background noise
	\end{itemize}
	\centering
	\animategraphics[loop,width=0.6\textwidth]{5}{Figures/gas_frames_}{1}{9}
\end{frame}

\begin{frame}{Thank You!}
\includegraphics[height=35pt]{figures/sasha.png}
\includegraphics[height=35pt]{figures/jim.jpg}
\includegraphics[height=35pt]{figures/peng.jpg}
\includegraphics[height=35pt]{figures/hassan.jpg}
\includegraphics[height=35pt]{figures/saleh.jpeg}
\includegraphics[height=35pt]{figures/yuying.jpeg}
\includegraphics[height=35pt]{figures/nathan.jpg}
\includegraphics[height=35pt]{figures/steve.jpeg}
\includegraphics[height=35pt]{figures/josh.jpg}

\vspace{1em}
\textbf{Sparse Relaxed Regularized Regression for Linear Mixed-Effect Models}\\
{\small
Together with Aleksandr Aravkin, James Burke, Damian Santomauro, and Peng Zheng
\begin{itemize}
	\item ``A Relaxation Approach to Feature Selection for Linear Mixed Effects Models''\\ \textit{submitted to Journal of Computational and Graphical Statistics (JCGS)}
	\item ``Analysis of Relaxation Methods for Feature Selection in Mixed Effects Models'' \\ \textit{submitted to Journal of Computational Optimization and Applications (COAP)}
	\item ``\texttt{pysr3}: A Python Package for Spare Relaxed Regularized Regression'' \\ \textit{Journal of Open-Source Software (JOSS), 2023, ICCOPT 2022, SIAM OPT 2023}
\end{itemize}
}

\vspace{1em}
\textbf{PINODE: Physics-Informed Neural ODEs}\\
{\small
Together with Hassan Mansour, Saleh Nabi, and Yuying Liu
\begin{itemize}
	\item ``Physics-Informed Koopman Network'' \\
	\textit{SIAM ADS 2023, arXiv:2211.09419}
	\item ``Physics-Informed Neural ODE: Embedding Physics into Models using Collocation Points''\\
	\textit{Submitted to Nature Special Issue on Physics-Informed Machine Learning}
\end{itemize}
}

\vspace{1em}
\textbf{Single-Pixel Imaging with Reduced-Order Models}\\
{\small
Together with J. Nathan Kutz, Steven Brunton, Joshua Rapp, Hassan Mansour, and Saleh Nabi
\begin{itemize}
	\item ``Single pixel imaging of spatio-temporal flows using differentiable latent dynamics'' \textit{in preparation}
\end{itemize}
}
\end{frame}

\begin{frame}[noframenumbering,plain,allowframebreaks]{References}
    \printbibliography[heading=none]
\end{frame}

\appendix
\section{Appendix: \texorpdfstring{$\mathcal{MSR}3$}{MSR3}}

\end{document}