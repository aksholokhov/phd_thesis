\documentclass[8pt]{beamer}
\input{useful_packages}
\input{math_macros}


% \usetheme[progressbar=frametitle]{metropolis}
\usepackage{appendixnumberbeamer}
\addbibresource{../bibliography.bib}

% PRESETS OF PACKAGES AND MATH %

\usepackage{xspace}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{lmodern}
\newcommand{\themename}{\textbf{\textsc{metropolis}}\xspace}
\newcommand{\ouralgo}{\ensuremath{\mathcal{MSR}3}}
\title{PhD Defense}
\date{\today}
\author{Aleksei Sholokhov}
%\titlegraphic{\includegraphics[width=0.5\textwidth]{Figures/UW_amath.pdf}\hspace{3em}\includegraphics[height=1cm]{Figures/IHME_Logo.jpg}}

\begin{document}

\maketitle

%\section{Introduction}
\begin{frame}{Plan of the Defense}
Show topics and published papers. Mention covid
\end{frame}


\begin{frame}{Linear Mixed-Effect (LME) Models}

	Dataset: $m$ groups $(X_i, Z_i, y_i),\quad i = 1, \dots m$, each has $n_i$ observations
	\begin{itemize}
		\item 	$X_i \in \R^{n_i \times p}$ -- group $i$ design matrix for fixed features
		\item 	$Z_i \in \R^{n_i \times q}$ -- group $i$ design matrix for random features
		\item 	$y_i \in \R^{n_i}$ -- group $i$ observations  
		\item   $u_i \in \R^q$ -- random effects
		\item   $\Gamma \in \R^{q \times q}$ -- covariance matrix of random effects, often $\Gamma = \diag{(\gamma)}$
		\item   $\Lambda_i \in R^{n_i \times n_i}$ -- covariance matrix for observation noise
	\end{itemize}
	

	\begin{columns}[T,onlytextwidth]
	
    \column{0.5\textwidth}
    \vspace{3em}
    Model:
	 	\[
   		\begin{split}
   			y_i & = X_i\beta + {\color{red}Z_i u_i} + \varepsilon_i \\
   			 \varepsilon_i & \sim \NN(0, \Lambda_i) \\
   			{\color{red} u_i} & {\color{red}\sim \NN(0, \Gamma)}
   		\end{split}
   		\]
   		   	
	Unknowns: $\beta$, $u_i$, $\gamma$, sometimes $\Lambda_i$.
	
    \column{0.5\textwidth}
    	\centering  
   	\begin{figure}
   		\includegraphics[width=0.9\textwidth]{Figures/lme_example_random_prediction}
   	\end{figure}
   	   		

   	
  \end{columns}
\end{frame}

\begin{frame}{Mixed-Effect Models}
Mixed-effect models
\begin{itemize}
	\item Used for analyzing \textbf{combined data} across a range of \textbf{groups}.
	\item Use covariates to separate the \textbf{population variability} from the \textbf{group variability}.
	\item \textbf{Borrow strength} across groups to estimate key statistics. % when data within units are sparse or highly variable.
\end{itemize}

\begin{figure}
	\centering
	\includegraphics[width=0.8\textwidth]{Figures/ihme_predictions.png}\footnote{Picture is taken from \href{covid19.healthdata.org}{covid19.healthdata.org} }
\end{figure}

\end{frame}

%\begin{frame}{Notation}
%	\eq{
%   		y_i & = X_i\beta + Z_iu_i + \varepsilon_i \quad i = 1\dots m \\
%   		\varepsilon_i & \sim \NN(0, \Lambda_i) \\
%   		u_i & \sim \NN(0, \Gamma)
%   	}   	
%   	
%   	\begin{itemize}
%   		\item $p$ -- number of fixed features, $q$ -- number of random effects.
%   		\item $\beta \in \R^p$ -- fixed effects, or mean effects
%   		\item $u_i \in \R^q$ -- random effects
%   		\item $\Gamma \in \R^{q \times q}$ -- covariance matrix of random effects, often $\Gamma = \diag{(\gamma)}$
%   		\item $\varepsilon_i \in \R^{n_i}$ -- observation noise
%   		\item $\Lambda_i \in R^{n_i \times n_i}$ -- covariance matrix for noise
%   	\end{itemize}
%   	Unknowns: $\beta$, $u_i$, $\gamma$, sometimes $\Lambda_i$.
%\end{frame}

%\section{Sparse Relaxed Regularized Regression (SR3) for Linear Mixed-Effects Models}
%\subsection{ Mixed-Effects Models}


\begin{frame}{Feature Selection for Mixed-Effect Models}
Practitioners:
\begin{itemize}
	\item<1-> Often seek \textbf{sparse models} which only use \textbf{most informative} covariates. 
	\item<2-> Want the algorithm to be \textbf{efficient} but also \textbf{flexible} in using various regularizers.
	\item<3-> Want a library to be \textbf{universal and compatible} with e.g. \texttt{scikit-learn}.
\end{itemize}
\vspace{1em}
\uncover<4->{
Optimization problem:
\eq{
	\mathcal{FS-LME} \quad \min_{\beta \in \R^p,\, \gamma \in \R^{q}_+} \LL(\beta, \gamma) + R(\beta, \gamma)
}

Where $\LL$:
\eq{
	\label{eq:lmm_objective}
	\mathcal{L}(\beta, \gamma) & = \sum_{i = 1}^m \half(y_i - X_i\beta)^T(Z_i\Gamma Z_i^T + \Lambda_i)^{-1}(y_i - X_i\beta) + \\ & + \half\log{\det{\pa{Z_i \Gamma Z_i^T + \Lambda_i}}}, \quad \Gamma = \diag{(\gamma)}
	}
}
\uncover<5->{
\begin{itemize}
	\item $\LL(\beta, \gamma)$ is smooth on its domain, quadratic w.r.t. $\beta$ and $\bar\eta$-weakly-convex w.r.t. $\gamma$.
	\item $R(\beta, \gamma)$ is closed, proper, with easily computed \textit{prox operator}
\end{itemize}
}

\end{frame}


\begin{frame}{Regularization}
\begin{itemize}
	\item $R(\beta, \gamma)$ is closed, proper, with easily computed \textit{prox operator}
\end{itemize}

\eq{
	\prox_{\alpha R + \delta_{\CC}}(\tbeta, \tgamma) & := \argmin_{(\beta, \gamma) \in \CC} R(\beta, \gamma) + \frac{1}{2\alpha}\|(\beta, \gamma) - (\tbeta, \tgamma)\|_2^2, \\ & \text{ where } \CC := \R^p \times R^q_+ \\
}

Examples: 
\begin{itemize}
	\item $R(x) = \lambda\sum_{j=1}^p w_j\|x_j\|_1$ -- LASSO and Adaptive LASSO penalties \footcite{Krishna2008,Lin2013}
	\item $R(x) = \lambda \|x\|_0$ -- $\ell_0$ penalty \footcite{Jones2011}
	\item $R(x)$ -- SCAD penalty \footcite{Fan2012}
\end{itemize}

\begin{figure}
     \centering
     \begin{subfigure}[b]{0.25\textwidth}
         \centering
         \includegraphics[width=\textwidth]{Figures/l0_regularizer.pdf}
         \caption{$\ell_0$}
     \end{subfigure}%
     \begin{subfigure}[b]{0.25\textwidth}
         \centering
         \includegraphics[width=\textwidth]{Figures/l1_regularizer.pdf}
         \caption{$\ell_1$}
     \end{subfigure}%
     \begin{subfigure}[b]{0.25\textwidth}
         \centering
         \includegraphics[width=\textwidth]{Figures/lh_regularizer.pdf}
         \caption{$\ell_p,\, p=1/2$}
     \end{subfigure}%
     \begin{subfigure}[b]{0.25\textwidth}
         \centering
         \includegraphics[width=\textwidth]{Figures/scad_regularizer.pdf}
         \caption{SCAD}
     \end{subfigure}
     \caption{Four commonly-used regularizers which promote sparsity}
     \label{fig:three graphs}
\end{figure}

\end{frame}

\begin{frame}{Proximal Gradient Descent for Feature Selection in MLE}
\begin{algorithm}[H]
\TitleOfAlgo{PGD for standard LMEs}
\SetAlgoLined
$\beta^+ \leftarrow\beta_0, \quad \gamma^+\leftarrow\gamma_0, \quad \alpha \leftarrow 1/L$ \tcp*[f]{Initialization}\\
$x^+ = [\beta^+, \gamma^+]$;\\
\While{\text{making progress}}{
	$x^+ \leftarrow \prox_{\alpha^{-1}R + \delta_{\mathcal{C}}}(x^+ - \alpha \nabla_x\LL(x^+))$	\tcp*[f]{PGD iterations}\\	
}
\Return{$x^+ = [\beta^+, \gamma^+]$}
\end{algorithm}
\only<2>{
\vspace{2em}
Basic Assumptions for the PGD Algorithm (Theorem 10.15 from \footcite*{AB17})
\begin{enumerate}
	\item $R$ is a closed proper convex function
	\item $\LL$ is closed and proper, $\dom{\LL}$ convex, $\dom R \subset int(\dom\LL)$, and $\LL$ is $L$-smooth over $int(\dom\LL)$.
	\item The problem has an optimal solution with an optimal value $\LL^*$
\end{enumerate}
}
\only<3>{
	\begin{columns}[T,onlytextwidth]
    \column{0.5\textwidth}
    \vspace{2em}
    Synthetic Benchmark:
    \begin{itemize}
    	\item 100 randomly-generated problems.
    	\item $p = q = 20$.
		\item $\beta = \gamma = \frac{1}{2}[1,2,3,\dots,10, 0\dots,0]$
		\item 9 groups from $3$ to $15$ observations
		\item $X_i \sim \NN(0, I)^p$, $Z_i = X_i$, $\varepsilon_i \sim \NN(0, 0.3^2I)$
		\item Golden search for $\lambda \in [0, 10^5]$
		\item Final model is chosen to maximize BIC
    \end{itemize}
	
    \column{0.5\textwidth}
    	\vspace{1em}
    	\centering  
		\begin{figure}
			\includegraphics[width=\textwidth]{Figures/benchmark_pgd.jpg}
		\end{figure}
  \end{columns}
}
\end{frame}


\begin{frame}{Sparse Relaxed Regularized Regression ($\mathcal{SR}3$) \footcite{Zheng2018RelaxAndSplit}}
\begin{equation}
	\min_{x} f(x) + R(x)\quad \goto \quad \min_{x, w} f(x) + \frac{\eta}{2} \|x-w\|_2^2 + R(w)
\end{equation}

\begin{figure}
	\includegraphics[width=0.7\textwidth]{Figures/intuition_prev_paper.png}
\end{figure}

\textbf{Objectives:}
\begin{itemize}
	\item Introduce $\mathcal{SR}3$ relaxation to linear mixed-effects models - $\mathcal{MSR}3$ (see \footcite{sholokhov2022relaxation}).
	\item Develop theoretical foundations for it (see \footcite{aravkin2022jimtheory}).
	\item Implement it as a \texttt{scikit-learn}-compatible Python package -- \texttt{pysr3} (see \footcite{sholokhov2023pysr3}).
\end{itemize}
\vspace{1em}

\end{frame}


\begin{frame}{SR3-Relaxation for Mixed-Effect Models ($\ouralgo$)}
Original problem $\mathcal{FS-LME}$:
\eq{
	\min_{\beta \in \R^p,\, \gamma \in \R^{q}_+} \LL(\beta, \gamma) + R(\beta, \gamma)
}
Relaxed problem $\ouralgo$:
\eq{
	\label{eq:msr3_formulation_explicit}
	\min_{\beta, \tbeta \in \R^p,\, \gamma, \tgamma \in \R^{q}_+} \LL(\beta, \gamma) + \phi_\mu(\gamma) + \kappa_\eta(\beta - \tbeta, \gamma - \tgamma) + R(\tbeta, \tgamma)
}
where the \textit{relaxation} $\kappa_\eta$ decouples the likelihood and the regularizer 
\eq{
	\kappa_\eta(\beta - \tbeta, \gamma - \tgamma) := \frac{\eta}{2}\|\beta - \tbeta\|^2_2 + \frac{\eta}{2}\|\gamma - \tgamma\|_2^2, \quad \eta > \bar\eta
}
and the \textit{perspective mapping} $\phi_\mu$ replaces $\gamma \geq 0$ with a log-barrier 
\eq{
	\phi_\mu(\gamma) := \begin{cases}
		-\mu\sum_{i=1}^q \ln(\gamma_i/\mu), & \mu > 0 \\
		\delta_{\R_+^q}(\gamma), & \mu = 0 \\
		+\infty, & \mu < 0
	\end{cases}
} 
\end{frame}

\begin{frame}{Value Function of $\mathcal{MSR}3$}
$\ouralgo$-relaxation replaces the original likelihood $\LL$ with a \textit{value function} $u_{\eta,\mu}$:
\eq{
	\label{eq:value_function_definition}
	v_{\eta,\mu}(\tbeta,\tgamma) & := \min_{(\beta, \gamma)} \LL_{\eta,\mu}((\beta, \gamma),(\tbeta, \tgamma)) \\  & := \min_{(\beta, \gamma)} \LL(\beta, \gamma) + \phi_\mu(\gamma) + \kappa_\eta(\beta - \tbeta, \gamma - \tgamma)
}
so $\ouralgo$-formulation~(\ref{eq:msr3_formulation_explicit}) becomes
\eq{
	\min_{\tbeta, \tgamma \in \CC} v_{\eta,\mu}(\tbeta, \tgamma) + R(\tbeta, \tgamma)
}

\textbf{NB:} $v_{\eta,\mu}(\tbeta, \tgamma)$ is smooth on $\CC$ and can be evaluated using Interior Point (IP) method

%When $\eta$ is larger than the weak-convexity constant
%\begin{itemize}
%	\item $v_{\eta,\mu}$ is well-defined and continuously differentiable.
%	\item As $\mu \rightarrow 0$ and $\eta \rightarrow \infty$, cluster points of solutions to $\ouralgo$ are first-order stationary points for $\mathcal{FS-LME}$ \\ 
%	\item $v_{\eta, \mu}$ don't need to be evaluated precisely.
%\end{itemize}

% \textbf{Key observation}: in practice, we don't need accurate solutions for~(\ref{eq:value_function_definition}): a few Newton iterations keep the solution close to the central path.
\end{frame}

%\begin{frame}{Value Function Reformulation}
%\begin{figure}
%	\includegraphics[width=\textwidth]{Figures/intuition_prev_paper}
%	\caption{\label{fig:intuition_prev} Picture from \cite{Zheng2018RelaxAndSplit}: for a linear problem, value function relaxation ``squashes'' level-sets simplifying the optimization landscape.}
%\end{figure}
%\end{frame}

\begin{frame}{Value Function of $\mathcal{MSR}3$}
\[
	\min_{\beta, \gamma \in \CC} \LL(\beta, \gamma) + R(\beta, \gamma) \quad \text{vs} \quad \min_{\tbeta, \tgamma \in \CC} v_{\eta,\mu}(\tbeta, \tgamma) + R(\tbeta, \tgamma)
\]

\begin{figure}
	\includegraphics[width=0.7\textwidth]{Figures/intuition_current.pdf}
	\label{fig:intuition_sr3}
\end{figure}
\end{frame}

\begin{frame}{$\mathcal{MSR}3$: Algorithm}

\begin{algorithm}[H]
\TitleOfAlgo{PGD for $\mathcal{MSR}3$}
\SetAlgoLined
$\tbeta^+ \leftarrow \tbeta_0, \quad \tgamma^+ \leftarrow \tgamma_0, \quad \alpha \leftarrow 1/\eta, \quad \eta > \bar{\eta}$ \tcp*[f]{Initialization}\\
$\tw^+ := [\tbeta^+, \tgamma^+], \quad x^+ := [\beta,\gamma]$ \\	
\While{\text{making progress in $\tw$}}{
	\text{$x^+ \leftarrow$ IP solution on $\LL_{\eta,\mu}(x^+, \tw^+)$ s.t. $x^+ \in \CC$} \tcp*[f]{IP Iterations}\\	
	$\nabla_\tw v_{\eta,0}(\tw^+) \leftarrow  \nabla_\tw\LL_{\eta,0}(x^+, \tw^+)$ \tcp*[f]{Evaluate Gradient} \\ 
	$\tw^+ \leftarrow \prox_{\alpha^{-1}R + \delta_{\mathcal{C}}}(\tw^+ - \alpha \nabla_\tw v_{\eta,0}(\tw^+))$	\tcp*[f]{PGD on Value Function}\\
}
\Return{$\tw^+ = [\tbeta^+, \tgamma^+]$}
\end{algorithm}
\only<2>{
\vspace{1em}
Theoretical Results \footcite{aravkin2022jimtheory}:
\begin{enumerate}
	\item The problem has an optimal solution with an optimal value $\Phi^*$ (\textbf{Theorem 5})
	\item $v_{\eta,\mu}$ is well-defined (\textbf{Theorem 5}) and continuously differentiable (\textbf{Theorem 10})
	\item $\nabla v_{\eta,\mu}$ is locally $\widetilde{L}$-continuous when $R$ is 1-coercive (\textbf{Theorem 14})
	\item As $\mu \rightarrow 0$ (\textbf{Theorem 6}) or $\eta \rightarrow \infty$ (\textbf{Theorem 7}), cluster points of solutions to $\ouralgo$ are FOSPs for $\mathcal{FS-LME}$ 
\end{enumerate}
}
\end{frame}

\begin{frame}{$\mathcal{MSR}3$: Results}
    	\centering  
		\begin{figure}
			\includegraphics[width=0.7\textwidth]{Figures/benchmark_pgd_msr3.jpg}
		\end{figure}
\end{frame}

\begin{frame}{$\mathcal{MSR}3$: Algorithm}

\begin{algorithm}[H]
\TitleOfAlgo{PGD for $\mathcal{MSR}3$}
\SetAlgoLined
$\tbeta^+ \leftarrow \tbeta_0, \quad \tgamma^+ \leftarrow \tgamma_0, \quad \alpha \leftarrow 1/\eta, \quad \eta > \bar{\eta}$ \tcp*[f]{Initialization}\\
$\tw^+ := [\tbeta^+, \tgamma^+], \quad x^+ := [\beta,\gamma]$ \\	
\While{\text{making progress in $\tw$}}{
	\text{$x^+ \leftarrow$ \textcolor{red}{IP solution on $\LL_{\eta,\mu}(x^+, \tw^+)$ s.t. $x^+ \in \CC$}} \tcp*[f]{IP Iterations}\\	
	$\nabla_\tw v_{\eta,0}(\tw^+) \leftarrow  \nabla_\tw\LL_{\eta,0}(x^+, \tw^+)$ \tcp*[f]{Evaluate Gradient} \\ 
	$\tw^+ \leftarrow \prox_{\alpha^{-1}R + \delta_{\mathcal{C}}}(\tw^+ - \alpha \nabla_\tw v_{\eta,0}(\tw^+))$	\tcp*[f]{PGD on Value Function}\\
}
\Return{$\tw^+ = [\tbeta^+, \tgamma^+]$}
\end{algorithm}
\vspace{2em}
\only<2>{
\textbf{Key Observation:} $\nabla_\tw v(\tw)$ does not need to be evaluated exactly. We only need to come close enough to the central path.
}

\end{frame}


\begin{frame}{$\mathcal{MSR}3$-fast: Algorithm}
\begin{algorithm}[H]
\TitleOfAlgo{$\mathcal{MSR}3$-fast}
\SetAlgoLined
$\tbeta^+ \leftarrow \tbeta_0, \quad \tgamma^+ \leftarrow \tgamma_0, \quad \alpha \leftarrow 1/\eta, \quad \eta > \bar{\eta}$ \tcp*[f]{Initialization}\\
$\tw^+ := [\tbeta^+, \tgamma^+], \quad x^+ := [\beta,\gamma]$ \\	
\While{\text{making progress}}{
	\While{\textcolor{red}{not close enough to the central path}}{
		\text{$x^+ \leftarrow$ IP iteration on $\LL_{\eta,\mu}(x^+, \tw^+)$ s.t. $x^+ \in \CC$} \tcp*[f]{IP Iterations}\\	
	}
	\text{\textcolor{red}{Decrease $\mu$}}\\
	$\nabla_\tw v_{\eta,\mu}(\tw^+) \leftarrow  \nabla_\tw\LL_{\eta,\mu}(x^+, \tw^+)$ \tcp*[f]{Evaluate Gradient} \\ 
	$\tw^+ \leftarrow \prox_{\alpha^{-1}R + \delta_{\mathcal{C}}}(\tw^+ - \alpha \nabla_\tw v_{\eta,\mu}(\tw^+))$	\tcp*[f]{PGD on Value Function}\\
}
\Return{$\tw^+ = [\tbeta^+, \tgamma^+]$}
\end{algorithm}
\end{frame}


\section{Experiments}
\subsection{Application to Synthetic Problems}

\begin{frame}{$\ouralgo$-fast: Results}
\begin{itemize}
	\item The number of fixed effects $p$ and random effects $q$ is 20.
	\item $\beta = \gamma = \frac{1}{2}[1,2,3,\dots,10, 0\dots,0]$
	\item 9 groups with sizes [10, 15, 4, 8, 3, 5, 18, 9, 6]
	\item $X_i \sim \NN(0, I)^p$, $Z_i = X_i$, $\varepsilon_i \sim \NN(0, 0.3^2I)$
	\item Each experiment is repeated 100 times.
	\item Grid-search for $\eta \in [10^{-4}, 10^{2}]$, golden search for $\lambda \in [0, 10^5]$
	\item Final model is chosen to maximize BIC
\end{itemize}
\begin{figure}
	\includegraphics[width=0.7\textwidth]{Figures/benchmark.jpg}
\end{figure}
\begin{itemize}
	\item[\textcolor{green}{+}] $\ouralgo$-relaxation improves feature selection performance of the original likelihood.
	\item[\textcolor{green}{+}] $\ouralgo$-fast optimization accelerates the compute time by $\sim 10^2$.
   	\item[\textcolor{red}{--}] Initialization of $\eta$ is problem-specific
\end{itemize}
\end{frame}

\begin{frame}{Comparison to Other Libraries}
\begin{table}
	\input{../extras/table_glmm_current.tex}
\end{table}
\end{frame}

\begin{frame}{Choice of $\eta$}
	\begin{figure}
		\includegraphics[width=\textwidth]{Figures/eta_L1.pdf}
	\end{figure}
\end{frame}

\begin{frame}{$\ell_0$-based Covariate Selection for Bullying Study from GBD}
	\begin{figure}
		\includegraphics[width=\textwidth]{Figures/bullying_data_assessment_selection.pdf}
		\caption{Fixed and random covariate selection for Bullying dataset\footnote{Institute for Health Metrics and Evaluation (IHME). Bullying Victimization Relative Risk Bundle GBD 2020. Seattle, United States of America (USA), 2021.}. The model selected 9 covariates, 7 of which were historically significant, and did not select 4 covariates, 1 of which was historically significant.}
	\end{figure}
\end{frame}

\begin{frame}{Software}
	\includegraphics[width=\textwidth]{Figures/pysr3_screenshot.png}
	The code is available on GitHub: \href{github.com/aksholokhov/pysr3}{https://github.com/aksholokhov/pysr3}\footfullcite{sholokhov2023pysr3}
	\begin{itemize}
		\item All estimators are fully compatible to \texttt{scikit-learn} library.
		\item Implements SR3 for linear, generalized-linear, and linear mixed-effect models.
		\item Has tutorials, tests, and documentation.
	\end{itemize}
\end{frame}

\section{PINODE}

\subsection{Intro to Physics-Informed Reduced-Order Models (ROMs)}

\begin{frame}{Data-Driven Modeling of Physical Systems}

1) People used to model physical systems with first-principle knowledge
2) Data-Driven modelling of dynamical systems became a big thing  
3) However, it requires a lot of data 
4) Incorporating prior knowledge is a big recent trend, so history does a spiral

\end{frame}


\begin{frame}{Incorporating Knowledge into Models}
1) There are multiple ways of incorporating knowledge into system 
4) The overall umbrella term for it is physics-informed machine learning
2) Some use the equations that model phenomena
3) Some take aspects of it, e.g. symmetries and preservation laws, and forces
A network to respect those
5) Our work falls into the first category of approaches
\end{frame}


\begin{frame}{Reduced-Order Models (ROMs)}
\includegraphics[width=\textwidth]{Figures/roms_1}
\end{frame}

\begin{frame}{Reduced-Order Models (ROMs)}
\includegraphics[width=\textwidth]{Figures/roms_2}
\end{frame}

\begin{frame}{Reduced-Order Models (ROMs)}
\includegraphics[width=\textwidth]{Figures/roms_3}
\end{frame}

\begin{frame}{Reduced-Order Models (ROMs)}
\includegraphics[width=\textwidth]{Figures/roms_4}
\end{frame}

\begin{frame}{Physics-Informed Loss}
%1) We introduce physics to this system by adding a term which regularizes latent gradient field. 
%2) In particular, it forces it to be equal to what a true physics should be under such projection.
%3) We can not evaluate physics everywhere but we can at particular carefully-selected points. We call these points collocation points.
%4) We feed a lot of collocation points and ask a network to do interpolation 
\includegraphics[width=\textwidth]{Figures/collocations_1.png}
\end{frame}

\begin{frame}{Physics-Informed Loss}
	\includegraphics[width=\textwidth]{Figures/collocations_2.png}
\end{frame}

\begin{frame}{Physics-Informed Loss}
	\includegraphics[width=\textwidth]{Figures/collocations_3.png}
\end{frame}

\begin{frame}{Physics-Informed Loss}
	\includegraphics[width=\textwidth]{Figures/collocations_4.png}
\end{frame}

\begin{frame}{Physics-Informed Loss}
	\includegraphics[width=\textwidth]{Figures/collocations_5.png}
\end{frame}


\begin{frame}{Results: Extrapolation to Unknown Regions}
% 1) We show that network can indeed interpolate between collocations. Moreover, it can fill the whole unknown regimes of behavior. (Duffing example with explanation)
Duffing Oscillator on a low-dimensional (2D) manifold:
\begin{equation}
    \label{eq:duffing_definition}
    \begin{split}
    \frac{dz_1}{dt} & = z_2 \\ 
    \frac{dz_2}{dt} & = z_1 - z_1^3
    \end{split}
\end{equation}

Projection to a high-dimensional (128) space:
\begin{equation}
    \label{eq:duffing_true_decoder}
    \bd{x} := \mathcal{A}(\bd{z}) = A\bd{z}^3, \quad A \in \mathbb{R}^{128 \times 2}, \quad A_{ij} \sim_{i.i.d.} \mathcal{N}(0, 1)
\end{equation}

\begin{figure}
	\centering
	\includegraphics[width=0.8\textwidth]{Figures/duff_results.png}
\end{figure}

\end{frame}

\begin{frame}{Results: Stable Long-Term Predictions}
\begin{figure}
	\begin{subfigure}[b]{0.3\textwidth}
		\includegraphics[width=\textwidth]{Figures/duff_full_train.pdf}
	\end{subfigure}%
	\begin{subfigure}[b]{0.7\textwidth}
		\centering
		\includegraphics[width=0.7\textwidth]{Figures/duffing_periods.pdf}
	\end{subfigure}
\end{figure}
\begin{figure}
	\includegraphics[width=0.7\textwidth]{Figures/duffing_periods_examples.png}
\end{figure}
\end{frame}

\begin{frame}{Results: Learning From Collocations}
1) Finally we show that collocations can be even more useful than the data itself.
2) The difference is especially prominent in low-data regime.
3) It shows that collocations are powerful source of information and that the network can indeed interpolate between them.
\end{frame}

\begin{frame}{Single-Pixel Imaging}
%1) Now we switch gears to show you an application to single-pixel imaging
%2) It shows how powerful these differentiable reduced-order models can be in applications besides forecasting and motivate the need to improve those.
%3) SPI setup is a single-pixel camera with mirror array… 
\begin{figure}
	\includegraphics[width=\textwidth]{Figures/SPI_setup.pdf}
\end{figure}
\end{frame}

\begin{frame}{Single-Pixel Imaging}
	% This is how SPI setup translates into Math
	\includegraphics[width=\textwidth]{Figures/spi_math.png}
\end{frame}


\begin{frame}{Compressive Sensing with Reduced-Order Models}
	%1) We introduce a ROM as a relaxation of a PDE-constrained compressive sensing.
	%2) We search in the latent space of a ROM for the right trajectory that matches both the ROMs predictions and the compressive-sensing observations
	\includegraphics[width=\textwidth]{Figures/cs_schematics.png}
\end{frame}

\begin{frame}{Results: Burger's Equation}
	When we capture 32 samples per frame:
	\includegraphics[width=\textwidth]{Figures/cs_burgers_comparison_32.pdf}
\end{frame}

\begin{frame}{Results: Burger's Equation}
	When we capture 32 samples per frame:
	\includegraphics[width=\textwidth]{Figures/cs_burgers_comparison_32.pdf}
	
	When we capture 2 samples per frame:
	\includegraphics[width=\textwidth]{Figures/cs_burgers_comparison_2.pdf}
\end{frame}

\begin{frame}{Results: Burger's Equation}
	Aggregated results
\end{frame}

\begin{frame}{Results: Interpretation}
	
\end{frame}

\begin{frame}{Results: Kolmogorov Flow OR Real Example}
\end{frame}

\begin{frame}{Conclusion}
	Results on Burgers
	Maybe results on a harder problem
\end{frame}

\appendix
\begin{frame}{Designing an Algorithm}
	$G_{\nu,\eta}$ encodes both gradient of a Lagrangian (lines 1-2) and the complementarity condition (line 3):
	\eq{
		G_{\nu,\eta}((\beta,\gamma,v),(\tbeta,\tgamma)) := \begin{bmatrix}
			\nabla_\beta \LL(\beta, \gamma) + \eta(\beta-\tbeta) \\
			\nabla_\gamma \LL(\beta, \gamma) + \eta(\gamma-\tgamma) - v\\
			v \bigodot \gamma - \mu\textbf{1}
		\end{bmatrix}
	}
	We apply Newton method to $G$ while geometrically decreasing $\mu$. \\
\textbf{Lemma:} For every $(\mu,\eta) \in \R_+\times\R_{++}$,
\eq{
	&(\hat\beta,\hat\gamma) = \argmin_{(\beta,\gamma)}\LL_{\eta,\mu}((\beta, \gamma),(\tbeta, \tgamma)) \\
	& \iff \\
	& \exists \hat{v} \in \R_{+}^q \text{ s.t. } G_{\nu,\eta}((\beta,\gamma,\hat{v}),(\tbeta,\tgamma)) = 0
}
If $\mu > 0$, then $\hat{v} = -\nabla\phi_\mu(\hat{\gamma})$, and if $\mu = 0$, then $\hat{v}$ is the unique KKT multiplier associated with the constraint $0 \leq \gamma$.
\end{frame}

\begin{frame}{$\ouralgo$-fast Algorithm}
\input{../extras/08Pseudocode}
\end{frame}

\begin{frame}{References}
\printbibliography
\end{frame}

\end{document}