\documentclass[12pt,a4paper]{book}

% PRESETS OF PACKAGES AND MATH %
\input{latex-macros/useful_packages}
\input{math_macros-jim}

\usepackage{natbib}
% Color 
\newcommand{\blue}[1]{\textcolor{blue}{#1}}
\newcommand{\red}[1]{\textcolor{red}{#1}}
\newcommand{\plum}[1]{\textcolor{Plum}{#1}}
\newcommand{\teal}[1]{\textcolor{teal}{#1}}


%%%%% USER CONFIGURATION %%%%%
\newcommand{\userName}{Aleksei Sholokhov}
\newcommand{\userId}{aksh}
\newcommand{\department}{Department of Applied Mathematics}
\newcommand{\institution}{University of Washington}
\newcommand{\projectNameShort}{}
\newcommand{\doctitle}{Doctoral Thesis}
\newcommand{\projectNameLong}{Optimization Methods for Parameter Identifications \\ in Settings with Only Partial Knowledge}

% Cast
\newcommand{\Peng}[1][]{\textbf{Peng\ifthenelse{\equal{#1}{}}{ }{ (#1):}}}
\newcommand{\Sasha}[1][]{\textbf{Sasha\ifthenelse{\equal{#1}{}}{ }{ (#1):}}}
\newcommand{\Lesha}[1][]{\textbf{\userId\ifthenelse{\equal{#1}{}}{ }{ (#1):}}}
\newcommand{\Brad}[1][]{\textbf{Brad\ifthenelse{\equal{#1}{}}{ }{ (#1):}}}
\newcommand{\Jim}[1][]{\textbf{Jim\ifthenelse{\equal{#1}{}}{ }{ (#1):}}}
\newcommand{\ouralgo}{MSR3 }

% Commands which are specific for this document 
\DeclareMathOperator{\bBeta}{\bar{\beta}}
\DeclareMathOperator{\bgamma}{\bar{\gamma}}
\DeclareMathOperator{\bSigma}{\bar{\Sigma}}

%%%%%%%%%%%%%%%
% PAGE FORMAT %
%%%%%%%%%%%%%%%

\pagestyle{fancy}
\setlength\parindent{0in}
\setlength\parskip{0.1in}
\setlength\headheight{15pt}
\linespread{1.18}

%%%%%%%%%%% HEADER / FOOTER %%%%%%%%%%%
\chead{\textit{\projectNameShort}}
\lhead{\textsc{\userName}}
\rhead{\textsc{\doctitle}}
\rfoot{}
\cfoot{\color{gray} \textsc{\thepage~/~\pageref*{LastPage}}}
\lfoot{}

% University Logo
%\newcommand{\univlogo}{%
%  \noindent % University Logo
%  \begin{wrapfigure}{r}{0.25\textwidth}
%    \vspace{-24pt}
%    \begin{center}
%      \includegraphics[width=0.25\textwidth]{Images/univ-logo.jpg}
%    \end{center}
%    \vspace{-10pt}
%  \end{wrapfigure}
%}

%\renewcommand{\thesection}{\Roman{section}}
% \renewcommand{\thesubsection}{\thesection.\Roman{subsection}}

%%%%%%%%%%%%
% CAPTIONS %
%%%%%%%%%%%%

%%%% Paragraph separation
%\setlength{\parskip}{.5em}

\numberwithin{equation}{section} % Number equations within sections (i.e. 1.1, 1.2, 2.1, 2.2 instead of 1, 2, 3, 4)
\numberwithin{figure}{section} % Number figures within sections (i.e. 1.1, 1.2, 2.1, 2.2 instead of 1, 2, 3, 4)
\numberwithin{table}{section} % Number tables within sections (i.e. 1.1, 1.2, 2.1, 2.2 instead of 1, 2, 3, 4)

%%%%%%%%%%%%
% DOCUMENT %
%%%%%%%%%%%%

\begin{document}

\title{\doctitle}
~\\
{\center
{\large 
{\large
\projectNameLong
}
\\[10mm]
\userName
\\[20mm]
{A dissertation \\
 submitted in partial fulfillment of the\\ requirements for the degree of}
\\[5mm]
{Doctor of Philosophy}
\\[20mm]
{\center
\textbf{Reading Committee}\\
Aleksandr Aravkin (co-chair),\\
J. Nathan Kutz (co-chair), \\
James Burke, \\
Jonathan Wakefield (GSR)
}
\\[20mm]
Program Authorized to Offer Degree:
\\[5mm]
Applied Mathematics
\\[5mm]
\institution
}
\\[10mm]
Â©Copyright 2023\\
\userName

}  

\newpage
{
\center 
\institution
\\[10mm]
\textbf{Abstract}
\\[10mm]
\projectNameLong
\\[10mm]
\userName
\\[10mm]
Chairs of the Supervisory Committee:\\
Aleksandr Aravkin and J. Nathan Kutz\\
\department
\\[10mm]
}

This work summarizes two projects focused on incorporating prior knowledge into machine learning models. In the first project we developed universal feature selection methods for linear mixed-effect models. Namely, we extended Sparse Relaxed Regularized Regression (SR3) to Linear Mixed-Effect (LME) likelihood and showed how one can minimize such likelihood with a proximal gradient descent. We also develop theoretical underpinnings of the proposed extension, including consistency results, variational properties, implementability of optimization methods, and convergence results. In particular we provide convergence analyses for a basic implementation of SR3 for LME (called MSR3) and an accelerated hybrid algorithm (called MSR3-fast). Numerical results show the utility and speed of these algorithms on realistic simulated datasets. Finally, we provide an open-source implementation of both algorithms in an open source python package \texttt{pysr3}. This package offers complete compatibility with \texttt{scikit-learn}, so all \texttt{pysr3} models can be used in pipeline with classic modelling blocks such as data pre-processors, randomized grid search, cross-validation, and quality metrics.

The second line of work develops a framework for training Reduced-Order Models (ROMs) with Physics-Informed Neural Ordinary Differential Equations (PINODE). Our innovation builds on ideas from classical collocation methods of numerical analysis and illustrates how one can use collocation points to transfer knowledge from a known equation to a model that approximates solutions of that equation. We show that the addition of our physics-informed loss allows for exceptional data supply strategies that improves the performance of ROMs in data-scarce settings, where training high-quality data-driven models is impossible. The resulting ROMs are able to extrapolate forward in time considerably more effectively,  perform better for unseen initial conditions, and exhibit less sensitivity to noise. Finally, we show how such ROMs can be used as strong regularizers in single-pixel imaging (SPI) allowing to reduce data-intake by an order of magnitude relatively to current state-of-the-art algorithms.

\tableofcontents

\listoftodos

\chapter{Introduction}
\include{introduction}

\chapter{MSR3: Sparse Relaxed Regularized Regression for Mixed-Effect Models}
\label{ch:msr3}
\include{msr3}

\chapter{PINODE: Physics-Informed Neural Ordinary Differential Equations}
\label{ch:pinode}
\include{pinode}

\input{cs}

\chapter{Conclusion}
\include{conclusion}

\bibliographystyle{apalike}
\bibliography{bibliography}

\clearpage

\appendix
\chapter{Appendix}
\section{Acknowledgement}
\input{appendix/acknowledgements}

\section{Derivatives of Marginalized Log-likelihood for Linear Mixed Models}
\label{appendix:derivatives_of_lmm}
\input{appendix/derivatives_of_lmms}

\section{Derivation of Selected Proximal Operators from Table \ref{table:proxes}}
\label{appendix:proxes}
\input{appendix/proxes}

\section{Existence of Minimizers (Theorems~\ref{thm:basic existence} and~\ref{thm:basic existence2})}
\label{adx:basic existence}
\input{appendix/existence_of_solutions_for_lme}

\section{Lipschitz-constant for Likelihood of a Linear Mixed-Effects Model}
\label{appendix:lipschitz_constant}
\input{appendix/lipschitz_constant}

\section{Detailed Results from Simulation from Table \ref{table:comparison_of_algorithms}}
\label{appendix:detailed_comparison}
\begin{table}[t]
    \centering
    \resizebox{\columnwidth}{!}{\input{tables/performance_table_long_current}}
    \caption{Comparison of performance of algorithms}
    \label{table:detailed_comparison_of_algorithms}
\end{table}

\section{Description of Real-World Datasets}
\subsection{GBD Bullying Data}
\input{appendix/bullying_features_description}

\subsection{COVID-19 Contact Rate Forecasting Data}
\input{tables/covid_groups_description}
\input{tables/covid_coefficients}

\section{Detailed Design of Experiments for PINODE}
\input{appendix/pinode_appendix}

\end{document}
