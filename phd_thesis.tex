\documentclass[12pt,a4paper]{book}

% PRESETS OF PACKAGES AND MATH %
\input{latex-macros/useful_packages}
\input{math_macros-jim}

\usepackage{natbib}
% Color 
\newcommand{\blue}[1]{\textcolor{blue}{#1}}
\newcommand{\red}[1]{\textcolor{red}{#1}}
\newcommand{\plum}[1]{\textcolor{Plum}{#1}}
\newcommand{\teal}[1]{\textcolor{teal}{#1}}


%%%%% USER CONFIGURATION %%%%%
\newcommand{\userName}{Aleksei Sholokhov}
\newcommand{\userId}{aksh}
\newcommand{\department}{Department of Applied Mathematics}
\newcommand{\institution}{University of Washington}
\newcommand{\projectNameShort}{}
\newcommand{\doctitle}{Doctoral Thesis}
\newcommand{\projectNameLong}{Optimization Methods for Parameter Identifications \\ in Settings with Only Partial Knowledge}

% Cast
\newcommand{\Peng}[1][]{\textbf{Peng\ifthenelse{\equal{#1}{}}{ }{ (#1):}}}
\newcommand{\Sasha}[1][]{\textbf{Sasha\ifthenelse{\equal{#1}{}}{ }{ (#1):}}}
\newcommand{\Lesha}[1][]{\textbf{\userId\ifthenelse{\equal{#1}{}}{ }{ (#1):}}}
\newcommand{\Brad}[1][]{\textbf{Brad\ifthenelse{\equal{#1}{}}{ }{ (#1):}}}
\newcommand{\Jim}[1][]{\textbf{Jim\ifthenelse{\equal{#1}{}}{ }{ (#1):}}}
\newcommand{\ouralgo}{MSR3 }

% Commands which are specific for this document 
\DeclareMathOperator{\bBeta}{\bar{\beta}}
\DeclareMathOperator{\bgamma}{\bar{\gamma}}
\DeclareMathOperator{\bSigma}{\bar{\Sigma}}

%%%%%%%%%%%%%%%
% PAGE FORMAT %
%%%%%%%%%%%%%%%

\pagestyle{fancy}
\setlength\parindent{0in}
\setlength\parskip{0.1in}
\setlength\headheight{15pt}
\linespread{1.18}

%%%%%%%%%%% HEADER / FOOTER %%%%%%%%%%%
\chead{\textit{\projectNameShort}}
\lhead{\textsc{\userName}}
\rhead{\textsc{\doctitle}}
\rfoot{}
\cfoot{\color{gray} \textsc{\thepage~/~\pageref*{LastPage}}}
\lfoot{}

% University Logo
%\newcommand{\univlogo}{%
%  \noindent % University Logo
%  \begin{wrapfigure}{r}{0.25\textwidth}
%    \vspace{-24pt}
%    \begin{center}
%      \includegraphics[width=0.25\textwidth]{Images/univ-logo.jpg}
%    \end{center}
%    \vspace{-10pt}
%  \end{wrapfigure}
%}

%\renewcommand{\thesection}{\Roman{section}}
% \renewcommand{\thesubsection}{\thesection.\Roman{subsection}}

%%%%%%%%%%%%
% CAPTIONS %
%%%%%%%%%%%%

%%%% Paragraph separation
%\setlength{\parskip}{.5em}

\numberwithin{equation}{section} % Number equations within sections (i.e. 1.1, 1.2, 2.1, 2.2 instead of 1, 2, 3, 4)
\numberwithin{figure}{section} % Number figures within sections (i.e. 1.1, 1.2, 2.1, 2.2 instead of 1, 2, 3, 4)
\numberwithin{table}{section} % Number tables within sections (i.e. 1.1, 1.2, 2.1, 2.2 instead of 1, 2, 3, 4)

%%%%%%%%%%%%
% DOCUMENT %
%%%%%%%%%%%%

\begin{document}
\thispagestyle{empty}
\title{\doctitle}
~\\
{\center
{\large 
{\large
\projectNameLong
}
\\[10mm]
\userName
\\[15mm]
{A dissertation \\
 submitted in partial fulfillment of the\\ requirements for the degree of}
\\[10mm]
{Doctor of Philosophy}
\\[10mm]
\institution
\\[5mm]
2023
\\[15mm]
{\center
Reading Committee:\\
Aleksandr Aravkin (chair),\\
Jose Kutz (chair), \\
James Burke
}
\\[15mm]
Program Authorized to Offer Degree:
\\[5mm]
Applied Mathematics
\\[5mm]
}
}  

\newpage
\thispagestyle{empty}
{
\center

Â©Copyright 2023\\
\userName

}

\newpage
\thispagestyle{empty}
{
\center 
\institution
\\[10mm]
\textbf{Abstract}
\\[10mm]
\projectNameLong
\\[10mm]
\userName
\\[10mm]
Chairs of the Supervisory Committee:\\
Aleksandr Aravkin and J. Nathan Kutz\\
\department
\\[10mm]
}

This work summarizes two projects focused on incorporating prior knowledge into machine learning models. In the first project, a universal feature selection method for linear mixed-effect models is developed. Namely, Sparse Relaxed Regularized Regression (SR3) is extended to the case of Linear Mixed-Effect (LME) likelihoods, and we prove that one can minimize such likelihoods with proximal gradient descent. Theoretical underpinnings of the proposed extension are also presented, including consistency results, variational properties, implementability of optimization methods, and convergence results. In particular, convergence analyses are provided for a basic implementation of SR3 for LME and an accelerated hybrid algorithm. Numerical results show the utility and speed of these algorithms on real and simulated datasets. Finally, both algorithms are implemented in an open-source python package \texttt{pysr3}. Conveniently, this package offers complete compatibility with \texttt{scikit-learn}, so all \texttt{pysr3} models can be used in a pipeline with classic modeling blocks such as data pre-processors, randomized grid search, cross-validation, and quality metrics. The second line of work develops a framework for training Reduced-Order Models (ROMs) with Physics-Informed Neural Ordinary Differential Equations (PINODE). In particular, a classic technique of collocation points is adapted to transfer knowledge from a known equation to a model that approximates solutions of that equation. The addition of a physics-informed loss allows for exceptional data supply strategies that improve the performance of ROMs in data-scarce settings, where training high-quality data-driven models is impossible. The resulting ROMs extrapolate forward in time more accurately, perform better for unseen initial conditions, and exhibit less sensitivity to noise. Finally, I show how such ROMs can be used as strong regularizers in single-pixel imaging (SPI), enabling the reduction of samples-per-frame rate by an order of magnitude relative to other state-of-the-art algorithms.

\newpage
\tableofcontents

\chapter*{Introduction}
\include{introduction}

\chapter{MSR3: Sparse Relaxed Regularized Regression for Mixed-Effect Models}
\label{ch:msr3}
\include{msr3}

\addtocontents{toc}{\protect\newpage}
\chapter{PINODE: Physics-Informed Neural Ordinary Differential Equations}
\label{ch:pinode}
\include{pinode}

\input{cs}

\chapter*{Conclusion}
\include{conclusion}

\bibliographystyle{apalike}
\bibliography{bibliography}

\clearpage

\addtocontents{toc}{\protect\newpage}

\appendix
\chapter{MSR3}

\section*{Acknowledgement}
\input{appendix/acknowledgements}

\section{Derivatives of Marginalized Log-likelihood for Linear Mixed Models}
\label{appendix:derivatives_of_lmm}
\input{appendix/derivatives_of_lmms}

\section{Derivation of Selected Proximal Operators from Table \ref{table:proxes}}
\label{appendix:proxes}
\input{appendix/proxes}

\section{Existence of Minimizers (Theorems~\ref{thm:basic existence} and~\ref{thm:basic existence2})}
\label{adx:basic existence}
\input{appendix/existence_of_solutions_for_lme}

\section{Lipschitz-constant for Likelihood of a Linear Mixed-Effects Model}
\label{appendix:lipschitz_constant}
\input{appendix/lipschitz_constant}

\section{Detailed Results from Simulation from Table \ref{table:comparison_of_algorithms}}
\label{appendix:detailed_comparison}
\begin{table}[t]
    \centering
    \resizebox{\columnwidth}{!}{\input{tables/performance_table_long_current}}
    \caption{Comparison of performance of algorithms}
    \label{table:detailed_comparison_of_algorithms}
\end{table}

\section{Description of Real-World Datasets}
\subsection{GBD Bullying Data}
\input{appendix/bullying_features_description}

\subsection{COVID-19 Contact Rate Forecasting Data}
\input{tables/covid_groups_description}
\input{tables/covid_coefficients}

\chapter{PINODE}
\section{Detailed Design of Experiments for PINODE}
\input{appendix/pinode_appendix}

\end{document}
