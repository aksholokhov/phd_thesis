In this work I gave two examples of how partial knowledge can be utilized to improve performance of machine learning models. First, in Section~\ref{sec:MSR3} we showed how complex priors can be embedded into a linear mixed-effects model via a suitable proximal operator, and how the resulting algorithm can be implemented using a proximal gradient descent. We note that a complex combination of priors often yields ill-conditioned problems, and we propose a MSR3 relaxation to address it. We also prove that a solution of such relaxation approaches the solution of the original problem as the relaxation tightens. Next, we showed how partial knowledge of physics can be embedded into deep-learning-based reduced-order models. Namely, we show how to project known physical equations that govern behavior of the system in an observable space into a chosen latent space. In that space one can require that the gradient field should match the projected true dynamics. We show that this methodology leads to identification of more stable latent manifolds, and using such manifolds for forecasting leads to more accurate predictions and lesser sensitivity to noise. We also showed how the reduced-order models described above can be used for compressive sensing. We demonstrated that they can accurately reconstruct the dynamics from partial observations, significantly surpassing current state-of-the-art compressive-sensing methods. We hope that these developments pave the way to higher-quality machine learning models which effectively marry data and the humanity's hard-won expert knowledge. 

\paragraph{Perspective Research Directions} Due to the limited time and resources we left many promising directions for future exploration. One such direction is extending sparse relaxed regularized regression (SR3) to generalized linear and generalized mixed-effects models. The fundamental difficulty that arises in such extension is the absence of closed-form marginal log-likelihoods. It implies that one would have three nested levels of optimization when they apply an SR3 relaxation on top of such likelihood: evaluating the likelihood, evaluating a value function, and optimizing over the value function. Re-using the recipe that we developed in Section~\ref{sec:MSR3} could help to surmount such obstacle. For example, Algorithm~\ref{alg:pgd_for_value} had two nested optimization methods: for evaluating the value function and for optimizing over it. However we showed that one can ``blend'' both levels of optimization and achieve superior performance results. The same methodology of ``blending'' could possibly be applied to more than two nested optimization loops to yield algorithms that, in certain scenarios, work faster than non-nested optimization methods.

	Our development of physics-informed reduced-order models also invites future work in several promising directions. First, a more systematic way of choosing collocation points would greatly benefit the method. In Section~\ref{sec:collocations_conditions} we define collocation points and provide criteria for a good choice of a family of colocation points. However, we ultimately leave the reader without a constructive algorithm for identifying candidate families of collocation points suitable for their problem. At the same time, certain differential equations admit families of basis functions that can span their solutions. Such basis functions frequently poses easily-computable derivatives, which makes them perfect candidates for collocation points for PINODE. Bridging the gap between this new methodology and those classic results would undoubtedly yield a fruitful line of works. Finally, one could utilize PINODE models not only in compressive sensing applications but in any application where one needs to quickly search over a large space of possible simulations. These examples include medical imaging, model discovery, online control, fast simulations, and many others.

\paragraph{Acknowledgements} I would like to thank ... (TBD) 