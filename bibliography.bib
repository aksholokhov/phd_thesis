@article{Zheng2018RelaxAndSplit,
abstract = {We develop and analyze a new 'relax-and-split' (RS) approach for inverse problems modeled using nonsmooth nonconvex optimization formulations. RS uses a relaxation technique together with partial minimization, and brings classic techniques including direct factorization, matrix decompositions, and fast iterative methods to bear on nonsmooth nonconvex problems. We also extend the approach to robustify any such inverse problem through trimming, a mechanism that robustifies inverse problems to measurement outliers. We then show practical performance of RS and trimmed RS (TRS) on a diverse set of problems, including: (1) phase retrieval, (2) semi-supervised classification, (3) stochastic shortest path problems, and (4) nonconvex clustering. RS/TRS are easy to implement, competitive with existing methods, and show promising results on difficult inverse problems with nonsmooth and nonconvex features.},
author = {Zheng, Peng and Aravkin, Aleksandr},
doi = {10.1088/1361-6420/aba417},
file = {:Users/aksh/Documents/Papers/2020/Zheng, Aravkin/Relax-and-split method for nonconvex inverse problems Relax-and-split method for nonconvex/Zheng, Aravkin - 2020 - Relax-and-split method for nonconvex inverse problems Relax-and-split method for nonconvex.pdf:pdf},
issn = {13616420},
journal = {Inverse Problems},
keywords = {nonconvex optimization,nonsmooth models,splitting methods},
mendeley-groups = {Relax-and-Split examples},
number = {9},
publisher = {IOP Publishing},
title = {{Relax-and-split method for nonconvex inverse problems}},
volume = {36},
year = {2020}
}


@article{Kucukelbir2017,
abstract = {Derivatives, mostly in the form of gradients and Hessians, are ubiquitous in machine learning. Automatic differentiation (AD), also called algorithmic differentiation or simply "autodiff", is a family of techniques similar to but more general than backpropagation for efficiently and accurately evaluating derivatives of numeric functions expressed as computer programs. AD is a small but established field with applications in areas including computational fluid dynamics, atmospheric sciences, and engineering design optimization. Until very recently, the fields of machine learning and AD have largely been unaware of each other and, in some cases, have independently discovered each other's results. Despite its relevance, general-purpose AD has been missing from the machine learning toolbox, a situation slowly changing with its ongoing adoption under the names "dynamic computational graphs" and "differentiable programming". We survey the intersection of AD and machine learning, cover applications where AD has direct relevance, and address the main implementation techniques. By precisely defining the main differentiation techniques and their interrelationships, we aim to bring clarity to the usage of the terms "autodiff", "automatic differentiation", and "symbolic differentiation" as these are encountered more and more in machine learning settings.},
archivePrefix = {arXiv},
arxivId = {1502.05767},
author = {Kucukelbir, Alp and Tran, Dustin and Ranganath, Rajesh and Gelman, Andrew and Blei, David M.},
doi = {10.1016/j.advwatres.2018.01.009},
eprint = {1502.05767},
file = {:Users/aksh/Downloads/17-468.pdf:pdf},
isbn = {0002-9645 (Print)$\backslash$r0002-9645 (Linking)},
issn = {03091708},
journal = {Journal of Machine Learning},
mendeley-groups = {Algorithmic Differentiation},
number = {14},
pages = {1--45},
pmid = {7020497},
title = {{Automatic Differentiation in Machine Learning: a Survey Atılım}},
url = {http://jmlr.org/papers/v18/16-107.html},
volume = {18},
year = {2017}
}

@book{Shalev-Shwartz2014,
abstract = {Machine learning is one of the fastest growing areas of computer science, with far-reaching applications. The aim of this textbook is to introduce machine learning, and the algorithmic paradigms it offers, in a principled way. The book provides an extensive theoretical account of the fundamental ideas underlying machine learning and the mathematical derivations that transform these principles into practical algorithms. Following a presentation of the basics of the field, the book covers a wide array of central topics that have not been addressed by previous textbooks. These include a discussion of the computational complexity of learning and the concepts of convexity and stability; important algorithmic paradigms including stochastic gradient descent, neural networks, and structured output learning; and emerging theoretical concepts such as the PAC-Bayes approach and compression-based bounds. Designed for an advanced undergraduate or beginning graduate course, the text makes the fundamentals and algorithms of machine learning accessible to students and non-expert readers in statistics, computer science, mathematics, and engineering.},
address = {Cambridge},
author = {Shalev-Shwartz, Shai and Ben-David, Shai},
booktitle = {Understanding Machine Learning: From Theory to Algorithms},
doi = {10.1017/CBO9781107298019},
file = {:Users/aksh/Documents/Books/Машинное обучение и анализ данных /Про все и сразу/understanding-machine-learning-theory-algorithms.pdf:pdf},
isbn = {9781107298019},
mendeley-groups = {Essential Books},
pages = {1--397},
publisher = {Cambridge University Press},
title = {{Understanding Machine Learning}},
url = {http://ebooks.cambridge.org/ref/id/CBO9781107298019},
volume = {9781107057},
year = {2014}
}

@book{Hastie2017,
abstract = {During the past decade there has been an explosion in computation and information technology. With it has come a vast amount of data in a variety of fields such as medicine, biology, finance, and marketing. The challenge of understanding these data has led to the development of new tools in the field of statistics, and spawned new areas such as data mining, machine learning, and bioinformatics. Many of these tools have common underpinnings but are often expressed with different terminology. This book describes the important ideas in these areas in a common conceptual framework. While the approach is statistical, the emphasis is on concepts rather than mathematics.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Hastie, Trevor and Tibshirani, Robert},
booktitle = {Math. Intell.},
doi = {111},
eprint = {arXiv:1011.1669v3},
isbn = {978-0-387-84857-0},
issn = {03436993},
mendeley-groups = {Essential Books},
pmid = {15512507},
title = {{The Elements of Statistical Learning Second Edition}},
year = {2017}
}

@article{Moody1992,
author = {Moody, John E},
mendeley-groups = {Other},
number = {1},
pages = {847--854},
title = {{The Effective Number of Parameters : An Analysis of Generalization and Regularization in Nonlinear Learning Systems}}
}

@incollection{Tipping2004,
author = {Tipping, Michael E.},
doi = {10.1007/978-3-540-28650-9_3},
file = {:Users/aksh/Downloads/bayesian{\_}regression{\_}overview.pdf:pdf},
mendeley-groups = {Bayesian Approach},
pages = {41--62},
title = {{Bayesian Inference: An Introduction to Principles and Practice in Machine Learning}},
url = {http://link.springer.com/10.1007/978-3-540-28650-9{\_}3},
year = {2004}
}

@article{Fox2006,
abstract = {Statistics lectures have been a source of much bewilderment and frustration for generations of students. This book attempts to remedy the situation by expounding a logical and unified approach to the whole subject of data analysis. This text is intended as a tutorial guide for senior undergraduates and research students in science and engineering. After explaining the basic principles of Bayesian probability theory, their use is illustrated with a variety of examples ranging from elementary parameter estimation to image processing. Other topics covered include reliability analysis, multivariate optimization, least-squares and maximum likelihood, error-propagation, hypothesis testing, maximum entropy and experimental design. The Second Edition of this successful tutorial book contains a new chapter on extensions to the ubiquitous least-squares procedure, allowing for the straightforward handling of outliers and unknown correlated noise, and a cutting-edge contribution from John Skilling on a novel numerical technique for Bayesian computation called 'nested sampling'.},
author = {Fox, Eric P. and Sivia, D. S.},
doi = {10.2307/1270652},
issn = {00401706},
journal = {Technometrics},
mendeley-groups = {Bayesian Approach},
title = {{Data Analysis: A Bayesian Tutorial}},
year = {2006}
}

@article{Zheng2019TrimmedMixedModels,
author = {Zheng, Peng and Aravkin, Aleksandr and Barber, Ryan and Sorensen, Reed and Murray, Christopher},
file = {:Users/aksh/Documents/Papers/Zheng et al/2019/Unknown/Zheng et al. - 2019 - Trimmed Constrained Mixed Effects Models Formulations and Algorithms.pdf:pdf},
keywords = {meta-analysis,mixed effects models,nonsmooth nonconvex optimization,trimming},
mendeley-groups = {Mixed Effect Models},
pages = {1--15},
title = {{Trimmed Constrained Mixed Effects Models : Formulations and Algorithms}},
year = {2019}
}

@article{Mosegaard1995SamplingFromPosterior,
abstract = {Probabilistic formulation of inverse problems leads to the definition of a probability distribution in the model space. This probability distribution combines a priori information with new information obtained by measuring some observable parameters (data). As, in the general case, the theory linking data with model parameters is nonlinear, the a posteriori probability in the model space may not be easy to describe (it may be multimodal, some moments may not be defined, etc.). When analysing an inverse problem, obtaining a maximum likelihood model is usually not sufficient, as we normally also wish to have information on the resolution power of the data. In the general case we may have a large number of model parameters, and an inspection of the marginal probability densities of interest may be impractical, or even useless. But it is possible to pseudorandomly generate a large collection of models according to the posterior probability distribution and to analyse and display the models in such a way that information on the relative likelihoods of model properties is conveyed to the spectator. This can be accomplished by means of an efficient Monte Carlo method, even in cases where no explicit formula for the a priori distribution is available. The most well known importance sampling method, the Metropolis algorithm, can be generalized, and this gives a method that allows analysis of (possibly highly nonlinear) inverse problems with complex a priori information and data with an arbitrary noise distribution.},
author = {Mosegaard, Klaus and Tarantola, Albert},
doi = {10.1029/94jb03097},
file = {:Users/aksh/Documents/Papers/Mosegaard, Tarantola/1995/Journal of Geophysical Research Solid Earth/Mosegaard, Tarantola - 1995 - Monte Carlo sampling of solutions to inverse problems.pdf:pdf},
journal = {Journal of Geophysical Research: Solid Earth},
keywords = {doi:10.1029/9,http://dx.doi.org/10.1029/94JB03097},
mendeley-groups = {Bayesian Approach,Monte Carlo Methods},
number = {B7},
pages = {12431--12447},
title = {{Monte Carlo sampling of solutions to inverse problems}},
volume = {100},
year = {1995}
}

@article{Murray2017MeasuringGlobalHealth,
abstract = {The 2014-2015 outbreak of Ebola virus (EBOV), originating from Guinea, is now responsible for the infection of {\textgreater} 20,000 people in 9 countries. Whereas past filovirus outbreaks in sub-Saharan Africa have been rapidly brought under control with comparably few cases, this outbreak has been particularly resistant to containment efforts. Both the general population and primary health care workers have been affected by this outbreak, with hundreds of doctors and nurses being infected in the line of duty. In the absence of approved therapeutics, several caregivers have turned to investigational new drugs as well as experimental therapies in an effort to save lives. This review aims to summarize the candidates currently under consideration for postexposure use in infected patients during the largest EBOV outbreak in history.},
author = {Murray, Christopher J.L. and Lopez, Alan D.},
doi = {10.1016/S0140-6736(17)32367-X},
file = {:Users/aksh/Documents/Papers/Murray, Lopez/2017/The Lancet/Murray, Lopez - 2017 - Measuring global health motivation and evolution of the Global Burden of Disease Study.pdf:pdf},
issn = {1474547X},
journal = {The Lancet},
mendeley-groups = {Epidemiology Modelling},
number = {10100},
pages = {1460--1464},
publisher = {Elsevier Ltd},
title = {{Measuring global health: motivation and evolution of the Global Burden of Disease Study}},
url = {http://dx.doi.org/10.1016/S0140-6736(17)32367-X},
volume = {390},
year = {2017}
}

@article{Polson2019BayesianRegularization,
abstract = {Bayesian regularization is a central tool in modern-day statistical and machine learning methods. Many applications involve high-dimensional sparse signal recovery problems. The goal of our paper is to provide a review of the literature on penalty-based regularization approaches, from Tikhonov (Ridge, Lasso) to horseshoe regularization.},
archivePrefix = {arXiv},
arxivId = {arXiv:1902.06269v1},
author = {Polson, Nicholas G. and Sokolov, Vadim},
doi = {10.1002/wics.1463},
eprint = {arXiv:1902.06269v1},
file = {:Users/aksh/Documents/Papers/Polson, Sokolov/2019/Wiley Interdisciplinary Reviews Computational Statistics/Polson, Sokolov - 2019 - Bayesian regularization From Tikhonov to horseshoe.pdf:pdf},
issn = {19390068},
journal = {Wiley Interdisciplinary Reviews: Computational Statistics},
keywords = {Bayesian regression,horseshoe,lasso,regularization},
mendeley-groups = {Bayesian Approach},
number = {4},
pages = {1--16},
title = {{Bayesian regularization: From Tikhonov to horseshoe}},
volume = {11},
year = {2019}
}

@article{Kuczera1998AssessmentOfParametersWithMCMC,
abstract = {Two Monte Carlo-based approaches for assessing parameter uncertainty in complex hydrologic models are considered. The first, known as importance sampling, has been implemented in the generalised likelihood uncertainty estimation (GLUE) framework of Beven and Binley. The second, known as the Metropolis algorithm, differs from importance sampling in that it uses a random walk that adapts to the true probability distribution describing parameter uncertainty. Three case studies are used to investigate and illustrate these Monte Carlo approaches. The first considers a simple water balance model for which exact results are known. It is shown that importance sampling is inferior to Metropolis sampling. Unless a large number of random samples are drawn, importance sampling can produce seriously misleading results. The second and third case studies consider more complex catchment models to illustrate the insights the Metropolis algorithm can offer. They demonstrate assessment of parameter uncertainty in the presence of bimodality, evaluation of the significance of split-sample tests, use of prior information and the assessment of confidence limits on hydrologic responses not used in calibration. When compared with the capabilities of traditional inference based on first-order approximation, the Metropolis algorithm provides a quantum advance in our capability to deal with parameter uncertainty in hydrologic models.},
author = {Kuczera, George and Parent, Eric},
doi = {10.1016/S0022-1694(98)00198-X},
file = {:Users/aksh/Documents/Papers/Kuczera, Parent/1998/Journal of Hydrology/Kuczera, Parent - 1998 - Monte Carlo assessment of parameter uncertainty in conceptual catchment models The Metropolis algorithm.pdf:pdf},
issn = {00221694},
journal = {Journal of Hydrology},
keywords = {Bayesian inference,Conceptual catchment models,Importance sampling,Markov Chain Monte Carlo sampling,Parameter uncertainty,Rainfall-runoff models},
mendeley-groups = {Monte Carlo Methods},
number = {1-4},
pages = {69--85},
title = {{Monte Carlo assessment of parameter uncertainty in conceptual catchment models: The Metropolis algorithm}},
volume = {211},
year = {1998}
}

@article{Carlin1995BayesianModelChoice,
abstract = {Markov chain Monte Carlo (MCMC) integration methods enable the fitting of models of virtually unlimited complexity, and as such have revolutionized the practice of Bayesian data analysis. However, comparison across models may not proceed in a completely analogous fashion, owing to violations of the conditions sufficient to ensure convergence of the Markov chain. In this paper we present a framework for Bayesian model choice, along with an MCMC algorithm that does not suffer from convergence difficulties. Our algorithm applies equally well to problems where only one model is contemplated but its proper size is not known at the outset, such as problems involving integer-valued parameters, multiple changepoints or finite mixture distributions. We illustrate our approach with two published examples.},
author = {Carlin, Bradley P. and Chib, Siddhartha},
doi = {10.1111/j.2517-6161.1995.tb02042.x},
file = {:Users/aksh/Documents/Papers/Carlin, Chib/1995/Journal of the Royal Statistical Society Series B (Methodological)/Carlin, Chib - 1995 - Bayesian Model Choice Via Markov Chain Monte Carlo Methods.pdf:pdf},
journal = {Journal of the Royal Statistical Society: Series B (Methodological)},
keywords = {bayes factor,finite mixture model,gibbs sampler,integer-valued,models of varying size,multiple changepoint model,parameters},
mendeley-groups = {Monte Carlo Methods},
number = {3},
pages = {473--484},
title = {{Bayesian Model Choice Via Markov Chain Monte Carlo Methods}},
volume = {57},
year = {1995}
}

@misc{Abraham2015MetaregEpidemBook,
address = {Seattle},
author = {Abraham, D},
isbn = {9780295991849},
keywords = {Epidemiology -- Statistical methods,Meta-analysis,World health -- Statistical methods},
mendeley-groups = {Epidemiology Modelling},
publisher = {University of Washington Press},
series = {Publications on global health},
title = {{An integrative metaregression framework for descriptive epidemiology}},
year = {2015}
}

@book{Davison1998BootstrapMethods,
abstract = {Bootstrap methods are computer-intensive methods of statistical analysis, which use simulation to calculate standard errors, confidence intervals, and significance tests. The methods apply for any level of modelling, and so can be used for fully parametric, semiparametric, and completely nonparametric analysis. This 1997 book gives a broad and up-to-date coverage of bootstrap methods, with numerous applied examples, developed in a coherent way with the necessary theoretical basis. Applications include stratified data; finite populations; censored and missing data; linear, nonlinear, and smooth regression models; classification; time series and spatial problems. Special features of the book include: extensive discussion of significance tests and confidence intervals; material on various diagnostic methods; and methods for efficient computation, including improved Monte Carlo simulation. Each chapter includes both practical and theoretical exercises. S-Plus programs for implementing the methods described in the text are available from the supporting website.},
author = {Buckland, S. T. and Davison, A. C. and Hinkley, D. V.},
booktitle = {Biometrics},
doi = {10.2307/3109789},
issn = {0006341X},
mendeley-groups = {Bootstrap},
title = {{Bootstrap Methods and Their Application}},
year = {1998}
}



@article{lindstrom_newton-raphson_1988,
	title = {Newton-{Raphson} and {EM} {Algorithms} for {Linear} {Mixed}-{Effects} {Models} for {Repeated}-{Measures} {Data}},
	doi = {10.1080/01621459.1988.10478693},
	language = {en},
	author = {Lindstrom, Mary J and Bates, Douglas M},
	month = dec,
	year = {1988},
	pages = {10},
	file = {Lindstrom and Bates - Newton-Raphson and EM Algorithms for Linear Mixed-.pdf:/Users/aksh/Documents/Papers/storage/JSIEDAXD/Lindstrom and Bates - Newton-Raphson and EM Algorithms for Linear Mixed-.pdf:application/pdf}
}


@article{Harville1977ML,
	title = {Maximum {Likelihood} {Approaches} to {Variance} {Component} {Estimation} and to {Related} {Problems}},
	volume = {72},
	issn = {0162-1459, 1537-274X},
	url = {http://www.tandfonline.com/doi/abs/10.1080/01621459.1977.10480998},
	doi = {10/ggbzhj},
	language = {en},
	number = {358},
	urldate = {2019-10-29},
	journal = {Journal of the American Statistical Association},
	author = {Harville, David A.},
	month = jun,
	year = {1977},
	note = {ZSCC: 0002743},
	pages = {320--338},
	file = {Harville - 1977 - Maximum Likelihood Approaches to Variance Componen.pdf:/Users/aksh/Documents/Papers/storage/7LWJ8HF6/Harville - 1977 - Maximum Likelihood Approaches to Variance Componen.pdf:application/pdf}
}

@article{Jain2017,
abstract = {A vast majority of machine learning algorithms train their models and perform inference by solving optimization problems. In order to capture the learning and prediction problems accurately structural constraints such as sparsity or low rank are frequently imposed or else the objective itself is designed to be a non-convex function. This is especially true of algorithms that operate in high-dimensional spaces or that train non-linear models such as tensor models and deep networks. The freedom to express the learning problem as a non-convex optimization problem gives immense modeling power to the algorithm designer, but often such problems are NP-hard to solve. A popular workaround to this has been to relax non-convex problems to convex ones and use traditional methods to solve the (convex) relaxed optimization problems. However this approach may be lossy and nevertheless presents significant challenges for large scale optimization. On the other hand, direct approaches to non-convex optimization have met with resounding success in several domains and remain the methods of choice for the practitioner, as they frequently outperform relaxation-based techniques - popular heuristics include projected gradient descent and alternating minimization. However, these are often poorly understood in terms of their convergence and other properties. This monograph presents a selection of recent advances that bridge a long-standing gap in our understanding of these heuristics. We hope that an insight into the inner workings of these methods will allow the reader to appreciate the unique marriage of task structure and generative models that allow these heuristic techniques to (provably) succeed. The monograph will lead the reader through several widely used non-convex optimization techniques, as well as applications thereof. The goal of this monograph is to both, introduce the rich literature in this area, as well as equip the reader with the tools and techniques needed to analyze these simple procedures for non-convex problems.},
archivePrefix = {arXiv},
arxivId = {1712.07897},
author = {Jain, Prateek and Kar, Purushottam},
doi = {10.1561/2200000058},
eprint = {1712.07897},
file = {:Users/aksh/Downloads/1712.07897.pdf:pdf},
issn = {19358245},
journal = {Foundations and Trends in Machine Learning},
mendeley-groups = {Other},
number = {3-4},
pages = {142--336},
title = {{Non-convex optimization for machine learning}},
volume = {10},
year = {2017}
}

@article{Ueda2010,
abstract = {The regularized Newton method (RNM) is one of the efficient solution methods for the unconstrained convex optimization. It is well-known that the RNM has good convergence properties as compared to the steepest descent method and the pure Newton's method. For example, Li, Fukushima, Qi and Yamashita showed that the RNM has a quadratic rate of convergence under the local error bound condition. Recently, Polyak showed that the global complexity bound of the RNM, which is the first iteration k such that ∥ ∇ f(x k)∥ ≤ $\epsilon$, is O($\epsilon$ -4), where f is the objective function and $\epsilon$ is a given positive constant. In this paper, we consider a RNM extended to the unconstrained "nonconvex" optimization. We show that the extended RNM (E-RNM) has the following properties. (a) The E-RNM has a global convergence property under appropriate conditions. (b) The global complexity bound of the E-RNM is O($\epsilon$ -2) if ∇ 2 f is Lipschitz continuous on a certain compact set. (c) The E-RNM has a superlinear rate of convergence under the local error bound condition. {\textcopyright} 2009 Springer Science+Business Media, LLC.},
author = {Ueda, Kenji and Yamashita, Nobuo},
doi = {10.1007/s00245-009-9094-9},
file = {:Users/aksh/Downloads/Ueda-Yamashita2010{\_}Article{\_}ConvergencePropertiesOfTheRegu.pdf:pdf},
issn = {0095-4616},
journal = {Applied Mathematics and Optimization},
keywords = {Global complexity bound,Global convergence,Local error bound,Regularized Newton methods,Superlinear convergence},
mendeley-groups = {Second-Order Optimization Methods},
month = {aug},
number = {1},
pages = {27--46},
title = {{Convergence Properties of the Regularized Newton Method for the Unconstrained Nonconvex Optimization}},
url = {http://link.springer.com/10.1007/s00245-009-9094-9},
volume = {62},
year = {2010}
}

@article{Peng2012LMMSelectionOverview,
abstract = {Mixed effect models are fundamental tools for the analysis of longitudinal data, panel data and cross-sectional data. They are widely used by various fields of social sciences, medical and biological sciences. However, the complex nature of these models has made variable selection and parameter estimation a challenging problem. In this paper, we propose a simple iterative procedure that estimates and selects fixed and random effects for linear mixed models. In particular, we propose to utilize the partial consistency property of the random effect coefficients and select groups of random effects simultaneously via a data-oriented penalty function (the smoothly clipped absolute deviation penalty function). We show that the proposed method is a consistent variable selection procedure and possesses some oracle properties. Simulation studies and a real data analysis are also conducted to empirically examine the performance of this procedure. {\textcopyright} 2012 Elsevier Inc.},
author = {Peng, Heng and Lu, Ying},
doi = {10.1016/j.jmva.2012.02.005},
file = {:Users/aksh/Downloads/1-s2.0-S0047259X12000395-main.pdf:pdf},
issn = {0047259X},
journal = {Journal of Multivariate Analysis},
keywords = {Group selection,Model selection,Oracle property,Penalized least squares,SCAD function},
mendeley-groups = {Feature/Effects Selection},
pages = {109--129},
publisher = {Elsevier Inc.},
title = {{Model selection in linear mixed effect models}},
url = {http://dx.doi.org/10.1016/j.jmva.2012.02.005},
volume = {109},
year = {2012}
}

@article{Audenaert2010spectral,
  title={Spectral radius of Hadamard product versus conventional product for non-negative matrices},
  author={Audenaert, Koenraad MR},
  journal={Linear algebra and its applications},
  volume={432},
  number={1},
  pages={366--368},
  year={2010},
  publisher={Elsevier}
}

@article{Drnovvsek2016inequalities,
  title={Inequalities on the spectral radius and the operator norm of Hadamard products of positive operators on sequence spaces},
  author={Drnov{\v{s}}ek, Roman and Peperko, Aljo{\v{s}}a and others},
  journal={Banach Journal of Mathematical Analysis},
  volume={10},
  number={4},
  pages={800--814},
  year={2016},
  publisher={Tusi Mathematical Research Group}
}

@article{Muller2013,
abstract = {Linear mixed effects models are highly flexible in handling a broad range of data types and are therefore widely used in applications. A key part in the analysis of data is model selection, which often aims to choose a parsimonious model with other desirable properties from a possibly very large set of candidate statistical models. Over the last 5-10 years the literature on model selection in linear mixed models has grown extremely rapidly. The problem is much more complicated than in linear regression because selection on the covariance structure is not straightforward due to computational issues and boundary problems arising from positive semidefinite constraints on covariance matrices. To obtain a better understanding of the available methods, their properties and the relationships between them, we review a large body of literature on linear mixed model selection. We arrange, implement, discuss and compare model selection methods based on four major approaches: information criteria such as AIC or BIC, shrinkage methods based on penalized loss functions such as LASSO, the Fence procedure and Bayesian techniques. {\textcopyright} 2013 Institute of Mathematical Statistics.},
author = {M{\"{u}}ller, Samuel and Scealy, J. L. and Welsh, A. H.},
doi = {10.1214/12-STS410},
file = {:Users/aksh/Documents/Papers/2013/M{\"{u}}ller, Scealy, Welsh/Model selection in linear mixed models/M{\"{u}}ller, Scealy, Welsh - 2013 - Model selection in linear mixed models.pdf:pdf},
issn = {08834237},
journal = {Statistical Science},
keywords = {AIC,BIC,Bayes factor,Cholesky decomposition,Fence,Information criteria,LASSO,Linear mixed model,Model selection,Shrinkage methods},
mendeley-groups = {Feature/Effects Selection},
number = {2},
pages = {135--167},
title = {{Model selection in linear mixed models}},
volume = {28},
year = {2013}
}

@article{Feinleib2001,
author = {Feinleib, Manning},
doi = {10.1093/aje/154.1.93-a},
editor = {Porta, Miquel},
isbn = {9780199976720},
issn = {1476-6256},
journal = {American Journal of Epidemiology},
mendeley-groups = {Epidemiology Modelling,Essential Books},
month = {jul},
number = {1},
pages = {93--94},
publisher = {Oxford University Press},
title = {{A Dictionary of Epidemiology, Fourth Edition - Edited by John M. Last, Robert A. Spasoff, and Susan S. Harris}},
url = {http://www.oxfordreference.com/view/10.1093/acref/9780199976720.001.0001/acref-9780199976720 https://academic.oup.com/aje/article-lookup/doi/10.1093/aje/154.1.93-a},
volume = {154},
year = {2001}
}

@article{Harville1974,
author = {Harville, David A.},
doi = {10.1093/biomet/61.2.383},
issn = {0006-3444},
journal = {Biometrika},
mendeley-groups = {Fundamentals of Mixed Models},
number = {2},
pages = {383--385},
title = {{Bayesian inference for variance components using only error contrasts}},
url = {https://academic.oup.com/biomet/article-lookup/doi/10.1093/biomet/61.2.383},
volume = {61},
year = {1974}
}
@article{Patterson1971,
author = {Patterson, H. D. and Thompson, R.},
doi = {10.2307/2334389},
issn = {00063444},
journal = {Biometrika},
mendeley-groups = {Fundamentals of Mixed Models},
month = {dec},
number = {3},
pages = {545},
title = {{Recovery of Inter-Block Information when Block Sizes are Unequal}},
url = {https://www.jstor.org/stable/2334389?origin=crossref},
volume = {58},
year = {1971}
}

@article{Harville1976,
author = {Harville, David},
doi = {10.1214/aos/1176343414},
file = {:Users/aksh/Documents/Papers/1986/Efron/Institute of Mathematical Statistics is collaborating with JSTOR to digitize, preserve, and extend access to The Annals of Statistics..org/Efron - 1986 - Institute of Mathematical Statistics is collaborating with JSTOR to digitize, preserve, and extend access to The Annals.pdf:pdf},
issn = {0090-5364},
journal = {The Annals of Statistics},
mendeley-groups = {Fundamentals of Mixed Models},
month = {mar},
number = {2},
pages = {384--395},
title = {{Extension of the Gauss-Markov Theorem to Include the Estimation of Random Effects}},
url = {http://projecteuclid.org/euclid.aos/1176343414},
volume = {4},
year = {1976}
}

@article{Dempster1977,
abstract = {A series solution of the general three-dimensional equations of linear elasticity is used to find accurate natural frequencies and mode shapes for the flexural vibrations of thick free circular plates. The approximate solution for thick plates, which includes shear and rotary inertia effects, is compared with the accurate series solution. It is found that the approximate solution yields frequencies of sufficient accuracy for most engineering applications within the range of applicability of the approximate theory. {\textcopyright} 1979 by ASME.},
annote = {Creators of EM algorithm as a concept (Q-functions and all this stuff)},
author = {Dempster, A. P. and Laird, N. M. and Rubin, D. B.},
doi = {10.1111/j.2517-6161.1977.tb01600.x},
file = {:Users/aksh/Documents/Papers/1977/Dempster, Laird, Rubin/Maximum Likelihood from Incomplete Data Via the EM Algorithm/Dempster, Laird, Rubin - 1977 - Maximum Likelihood from Incomplete Data Via the EM Algorithm.pdf:pdf},
issn = {00359246},
journal = {Journal of the Royal Statistical Society: Series B (Methodological)},
mendeley-groups = {EM-algorithm},
month = {sep},
number = {1},
pages = {1--22},
title = {{Maximum Likelihood from Incomplete Data Via the EM Algorithm}},
url = {http://doi.wiley.com/10.1111/j.2517-6161.1977.tb01600.x},
volume = {39},
year = {1977}
}

@article{Lindstrom1988,
author = {Lindstrom, Mary J. and Bates, Douglas M.},
doi = {10.2307/2290128},
issn = {01621459},
journal = {Journal of the American Statistical Association},
keywords = {Growth curve,Longitudinal data,Random effects},
mendeley-groups = {Optimization in MMs,Fundamentals of Mixed Models},
month = {dec},
number = {404},
pages = {1014},
title = {{Newton-Raphson and EM Algorithms for Linear Mixed-Effects Models for Repeated-Measures Data}},
url = {https://www.jstor.org/stable/2290128?origin=crossref},
volume = {83},
year = {1988}
}

@article{Tibshirani1996,
author = {Tibshirani, Robert},
doi = {10.1111/j.2517-6161.1996.tb02080.x},
issn = {00359246},
journal = {Journal of the Royal Statistical Society: Series B (Methodological)},
mendeley-groups = {Feature/Effects Selection},
month = {jan},
number = {1},
pages = {267--288},
title = {{Regression Shrinkage and Selection Via the Lasso}},
url = {http://doi.wiley.com/10.1111/j.2517-6161.1996.tb02080.x},
volume = {58},
year = {1996}
}


@article{Xu2015,
abstract = {The goal of this paper is to develop a double penalized hierarchical likelihood (DPHL) with a modified Cholesky decomposition for simultaneously selecting fixed and random effects in mixed effects models. DPHL avoids the use of data likelihood, which usually involves a high-dimensional integral, to define an objective function for variable selection. The resulting DPHL-based estimator enjoys the oracle properties with no requirement on the convexity of loss function. Moreover, a two-stage algorithm is proposed to effectively implement this approach. An H-likelihood-based Bayesian information criterion (BIC) is developed for tuning parameter selection. Simulated data and a real data set are examined to illustrate the efficiency of the proposed method.},
author = {Xu, Peirong and Wang, Tao and Zhu, Hongtu and Zhu, Lixing},
doi = {10.1007/s12561-013-9105-x},
issn = {18671772},
journal = {Statistics in Biosciences},
keywords = {Hierarchical likelihood,Mixed effects models,Modified Cholesky decomposition,Penalized likelihood,Variable selection},
number = {1},
pages = {108--128},
title = {{Double Penalized H-Likelihood for Selection of Fixed and Random Effects in Mixed Effects Models}},
volume = {7},
year = {2015}
}

@article{Ghosh2018,
abstract = {Mixed-effect models are very popular for analyzing data with a hierarchical structure. In medical applications, typical examples include repeated observations within subjects in a longitudinal design, patients nested within centers in a multicenter design. However, recently, due to the medical advances, the number of fixed-effect covariates collected from each patient can be quite large, e.g., data on gene expressions of each patient, and all of these variables are not necessarily important for the outcome. So, it is very important to choose the relevant covariates correctly for obtaining the optimal inference for the overall study. On the other hand, the relevant random effects will often be low-dimensional and pre-specified. In this paper, we consider regularized selection of important fixed-effect variables in linear mixed-effect models along with maximum penalized likelihood estimation of both fixed and random-effect parameters based on general non-concave penalties. Asymptotic and variable selection consistency with oracle properties are proved for low-dimensional cases as well as for high dimensionality of non-polynomial order of sample size (number of parameters is much larger than sample size). We also provide a suitable computationally efficient algorithm for implementation. Additionally, all the theoretical results are proved for a general non-convex optimization problem that applies to several important situations well beyond the mixed model setup (like finite mixture of regressions) illustrating the huge range of applicability of our proposal.},
archivePrefix = {arXiv},
arxivId = {1607.02883},
author = {Ghosh, Abhik and Thoresen, Magne},
doi = {10.1007/s10182-017-0298-z},
eprint = {1607.02883},
issn = {18638171},
journal = {AStA Advances in Statistical Analysis},
mendeley-groups = {Feature/Effects Selection},
number = {2},
pages = {179--210},
title = {{Non-concave penalization in linear mixed-effect models and regularized selection of fixed effects}},
volume = {102},
year = {2018}
}

@book{Miller2002,
annote = {Great reference to subset selection (exhaustive search methods)},
author = {Miller, Alan},
doi = {10.1201/9781420035933},
isbn = {9780429119187},
mendeley-groups = {Feature/Effects Selection},
month = {apr},
publisher = {Chapman and Hall/CRC},
title = {{Subset Selection in Regression}},
url = {https://www.taylorfrancis.com/books/9781420035933},
year = {2002}
}

@article{Fan2001,
annote = {Creators of SCAD. They fixed lasso's bias for larger coefficients by replacing it with SCAD. They also discussed "oracle properties": way to prove that the estimator is assimptotically unbiased.},
author = {Fan, Jianqing and Li, Runze},
doi = {10.1198/016214501753382273},
issn = {0162-1459},
journal = {Journal of the American Statistical Association},
mendeley-groups = {Feature/Effects Selection},
month = {dec},
number = {456},
pages = {1348--1360},
title = {{Variable Selection via Nonconcave Penalized Likelihood and its Oracle Properties}},
url = {http://www.tandfonline.com/doi/abs/10.1198/016214501753382273},
volume = {96},
year = {2001}
}

@article{Zou2006,
abstract = {The lasso is a popular technique for simultaneous estimation and variable selection. Lasso variable selection has been shown to be consistent under certain conditions. In this work we derive a necessary condition for the lasso variable selection to be consistent. Consequently, there exist certain scenarios where the lasso is inconsistent for variable selection. We then propose a new version of the lasso, called the adaptive lasso, where adaptive weights are used for penalising different coefficients in the ℓ1 penalty. We show that the adaptive lasso enjoys the oracle properties; namely, it performs as well us if the true underlying model were given in advance. Similar to the lasso, the adaptive lasso is shown to be near-minimax optimal. Furthermore, the adaptive lasso can be solved by the same efficient algorithm for solving the lasso. We also discuss the extension of the adaptive lasso in generalized linear models and show that the oracle properties still hold under mild regularity conditions. As a byproduct of our theory, the nonnegalive garotte is shown to be consistent for variable selection. {\textcopyright} 2006 American Statistical Association.},
author = {Zou, Hui},
doi = {10.1198/016214506000000735},
issn = {01621459},
journal = {Journal of the American Statistical Association},
keywords = {Asymptotic normality,Lasso,Minimax,Oracle inequality,Oracle procedure,Variable selection},
mendeley-groups = {Feature/Effects Selection},
number = {476},
pages = {1418--1429},
title = {{The adaptive lasso and its oracle properties}},
volume = {101},
year = {2006}
}

@article{Zou2005,
abstract = {Predicting the binding mode of flexible polypeptides to proteins is an important task that falls outside the domain of applicability of most small molecule and protein−protein docking tools. Here, we test the small molecule flexible ligand docking program Glide on a set of 19 non-$\alpha$-helical peptides and systematically improve pose prediction accuracy by enhancing Glide sampling for flexible polypeptides. In addition, scoring of the poses was improved by post-processing with physics-based implicit solvent MM- GBSA calculations. Using the best RMSD among the top 10 scoring poses as a metric, the success rate (RMSD ≤ 2.0 {\AA} for the interface backbone atoms) increased from 21{\%} with default Glide SP settings to 58{\%} with the enhanced peptide sampling and scoring protocol in the case of redocking to the native protein structure. This approaches the accuracy of the recently developed Rosetta FlexPepDock method (63{\%} success for these 19 peptides) while being over 100 times faster. Cross-docking was performed for a subset of cases where an unbound receptor structure was available, and in that case, 40{\%} of peptides were docked successfully. We analyze the results and find that the optimized polypeptide protocol is most accurate for extended peptides of limited size and number of formal charges, defining a domain of applicability for this approach.},
annote = {Creators of Elastic Net. The goal is to combine interpretability (lasso) and similtaneous selection for highly correlated predictors.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Zou, Hui and Hastie, Trevor},
doi = {10.1111/j.1467-9868.2005.00503.x},
eprint = {arXiv:1011.1669v3},
isbn = {9788578110796},
issn = {1369-7412},
journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
keywords = {icle},
mendeley-groups = {Feature/Effects Selection},
month = {apr},
number = {2},
pages = {301--320},
pmid = {25246403},
title = {{Regularization and variable selection via the elastic net}},
url = {http://doi.wiley.com/10.1111/j.1467-9868.2005.00503.x},
volume = {67},
year = {2005}
}


@article{Bradley1998,
abstract = {Computational comparison is made between two feature selection approaches for finding a separating plane that discriminates between two point sets in an n-dimensional feature space that utilizes as few of the n features (dimensions) as possible. In the concave minimization approach [19, 5] a separating plane is generated by minimizing a weighted sum of distances of misclassified points to two parallel planes that bound the sets and which determine the separating plane midway between them. Furthermore, the number of dimensions of the space used to determine the plane is minimized. In the support vector machine approach [27, 7, 1, 10, 24, 28], in addition to minimizing the weighted sum of distances of misclassified points to the bounding planes, we also maximize the distance between the two bounding planes that generate the separating plane. Computational results show that feature suppression is an indirect consequence of the support vector machine approach when an appropriate norm is us...},
annote = {Introduce L1 regularization to SVMs},
author = {Bradley, P and Mangasarian, O},
journal = {Proceedings of the International Conference on Machine Learning},
mendeley-groups = {Feature/Effects Selection},
number = {6},
pages = {82--90},
title = {{Feature Selection via Concave Minimization and Support Vector Machines}},
year = {1998}
}

@article{Lan2006,
annote = {Introduce shrinkage operator (lasso) for selecting FIXED effects in linear mixed models setting. This direction was later continued by Arun Krishna (2008)},
author = {Lan, Lan},
journal = {PhD thesis},
mendeley-groups = {Feature/Effects Selection},
title = {{Variable Selection in Linear Mixed Model for Longitudinal Data}},
year = {2006}
}


@article{Chen2003,
annote = {Introduced selection of random effects in mixed effect models using modified Cholesky decomposition of Gamma = DLL'D.},
author = {Chen, Zhen and Dunson, David B},
doi = {10.1111/j.0006-341X.2003.00089.x},
issn = {0006-341X},
journal = {Biometrics},
keywords = {aging,bayes factor,homogeneity test,latent variables,longitudinal data,mcmc,model aver-,variable selection,variance components},
mendeley-groups = {Feature/Effects Selection},
month = {dec},
number = {4},
pages = {762--769},
title = {{Random Effects Selection in Linear Mixed Models}},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.0006-341X.2003.00089.x},
volume = {59},
year = {2003}
}

@article{Xie2020,
abstract = {Reinforcement learning is one of the paradigms and methodologies of machine learning developed in the computational intelligence community. Reinforcement learning algorithms present a major challenge in complex dynamics recently. In the perspective of variable selection, we often come across situations where too many variables are included in the full model at the initial stage of modeling. Due to a high-dimensional and intractable integral of longitudinal data, likelihood inference is computationally challenging. It can be computationally difficult such as very slow convergence or even nonconvergence, for the computationally intensive methods. Recently, hierarchical likelihood (h-likelihood) plays an important role in inferences for models having unobservable or unobserved random variables. This paper focuses linear models with random effects in the mean structure and proposes a penalized h-likelihood algorithm which incorporates variable selection procedures in the setting of mean modeling via h-likelihood. The penalized h-likelihood method avoids the messy integration for the random effects and is computationally efficient. Furthermore, it demonstrates good performance in relevant-variable selection. Throughout theoretical analysis and simulations, it is confirmed that the penalized h-likelihood algorithm produces good fixed effect estimation results and can identify zero regression coefficients in modeling the mean structure.},
author = {Xie, Yanxi and Li, Yuewen and Xia, Zhijie and Yan, Ruixia and Luan, Dongqing},
doi = {10.1155/2020/8941652},
issn = {1076-2787},
journal = {Complexity},
mendeley-groups = {Feature/Effects Selection},
pages = {1--13},
title = {{A Penalized h-Likelihood Variable Selection Algorithm for Generalized Linear Regression Models with Random Effects}},
volume = {2020},
year = {2020}
}

@article{Lee1996,
abstract = {We propose a class of double hierarchical generalized linear models in which ran- dom effects can be specified for both the mean and dispersion. Heteroscedasticity between clusters can be modelled by introducing random effects in the dispersion model, as is hetero- geneity between clusters in the mean model.This class will, among other things, enable models with heavy-tailed distributions to be explored, providing robust estimation against outliers.The h-likelihood provides a unified framework for this new class of models and gives a single algorithm for fitting all members of the class. This algorithm does not require quadrature or prior probabilities.},
annote = {Proposed h-Likelihood for dealing with},
author = {Lee, Y. and Nelder, J. A.},
doi = {10.1111/j.2517-6161.1996.tb02105.x},
journal = {Journal of the Royal Statistical Society: Series B (Methodological)},
keywords = {canonical link,dispersion components,generalized linear mixed model,h-likelihood,hierarchical likelihood,likelihood,marginal likelihood,profile,quasi-h-likelihood,random effect,restricted maximum},
mendeley-groups = {Fundamentals of Mixed Models},
number = {4},
pages = {619--656},
title = {{Hierarchical Generalized Linear Models}},
volume = {58},
year = {1996}
}

@article{Laird1982,
author = {Laird, Nan M and Ware, James H},
doi = {10.2307/2529876},
file = {:Users/aksh/Documents/Papers/2007/Laird, Ware/Laird-Ware-Biometrics-1982.Pdf/Laird, Ware - 2007 - Laird-Ware-Biometrics-1982.Pdf.pdf:pdf},
issn = {0006341X},
journal = {Biometrics},
mendeley-groups = {Fundamentals of Mixed Models},
month = {dec},
number = {4},
pages = {963},
title = {{Random-Effects Models for Longitudinal Data}},
url = {http://www.stat.cmu.edu/{~}brian/720/week07/laird-ware-biometrics-1982.pdf https://www.jstor.org/stable/2529876?origin=crossref},
volume = {38},
year = {1982}
}

@article{Laird1987,
author = {Laird, Nan and Lange, Nicholas and Stram, Daniel},
doi = {10.1080/01621459.1987.10478395},
issn = {1537274X},
journal = {Journal of the American Statistical Association},
keywords = {Aitken acceleration,Growth curves with random parameters,Linear patterned covariance matrices,Mixed models,Restricted maximum likelihood},
mendeley-groups = {EM-algorithm,Fundamentals of Mixed Models},
number = {397},
pages = {97--105},
title = {{Maximum likelihood computations with repeated measures: Application of the EM algorithm}},
volume = {82},
year = {1987}
}

@article{Wu1983,
abstract = {The Cox regression model for censored survival data specifies that covariates have a proportional effect on the hazard function of the life-time distribution of an individual. In this paper we discuss how this model can be extended to a model where covariate processes have a proportional effect on the intensity process of a multivariate counting process. This permits a statistical regression analysis of the intensity of a recurrent event allowing for complicated censoring patterns and time dependent covariates. Furthermore, this formulation gives rise to proofs with very simple structure using martingale techniques for the asymptotic properties of the estimators from such a model. Finally an example of a statistical analysis is included.},
author = {Wu, C. F. Jeff},
doi = {10.1214/aos/1176346060},
issn = {0090-5364},
journal = {The Annals of Statistics},
mendeley-groups = {EM-algorithm},
month = {mar},
number = {1},
pages = {95--103},
title = {{On the Convergence Properties of the EM Algorithm}},
url = {http://projecteuclid.org/euclid.aos/1176345976 http://projecteuclid.org/euclid.aos/1176346060},
volume = {11},
year = {1983}
}

@article{Balakrishnan2017,
abstract = {The EM algorithm is a widely used tool in maximum-likelihood estimation in incomplete data problems. Existing theoretical work has focused on conditions under which the iterates or likelihood values converge, and the associated rates of convergence. Such guarantees do not distinguish whether the ultimate fixed point is a near global optimum or a bad local optimum of the sample likelihood, nor do they relate the obtained fixed point to the global optima of the idealized population likelihood (obtained in the limit of infinite data). This paper develops a theoretical framework for quantifying when and how quickly EM-type iterates converge to a small neighborhood of a given global optimum of the population likelihood. For correctly specified models, such a characterization yields rigorous guarantees on the performance of certain two-stage estimators in which a suitable initial pilot estimator is refined with iterations of the EM algorithm. Our analysis is divided into two parts: a treatment of the EM and first-order EM algorithms at the population level, followed by results that apply to these algorithms on a finite set of samples. Our conditions allow for a characterization of the region of convergence of EM-type iterates to a given population fixed point, that is, the region of the parameter space over which convergence is guaranteed to a point within a small neighborhood of the specified population fixed point. We verify our conditions and give tight characterizations of the region of convergence for three canonical problems of interest: symmetric mixture of two Gaussians, symmetric mixture of two regressions and linear regression with covariates missing completely at random.},
annote = {Latest, to date, statistical guarantees for EM algorithm},
archivePrefix = {arXiv},
arxivId = {1408.2156},
author = {Balakrishnan, Sivaraman and Wainwright, Martin J. and Yu, Bin},
doi = {10.1214/16-AOS1435},
eprint = {1408.2156},
issn = {0090-5364},
journal = {The Annals of Statistics},
keywords = {EM algorithm,First-order EM algorithm,Maximum likelihood estimation,Nonconvex optimization},
mendeley-groups = {EM-algorithm},
month = {feb},
number = {1},
pages = {77--120},
title = {{Statistical guarantees for the EM algorithm: From population to sample-based analysis}},
url = {http://projecteuclid.org/euclid.aos/1487667618},
volume = {45},
year = {2017}
}

@article{Liu1994,
author = {Liu, Chuanhai and Rubin, Donald B.},
doi = {10.2307/2337067},
issn = {00063444},
journal = {Biometrika},
mendeley-groups = {EM-algorithm},
month = {dec},
number = {4},
pages = {633},
title = {{The ECME Algorithm: A Simple Extension of EM and ECM with Faster Monotone Convergence}},
url = {https://www.jstor.org/stable/2337067?origin=crossref},
volume = {81},
year = {1994}
}

@article{Liu1998,
author = {Liu, C},
doi = {10.1093/biomet/85.4.755},
issn = {0006-3444},
journal = {Biometrika},
mendeley-groups = {EM-algorithm},
month = {dec},
number = {4},
pages = {755--770},
title = {{Parameter expansion to accelerate EM: the PX-EM algorithm}},
url = {https://academic.oup.com/biomet/article-lookup/doi/10.1093/biomet/85.4.755},
volume = {85},
year = {1998}
}

@article{Jennrich1986,
author = {Jennrich, Robert I. and Schluchter, Mark D.},
doi = {10.2307/2530695},
issn = {0006341X},
journal = {Biometrics},
mendeley-groups = {Fundamentals of Mixed Models},
month = {dec},
number = {4},
pages = {805},
title = {{Unbalanced Repeated-Measures Models with Structured Covariance Matrices}},
url = {https://www.jstor.org/stable/2530695?origin=crossref},
volume = {42},
year = {1986}
}

@inbook{TrustRegionBook,

title = {Part III Trust-Region Methods for Constrained Optimization with Convex Constraints},
booktitle = {Trust Region Methods},
chapter = {},
pages = {439-439},
doi = {10.1137/1.9780898719857.pt3},
URL = {https://epubs.siam.org/doi/abs/10.1137/1.9780898719857.pt3},
eprint = {https://epubs.siam.org/doi/pdf/10.1137/1.9780898719857.pt3}
}

@article{Nash2000,
abstract = {Truncated-Newton methods are a family of methods for solving large optimization problems. Over the past two decades, a solid convergence theory has been derived for the methods. In addition, many algorithmic enhancements have been developed and studied, resulting in a number of publicly available software packages. The result has been a collection of powerful, flexible, and adaptable tools for large-scale nonlinear optimization. (C) 2000 Elsevier Science B.V. All rights reserved.},
author = {Nash, Stephen G.},
doi = {10.1016/S0377-0427(00)00426-X},
file = {:Users/aksh/Documents/Papers/2000/Nash/A survey of truncated-Newton methods/Nash - 2000 - A survey of truncated-Newton methods.pdf:pdf},
issn = {03770427},
journal = {Journal of Computational and Applied Mathematics},
mendeley-groups = {Optimization in MMs},
number = {1-2},
pages = {45--59},
title = {{A survey of truncated-Newton methods}},
volume = {124},
year = {2000}
}

@article{Bertsekas1982,
author = {Bertsekas, Dimitri P.},
doi = {10.1137/0320018},
file = {:Users/aksh/Documents/Papers/1982/Optimization/Problems With Simple Constraints/Optimization - 1982 - Problems With Simple Constraints.pdf:pdf},
issn = {0363-0129},
journal = {SIAM Journal on Control and Optimization},
mendeley-groups = {Optimization in MMs},
month = {mar},
number = {2},
pages = {221--246},
title = {{Projected Newton Methods for Optimization Problems with Simple Constraints}},
url = {http://epubs.siam.org/doi/10.1137/0320018},
volume = {20},
year = {1982}
}

@article{Potra2000,
author = {Potra, Florian A. and Wright, Stephen J.},
doi = {10.1016/S0377-0427(00)00433-7},
issn = {03770427},
journal = {Journal of Computational and Applied Mathematics},
mendeley-groups = {Optimization in MMs},
month = {dec},
number = {1-2},
pages = {281--302},
title = {{Interior-point methods}},
url = {https://linkinghub.elsevier.com/retrieve/pii/S0377042700004337},
volume = {124},
year = {2000}
}

@article{Baraldi2019,
author = {Baraldi, Robert and Kumar, Rajiv and Aravkin, Aleksandr},
doi = {10.1109/TSP.2019.2946029},
issn = {1053-587X},
journal = {IEEE Transactions on Signal Processing},
mendeley-groups = {Relax-and-Split examples},
month = {nov},
number = {22},
pages = {5811--5823},
title = {{Basis Pursuit Denoise With Nonsmooth Constraints}},
url = {https://ieeexplore.ieee.org/document/8861392/},
volume = {67},
year = {2019}
}

@article{Zheng2019,
author = {Zheng, Peng and Askham, Travis and Brunton, Steven L. and Kutz, J. Nathan and Aravkin, Aleksandr Y.},
doi = {10.1109/ACCESS.2018.2886528},
issn = {2169-3536},
journal = {IEEE Access},
mendeley-groups = {Relax-and-Split examples},
pages = {1404--1423},
title = {{A Unified Framework for Sparse Relaxed Regularized Regression: SR3}},
url = {https://ieeexplore.ieee.org/document/8573778/},
volume = {7},
year = {2019}
}

@article{Champion2020,
author = {Champion, Kathleen and Zheng, Peng and Aravkin, Aleksandr Y. and Brunton, Steven L. and Kutz, J. Nathan},
doi = {10.1109/ACCESS.2020.3023625},
issn = {2169-3536},
journal = {IEEE Access},
mendeley-groups = {Relax-and-Split examples},
pages = {169259--169271},
title = {{A Unified Sparse Optimization Framework to Learn Parsimonious Physics-Informed Models From Data}},
url = {https://ieeexplore.ieee.org/document/9194760/},
volume = {8},
year = {2020}
}

@unpublished{Belyy2018,
abstract = {Multi-class classification with a very large number of classes, or extreme classification, is a challenging problem from both statistical and computational perspectives. Most of the classical approaches to multi-class classification, including one-vs-rest or multi-class support vector machines, require the exact estimation of the classifier's margin, at both the training and the prediction steps making them intractable in extreme classification scenarios. In this paper, we study the impact of computing an approximate margin using nearest neighbor (ANN) search structures combined with locality-sensitive hashing (LSH). This approximation allows to dramatically reduce both the training and the prediction time without a significant loss in performance. We theoretically prove that this approximation does not lead to a significant loss of the risk of the model and provide empirical evidence over five publicly available large scale datasets, showing that the proposed approach is highly competitive with respect to state-of-the-art approaches on time, memory and performance measures.},
archivePrefix = {arXiv},
arxivId = {1811.09863},
author = {Belyy, Anton and Sholokhov, Aleksei},
eprint = {1811.09863},
mendeley-groups = {Other},
title = {{MEMOIR: Multi-class Extreme Classification with Inexact Margin}},
url = {http://arxiv.org/abs/1811.09863},
year = {2018}
}

@article{Jajuga1991,
author = {Jajuga, Krzysztof},
doi = {10.1016/0165-0114(91)90064-W},
issn = {01650114},
journal = {Fuzzy Sets and Systems},
mendeley-groups = {Other},
month = {jan},
number = {1},
pages = {43--50},
title = {{L1-norm based fuzzy clustering}},
url = {https://linkinghub.elsevier.com/retrieve/pii/016501149190064W},
volume = {39},
year = {1991}
}

@article{Shalev-Shwartz2011,
doi = {10.1007/s10107-010-0420-4},
file = {::},
isbn = {9781595937933},
issn = {00255610},
journal = {Mathematical Programming},
keywords = {SVM,Stochastic gradient descent},
mendeley-groups = {Pegasos},
number = {1},
pages = {3--30},
title = {{Pegasos: Primal estimated sub-gradient solver for SVM}},
volume = {127},
year = {2011}
}

@article{Kim2019,
abstract = {Contextual multi-armed bandit algorithms are widely used in sequential decision tasks such as news article recommendation systems, web page ad placement algorithms, and mobile health. Most of the existing algorithms have regret proportional to a polynomial function of the context dimension, d. In many applications however, it is often the case that contexts are high-dimensional with only a sparse subset of size s0(« d) being correlated with the reward. We consider the stochastic linear contextual bandit problem and propose a novel algorithm, namely the Doubly-Robust Lasso Bandit algorithm, which exploits the sparse structure of the regression parameter as in Lasso, while blending the doubly-robust technique used in missing data literature. The high-probability upper bound of the regret incurred by the proposed algorithm does not depend on the number of arms and scales with log(d) instead of a polynomial function of d. The proposed algorithm shows good performance when contexts of different arms are correlated and requires less tuning parameters than existing methods.},
author = {Kim, Gi Soo and Paik, Myunghee Cho},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems},
mendeley-groups = {Other},
number = {NeurIPS},
title = {{Doubly-robust lasso bandit}},
volume = {32},
year = {2019}
}

@article{Vaida2005,
author = {Vaida, Florin and Blanchard, Suzette},
doi = {10.1093/biomet/92.2.351},
issn = {1464-3510},
journal = {Biometrika},
mendeley-groups = {AIC/BIC},
month = {jun},
number = {2},
pages = {351--370},
title = {{Conditional Akaike information for mixed-effects models}},
url = {http://academic.oup.com/biomet/article/92/2/351/233128/Conditional-Akaike-information-for-mixedeffects},
volume = {92},
year = {2005}
}

@article{Jones2011,
author = {Jones, Richard H.},
doi = {10.1002/sim.4323},
issn = {02776715},
journal = {Statistics in Medicine},
mendeley-groups = {AIC/BIC},
month = {nov},
number = {25},
pages = {3050--3056},
title = {{Bayesian information criterion for longitudinal and clustered data}},
url = {http://doi.wiley.com/10.1002/sim.4323},
volume = {30},
year = {2011}
}

@book{Pinheiro2000,
address = {New York, NY},
author = {Pinheiro, Jos{\'{e}} C. and Bates, Douglas M.},
booktitle = {Journal of the American Statistical Association},
doi = {10.1007/978-1-4419-0318-1},
isbn = {978-1-4419-0317-4},
issn = {0162-1459},
mendeley-groups = {Essential Books},
month = {sep},
number = {455},
pages = {1135--1136},
publisher = {Springer New York},
series = {Statistics and Computing},
title = {{Mixed-Effects Models in Sand S-PLUS}},
url = {http://www.tandfonline.com/doi/abs/10.1198/jasa.2001.s411 http://link.springer.com/10.1007/978-1-4419-0318-1},
volume = {96},
year = {2000}
}

@article{Krishna2008,
archivePrefix = {arXiv},
arxivId = {NIHMS150003},
author = {Bondell, Howard D. and Krishna, Arun and Ghosh, Sujit K.},
doi = {10.1111/j.1541-0420.2010.01391.x},
eprint = {NIHMS150003},
isbn = {6176321972},
issn = {0006341X},
journal = {Biometrics},
keywords = {epiblast,gfp fusion,histone h2b-,icm,lineage specification,live imaging,mouse blastocyst,pdgfr $\alpha$,primitive endoderm},
mendeley-groups = {Feature/Effects Selection},
month = {dec},
number = {4},
pages = {1069--1077},
pmid = {1000000221},
title = {{Joint Variable Selection for Fixed and Random Effects in Linear Mixed-Effects Models}},
url = {http://doi.wiley.com/10.1111/j.1541-0420.2010.01391.x},
volume = {66},
year = {2010}
}

@article{Fan2012,
abstract = {This paper is concerned with the selection and estimation of fixed and random effects in linear mixed effects models.We propose a class of nonconcave penalized profile likelihood methods for selecting and estimating important fixed effects. To overcome the difficulty of unknown covariance matrix of random effects, we propose to use a proxy matrix in the penalized profile likelihood. We establish conditions on the choice of the proxy matrix and show that the proposed procedure enjoys the model selection consistency where the number of fixed effects is allowed to grow exponentially with the sample size.We further propose a group variable selection strategy to simultaneously select and estimate important random effects, where the unknown covariance matrix of random effects is replaced with a proxy matrix.We prove that, with the proxy matrix appropriately chosen, the proposed procedure can identify all true random effects with asymptotic probability one, where the dimension of random effects vector is allowed to increase exponentially with the sample size. Monte Carlo simulation studies are conducted to examine the finite-sample performance of the proposed procedures. We further illustrate the proposed procedures via a real data example. {\textcopyright} 2012 Institute of Mathematical Statistics.},
author = {Fan, Yingying and Li, Runze},
doi = {10.1214/12-AOS1028},
issn = {0090-5364},
journal = {The Annals of Statistics},
keywords = {Adaptive Lasso,Group variable selection,Linear mixed effects models,Oracle property,SCAD},
mendeley-groups = {Feature/Effects Selection},
month = {aug},
number = {4},
pages = {2043--2068},
title = {{Variable selection in linear mixed effects models}},
url = {http://projecteuclid.org/euclid.aos/1351602536},
volume = {40},
year = {2012}
}

@article{Lin2013,
author = {Lin, Bingqing and Pang, Zhen and Jiang, Jiming},
doi = {10.1080/10618600.2012.681219},
issn = {10618600},
journal = {Journal of Computational and Graphical Statistics},
keywords = {BIC,LASSO,Mixed-effects models},
mendeley-groups = {Feature/Effects Selection},
number = {2},
pages = {341--355},
title = {{Fixed and random effects selection by REML and pathwise coordinate optimization}},
volume = {22},
year = {2013}
}

@article{Hui2017,
author = {Hui, Francis K.C. and M{\"{u}}ller, Samuel and Welsh, A. H.},
doi = {10.1080/01621459.2016.1215989},
issn = {1537274X},
journal = {Journal of the American Statistical Association},
keywords = {Fixed effects,Generalized linear mixed models,Lasso,Penalized likelihood,Quasi-likelihood,Variable selection},
mendeley-groups = {Feature/Effects Selection},
number = {519},
pages = {1323--1333},
title = {{Joint Selection in Mixed Models using Regularized PQL}},
volume = {112},
year = {2017}
}

@article{Fan2001,
author = {Fan, Jianqing and Li, Runze},
doi = {10.1198/016214501753382273},
issn = {0162-1459},
journal = {Journal of the American Statistical Association},
keywords = {hard thresholding,lasso,nonnegative garrote,oracle estimator,penalized likelihood,scad,soft thresholding},
mendeley-groups = {Feature/Effects Selection},
month = {dec},
number = {456},
pages = {1348--1360},
title = {{Variable Selection via Nonconcave Penalized Likelihood and its Oracle Properties}},
url = {http://www.tandfonline.com/doi/abs/10.1198/016214501753382273},
volume = {96},
year = {2001}
}

@article{Lange1989,
author = {Lange, Nicholas and Laird, Nan M.},
doi = {10.1080/01621459.1989.10478761},
issn = {0162-1459},
journal = {Journal of the American Statistical Association},
keywords = {Covariance components,Design of experiments,Longitudinal data analysis,Random-effects models,Repeated measures,Restricted maximum likelihood estimation,Variance components},
mendeley-groups = {Fundamentals of Mixed Models},
month = {mar},
number = {405},
pages = {241--247},
title = {{The Effect of Covariance Structure on Variance Estimation in Balanced Growth-Curve Models with Random Parameters}},
url = {http://www.tandfonline.com/doi/abs/10.1080/01621459.1989.10478761},
volume = {84},
year = {1989}
}

@article{Kojima1991,
author = {Kojima, Masakazu and Megiddo, Nimrod and Noma, Toshihito and Yoshise, Akiko},
doi = {10.1016/0167-6377(91)90010-M},
issn = {01676377},
journal = {Operations Research Letters},
mendeley-groups = {Interior Point Methods},
month = {jul},
number = {5},
pages = {247--254},
title = {{A unified approach to interior point algorithms for linear complementarity problems: A summary}},
url = {https://linkinghub.elsevier.com/retrieve/pii/016763779190010M},
volume = {10},
year = {1991}
}

@book{Nesterov1994,
author = {Nesterov, Yurii and Nemirovskii, Arkadii},
doi = {10.1137/1.9781611970791},
isbn = {978-0-89871-319-0},
mendeley-groups = {Interior Point Methods},
month = {jan},
publisher = {Society for Industrial and Applied Mathematics},
title = {{Interior-Point Polynomial Algorithms in Convex Programming}},
url = {http://epubs.siam.org/doi/book/10.1137/1.9781611970791},
year = {1994}
}

@book{Wright1997,
author = {Wright, Stephen J.},
doi = {10.1137/1.9781611971453},
isbn = {978-0-89871-382-4},
mendeley-groups = {Interior Point Methods},
month = {jan},
publisher = {Society for Industrial and Applied Mathematics},
title = {{Primal-Dual Interior-Point Methods}},
url = {http://epubs.siam.org/doi/book/10.1137/1.9781611971453},
year = {1997}
}

@article{IHME2020covidnature,
abstract = {We use COVID-19 case and mortality data from 1 February 2020 to 21 September 2020 and a deterministic SEIR (susceptible, exposed, infectious and recovered) compartmental framework to model possible trajectories of severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) infections and the effects of non-pharmaceutical interventions in the United States at the state level from 22 September 2020 through 28 February 2021. Using this SEIR model, and projections of critical driving covariates (pneumonia seasonality, mobility, testing rates and mask use per capita), we assessed scenarios of social distancing mandates and levels of mask use. Projections of current non-pharmaceutical intervention strategies by state—with social distancing mandates reinstated when a threshold of 8 deaths per million population is exceeded (reference scenario)—suggest that, cumulatively, 511,373 (469,578–578,347) lives could be lost to COVID-19 across the United States by 28 February 2021. We find that achieving universal mask use (95{\%} mask use in public) could be sufficient to ameliorate the worst effects of epidemic resurgences in many states. Universal mask use could save an additional 129,574 (85,284–170,867) lives from September 22, 2020 through the end of February 2021, or an additional 95,814 (60,731–133,077) lives assuming a lesser adoption of mask wearing (85{\%}), when compared to the reference scenario.},
author = {{IHME COVID-19 Forecasting Team}},
doi = {10.1038/s41591-020-1132-9},
issn = {1078-8956},
journal = {Nature Medicine},
mendeley-groups = {Other},
month = {oct},
title = {{Modeling COVID-19 scenarios for the United States}},
url = {http://www.nature.com/articles/s41591-020-1132-9},
year = {2020}
}

@misc{IHME2020Projections,
author = {IHME},
booktitle = {IHME COVID-19 Projections},
mendeley-groups = {Other},
title = {{IHME COVID-19 Projections}},
url = {https://covid19.healthdata.org/global},
year = {2020}
}

@article{Sugiura1978,
author = {Sugiura, Nariaki},
doi = {10.1080/03610927808827599},
issn = {0361-0926},
journal = {Communications in Statistics - Theory and Methods},
mendeley-groups = {AIC/BIC},
month = {jan},
number = {1},
pages = {13--26},
title = {{Further analysts of the data by akaike' s information criterion and the finite corrections}},
url = {http://www.tandfonline.com/doi/abs/10.1080/03610927808827599},
volume = {7},
year = {1978}
}

@article{Fang2011,
abstract = {For model selection in mixed effects models, Vaida and Blan-chard (2005) demonstrated that the marginal Akaike information criterion is appropriate as to the questions regarding the population and the con-ditional Akaike information criterion is appropriate as to the questions re-garding the particular clusters in the data. This article shows that the marginal Akaike information criterion is asymptotically equivalent to the leave-one-cluster-out cross-validation and the conditional Akaike informa-tion criterion is asymptotically equivalent to the leave-one-observation-out cross-validation.},
author = {Fang, Yixin},
file = {:Users/aksh/Documents/Papers/2011/Fang/Asymptotic Equivalence between Cross-Validations and Akaike Information Criteria in Mixed-Effects Models/Fang - 2011 - Asymptotic Equivalence between Cross-Validations and Akaike Information Criteria in Mixed-Effects Models.pdf:pdf},
journal = {Journal of Data Science},
keywords = {AIC,degrees of freedom,functional data,model selection},
mendeley-groups = {AIC/BIC},
pages = {15--21},
title = {{Asymptotic Equivalence between Cross-Validations and Akaike Information Criteria in Mixed-Effects Models}},
volume = {9},
year = {2011}
}

@article{Polyak1964,
author = {Polyak, B.T.},
doi = {10.1016/0041-5553(64)90137-5},
issn = {00415553},
journal = {USSR Computational Mathematics and Mathematical Physics},
mendeley-groups = {Other},
month = {jan},
number = {5},
pages = {1--17},
title = {{Some methods of speeding up the convergence of iteration methods}},
url = {https://linkinghub.elsevier.com/retrieve/pii/0041555364901375},
volume = {4},
year = {1964}
}

@book{Nesterov2004,
address = {Boston, MA},
author = {Nesterov, Yurii},
doi = {10.1007/978-1-4419-8853-9},
isbn = {978-1-4613-4691-3},
mendeley-groups = {Other},
publisher = {Springer US},
series = {Applied Optimization},
title = {{Introductory Lectures on Convex Optimization}},
url = {http://link.springer.com/10.1007/978-1-4419-8853-9},
volume = {87},
year = {2004}
}

@book{nocedal2006numerical,
  title={Numerical optimization},
  author={Nocedal, Jorge and Wright, Stephen},
  year={2006},
  publisher={Springer Science \& Business Media}
}
