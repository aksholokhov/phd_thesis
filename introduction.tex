Machine Learning has emerged as an important tool in applied mathematics problems as data-driven approaches have been increasingly outperforming the first-principle modeling in the tasks of classification, recognition, autonomous control, protein folding, and even playing video games. However, due to the absence of prior knowledge  purely data-driven approaches typically require a large amount of data to approximate behavior of first-principle models even in simplest scenarios. Thus the most promising modeling approaches of today are those that integrate both data-driven and first-principle components. These latter components frequently stem from domain expertise and pre-existing convictions. Incorporating prior knowledge into a data-driven method is a formidable task that demands a considerable amount of ingenuity. This thesis serves to illustrate two distinct examples of such endeavors.  Together, these examples represent a diverse range of challenges in incorporating prior knowledge into a model and describe the strategies employed to surmount them.

An instance of such a problem is feature selection for mixed-effect models. Feature selection problems typically arise when one believes that a small subset of features within a dataset captures the majority of the variance of the quantity of interest; this subset of features is commonly known as sparse support. The canonical feature selection methods typically provide control over the degree of sparsity for the solution and outputs a candidate support for it. In the absence of additional information about the support apart from its probable sparsity, these methods furnish a satisfactory toolkit for practitioners.

 Real-world practitioners, however, poses expert knowledge about the covariates in their data and wish to incorporate that knowledge as a set of priors and constraints. For instance, a practitioner may desire a model that employs at most 6 out of the 40 available features, with features A and B being mandatory components. Furthermore, if feature C is integrated, it must have a negative coefficient, and feature D must be independent of any grouping parameter. Attempts to directly address such constraints often result in the formulation of mathematically intractable problems. For example, the first constraint above makes the underlying optimization problem N-P complete. Additionally, practitioners require the ability to swap these priors as new hypotheses and knowledge come to light. This necessitates the implementation of an optimization routine that does not rely upon these priors, otherwise it would force the practitioner to re-implement the algorithm after every such swap, rendering the instrument impractical. Chapter~\ref{ch:msr3} elucidates my efforts in developing a universal feature selection approach for linear mixed-effects models that simultaneously confronts the numerical challenges that accompany it.

The second example serves to demonstrate how one can incorporate a degree of partial knowledge of physics into a deep neural network. The methods for data-driven modeling of physical phenomena surged in popularity, in large part due to the advancements in GPU computations and automatic differentiation tools. These methods have proven particularly useful in the tasks of reduced-order modeling.  Using data a neural network can find a representation of a system on a reduced-order manifold via a non-linear transformation from an observable space to that manifold. Classic techniques, such as Galerkin POD or DMD, are fundamentally incapable of finding such spaces because these methods are confined to linear, albeit optimal, transformations. However, as the no-free-lunch theorem implies, the exceptional approximation capabilities of deep networks come at the cost of increased instability. In particular, the networks tend to identify manifolds that overfit the data and, consequently, fail at extrapolation tasks. Such deficiencies compromise the efficacy of the entire system in which the network is utilized, including applications in control, model identification, and compressive sensing.

Recent works have shown that one can improve a neural-network based model by incorporating partial knowledge of physics. For example, a model can find a better reduced-order manifold if it knows that the it predicts the behavior of a shock wave that obeys Burger's equation. Chapter~\ref{ch:pinode} is devoted to the development of PINODE ROMs -- a framework for building Reduced-Order Models with Physics-Informed Neural Ordinary Differential Equations. It illustrates how one can transfer knowledge from a known equation to a model that predicts solutions of said equation using collocation points technique.  The resulting ROMs are able to extrapolate forward in time considerably more effectively, as well as perform better for unseen initial conditions and exhibit less sensitivity to noise. Finally, in Chapter~\ref{ch:cs} I demonstrate the utility of PINODE models in compressive sensing applications, where they effectively "stitch" together separate timeframes and significantly exceed the Nyquist sampling limit.

\paragraph{Attribution} Throughout my PhD I was fortunate to work with incredibly talented advisors and collaborators who significantly contributed to the content of this work. Chapter~\ref{ch:msr3} captures a sequence of joint works that were completed between 2019 and 2023 at the University of Washington's Institute for Health Metrics and Evaluations (IHME) together with Dr. Aleksandr Aravkin, Dr. James Burke, and Dr. Peng Zheng. The MSR3 framework algorithm was introduced in \cite{sholokhov2022relaxation}, with the theoretical foundations developed in \cite{aravkin2022jimtheory}, and the software implementation published in \cite{sholokhov2023pysr3}. Chapter~\ref{ch:pinode} represents my work at Mitsubishi Electric Research Labs (MERL) under the guidance of Dr. Hassan Mansour and Dr. Saleh Nabi. The PINODE framework was developed in \cite{sholokhov2023pinode} and is based on our earlier joint work with Dr. Yuying Liu on Physics-Informed Koopman Networks~(\cite{liu2022physics}). In particular, Figure~\ref{fig:pikn_burgers_compression} and the related example come from that work. Chapter~\ref{ch:cs} was developed in~\cite{sholokhov2023cs} together with Dr. Hassan Mansour, Dr. Saleh Nabi, Dr. J. Nathan Kutz, and Dr. Steven Brunton at the University of Washington. Many parts of the works listed above were taken verbatim to this document per an explicit permission of my co-authors.