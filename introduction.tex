\textbf{\textit{The current intro is a draft that captures content and, for now, disregards the form and grammar. It will be rewritten later in a more professional cadence.}}
\edit{Rewrite the introduction: improve style and grammar, add citations}

Machine Learning became an important tool in applied mathematics problems as data-driven approaches have been increasingly outperforming the first-principle ones over the last decade. The most successful approaches, however, combine first-principle and data-driven components. These first-principle components often come from our domain expertise and prior beliefs. Incorporating prior knowledge into a data-driven method is often challenging and requires a good share of ingenuity. This thesis showcases two quite different examples of such work. Together these examples cover a variety of challenges of incorporating priors knowledge into a model and describes approaches to overcome them.

One example is the feature selection problems for mixed-effect models. Feature selection problems typically arise when one believes that a small subset of features in dataset captures the majority of the variance of the quantity in question; this subset of features is often referred to as parse support. The canonical methods typically provide control over the degree of sparsity for the solution and outputs a candidate support for it. For a practitioner who has no additional information about the support besides its likely sparsity these methods provide sufficient toolkit.

The real-world practitioner, however, almost universally knows more than that. As a consequence, they want to incorporate their expert knowledge that mostly comes as a set of constraints. For example, a practitioner wants a model that uses at most 6 features out of 40, with features A and B being included at all times. If a feature C is included then it has to be negative, and a feature D should not depend on a grouping parameter. An attempt to tackle these constraints directly frequently leads to mathematically intractable problems, e.g. the first constraint makes the optimization problem N-P complete. Moreover, a practitioner wants to swap those priors in and out as more hypotheses and knowledge arise. It implies that the implementation of the optimization routine should not rely on the priors, otherwise it forces the practitioner to re-implement the algorithm after every such swap, rendering the instrument impractical. Chapter~\ref{ch:msr3} describes my work on developing such a universal feature selection approach for linear mixed-effects models while simultaneously tackling numerical challenges that arise from it. 

The second example illustrates how one can embed a partial knowledge of physics into a deep neural network. The field of data-driven modeling of physical phenomena soared in popularity over the last decade with the advancements of GPU computations and automatic differentiation tools. It has proven to be especially useful in the field of reduced-order modeling where a neural network can find a representation of a system on a reduced-order manifold via finding a non-linear transformation to that space based on data. The classic methods such as Galerkin POD or DMD are fundamentally unable to find such spaces because these methods are confined to linear, albeit optimal, transformations. However, as the no-free-lunch theorem goes, with great approximation power comes great instability. In particular, the networks tend to find manifolds that overfit the data and fail at extrapolation tasks. Such failures jeopardize the performance of the whole system where the network is used, e.g. in control, model identification, or compressive sensing applications.

A natural idea for improving neural-network based models would be incorporating a partial knowledge of physics of the phenomenon that the model predicts. For example,  we can find a better reduced-order manifold if we know that the model predicts the behavior of a shock wave that obeys Burger's equation. Chapter~\ref{ch:pinode} is devoted to the development of PINODE ROMs -- a framework for building Reduced-Order Models with Physics-Informed Neural Ordinary Differential Equations. It illustrates how one can transfer knowledge from a known equation to a ROM that predicts solutions of said equation using collocation points technique. The resulting ROMs extrapolate significantly better forward in time as well as for unseen initial conditions, and are much less sensitive to noise. Finally, in Chapter~\ref{ch:cs} I show how PINODE models can be put to work in compressive sensing applications where it "stitches" separate timeframes together and allows to significantly surpass the Nyqvist sampling limit.

\paragraph{Attribution} Throughout my PhD I was fortunate to work with incredibly talented advisors and collaborators who significantly contributed to the content of this work. Chapter~\ref{ch:msr3} captures a sequence of joint works that were completed between 2019 and 2023 at the University of Washington's Institute for Health Metrics and Evaluations (IHME) together with Dr. Aleksandr Aravkin, Dr. James Burke, and Dr. Peng Zheng. The MSR3 framework algorithm was introduced in \cite{sholokhov2022relaxation}, with the theoretical foundations developed in \cite{aravkin2022jimtheory}, and a software was later published in \cite{sholokhov2023msr3joss}. Chapter~\ref{ch:pinode} represents my work at Mitsubishi Electric Research Labs (MERL) that I accomplished under the guidance of Dr. Hassan Mansour and Dr. Saleh Nabi. The PINODE framework was developed in \cite{sholokhov2023pinode} and is based on our earlier joint work with Dr. Yuying Liu on Physics-Informed Koopman Networks~(\cite{liu2022physics}). In particular, Figure~\ref{fig:pikn_burgers_compression} and the related example come from that work. Chapter~\ref{ch:cs} was developed in~\cite{sholokhov2023cs} together with Dr. J. Nathan Kutz and Dr. Steven Brunton at the University of Washington. Many parts of the works listed above were taken verbatim to this document per an explicit permission of my co-authors.