Machine learning has emerged as an important tool in applied mathematics problems as data-driven approaches have begun to outperform  first-principle modeling in the tasks of classification (\cite{lecun1999object}), recognition (\cite{deng2009imagenet,shafiq2022deep}),  autonomous control (\cite{janai2020computer}), protein folding (\cite{jumper2021highly}), and even video games (\cite{chen2016evolution}). However, due to the absence of prior knowledge,  purely data-driven approaches typically require a large amount of data to approximate the behavior of first-principle models (\cite{kutz2013data,pan2018long}) . Thus, the most promising modeling approaches of today are those that integrate both data-driven and first-principle components (\cite{rackauckas2020udes,raissi2019physics}). These latter components frequently stem from domain expertise and pre-existing beliefs. Incorporating prior knowledge into a data-driven method is a formidable task that demands a considerable amount of ingenuity. This thesis serves to illustrate two distinct examples of such endeavors.  Together, these examples represent a diverse range of challenges in incorporating prior knowledge into a model and describe the strategies employed to surmount them.

An instance of such a problem is feature selection for mixed-effect models. Feature selection problems typically arise when one believes that a small subset of features within a dataset captures the majority of the variance of the quantity of interest; this subset of features is commonly known as sparse support. The canonical feature selection methods, e.g. \cite{ibrahim2011fixed,Fan2012}, typically provide control over the  sparsity of the solution and output a candidate support for it. In the absence of additional information about the support apart from its probable sparsity, these methods furnish a satisfactory toolkit for practitioners.

 Real-world practitioners, however, pose expert knowledge about the covariates in their data and wish to incorporate that knowledge as a set of priors and constraints. For instance, a practitioner may desire a model that employs at most 6 out of the 40 available features, with features A and B being mandatory components. Furthermore, if feature C is integrated, it must have a negative coefficient, and feature D must be independent of any grouping parameter. Attempts to directly address such constraints often result in the formulation of mathematically intractable problems. Additionally, practitioners require the ability to swap these priors as new hypotheses and knowledge come to light. This necessitates the implementation of an optimization routine that does not rely upon these priors; otherwise it would force the practitioner to re-implement the algorithm after every such swap, rendering the method impractical. Chapter~\ref{ch:msr3} elucidates my efforts in developing a universal feature selection approach for linear mixed-effects models that simultaneously offers flexibility for incorporating a variety of priors and confronts the numerical challenges that accompany such flexibility.

The second example serves to demonstrate how one can incorporate a degree of partial knowledge of physics into a deep neural network. Methods for data-driven modeling of physical phenomena have been increasingly popular in large part due to the advancements in GPU computations and automatic differentiation tools, such as pytorch (\cite{pytorch}), jax (\cite{jax2018github}), and Neural ODE frameworks (\cite{chen2018neuralode}). These methods have proven particularly useful in reduced-order modeling tasks (\cite{rackauckas2020udes}).  Using data, a neural network can find a representation of a system on a reduced-order manifold via a non-linear transformation from an observable space to that manifold. Classic techniques, such as Galerkin Proper Orthogonal Decomposition (POD) (\cite{fukunaga2013introduction}) or Dynamic Mode Decomposition (DMD) (\cite{schmid2022dynamic}), are fundamentally incapable of finding such spaces because these methods are confined to linear, albeit optimal, transformations (\cite{benner2015survey}). However, the exceptional approximation capabilities of deep networks come at the cost of increased instability. In particular, the networks tend to identify manifolds that overfit the data and, consequently, fail at extrapolation tasks. Such deficiencies compromise the efficacy of the entire system in which the network is utilized, including applications in control, model identification, and single-pixel imaging.

Recent works, e.g. \cite{raissi2019physics,liu2022physics}, have shown that one can improve a neural-network based model by incorporating partial knowledge of physics. For example, a model can find a better reduced-order manifold if it knows that it predicts the behavior of a shock wave that obeys Burger's equation. Chapter~\ref{ch:pinode} is devoted to the development of a framework for training Reduced-Order Models (ROMs) with Physics-Informed Neural Ordinary Differential Equations (PINODE). It illustrates how one can transfer knowledge from a known equation to a model that predicts solutions of said equation using collocation points.  The resulting ROMs are able to extrapolate forward in time considerably more effectively,  perform better for unseen initial conditions, and exhibit less sensitivity to noise. Finally, in Chapter~\ref{ch:cs} I demonstrate the utility of PINODE models in applications to single-pixel imaging (SPI), where they effectively "stitch" together separate timeframes and go below the Nyquist sampling limit.

\paragraph{Attribution} Throughout my PhD I was fortunate to work with incredibly talented advisors and collaborators who significantly contributed to the content of this work. Chapter~\ref{ch:msr3} captures a sequence of joint works that were completed between 2019 and 2023 at the University of Washington's Institute for Health Metrics and Evaluations (IHME) together with Dr. Aleksandr Aravkin, Dr. James Burke, and Dr. Peng Zheng. The MSR3 framework algorithm was introduced in \cite{sholokhov2022relaxation}, with the theoretical foundations developed in \cite{aravkin2022jimtheory}, and the software implementation published in \cite{sholokhov2023pysr3}. Chapter~\ref{ch:pinode} represents my work at Mitsubishi Electric Research Labs (MERL) under the guidance of Dr. Hassan Mansour and Dr. Saleh Nabi. The PINODE framework was developed in \cite{sholokhov2023pinode} (under peer review at the moment of writing this document) and is based on our earlier joint work with Dr. Yuying Liu on Physics-Informed Koopman Networks~(\cite{liu2022physics}). In particular, Figure~\ref{fig:pikn_burgers_compression} and the related example come from that work. Chapter~\ref{ch:cs} was developed in~\cite{sholokhov2023cs} together with Dr. Hassan Mansour, Dr. Saleh Nabi, Dr. J. Nathan Kutz, and Dr. Steven Brunton at the University of Washington, and is an ongoing work to date. Many parts of the works listed above were taken verbatim to this document per the explicit permission of my co-authors.